{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Welcome Percona Monitoring and Management (PMM) is a free, open-source database and system monitoring tool for MySQL, PostgreSQL, MongoDB, ProxySQL, and the servers they run on. This is the technical documentation for the latest release: PMM 2.13.0 What is Percona Monitoring and Management? How it works PMM Server PMM Client Percona Enterprise Platform Documentation map What is Percona Monitoring and Management? PMM is software that helps you improve the performance of database instances, simplify their management, and strengthen their security. With it, you can: Visualize a wide range of out-of-the-box system performance details Collect and analyze data across complex multi-vendor system topologies Drill-down and discover the cause of inefficiencies Anticipate performance issues, troubleshoot existing ones Watch for potential security issues and remedy them PMM is efficient, quick to set up and easy to use. It runs in cloud, on-prem, or across hybrid platforms. It is supported by Percona\u2019s legendary expertise in open source databases, and by a vibrant developer and user community . Try the online demo at https://pmmdemo.percona.com/ How it works PMM is a client/server application built by Percona with their own and third-party open-source tools. To set it up, you must: install and configure a PMM Client on each host to be monitored. The PMM Client package provides exporters for different database and system types, and administration tools and agents. install and run a PMM Server that communicates with clients, receiving metrics data and presenting it in a web-based user interface. We provide packages for both PMM Server and PMM Client. (See more in Architecture .) PMM Server PMM Server is the heart of PMM. It receives data from clients, collates it and stores it. Metrics are drawn as tables, charts and graphs within dashboards , each a part of the web-based user interface . PMM Server can run as: A Docker container ; An OVA/OVF virtual appliance running on VirtualBox, VMWare and other hypervisors; An Amazon AWS EC2 instance . Quickstart installation https://www.percona.com/software/pmm/quickstart PMM Client PMM Client runs on every database host or node you wish to monitor. The client collects server metrics, general system metrics, and query analytics data, and sends it to the server. You must set up and configure PMM clients for each monitored system type: MySQL Percona Server for MySQL MongDB PostgreSQL ProxySQL Amazon RDS Linux External services Percona Enterprise Platform Percona Enterprise Platform (in development) provides value-added services for PMM. Security Threat Tool Security Threat Tool checks registered database instances for a range of common security issues. (You must turn on Telemetry to use this service.) Documentation map","title":"Welcome"},{"location":"index.html#what-is-percona-monitoring-and-management","text":"PMM is software that helps you improve the performance of database instances, simplify their management, and strengthen their security. With it, you can: Visualize a wide range of out-of-the-box system performance details Collect and analyze data across complex multi-vendor system topologies Drill-down and discover the cause of inefficiencies Anticipate performance issues, troubleshoot existing ones Watch for potential security issues and remedy them PMM is efficient, quick to set up and easy to use. It runs in cloud, on-prem, or across hybrid platforms. It is supported by Percona\u2019s legendary expertise in open source databases, and by a vibrant developer and user community . Try the online demo at https://pmmdemo.percona.com/","title":"What is Percona Monitoring and Management?"},{"location":"index.html#how-it-works","text":"PMM is a client/server application built by Percona with their own and third-party open-source tools. To set it up, you must: install and configure a PMM Client on each host to be monitored. The PMM Client package provides exporters for different database and system types, and administration tools and agents. install and run a PMM Server that communicates with clients, receiving metrics data and presenting it in a web-based user interface. We provide packages for both PMM Server and PMM Client. (See more in Architecture .)","title":"How it works"},{"location":"index.html#pmm-server","text":"PMM Server is the heart of PMM. It receives data from clients, collates it and stores it. Metrics are drawn as tables, charts and graphs within dashboards , each a part of the web-based user interface . PMM Server can run as: A Docker container ; An OVA/OVF virtual appliance running on VirtualBox, VMWare and other hypervisors; An Amazon AWS EC2 instance . Quickstart installation https://www.percona.com/software/pmm/quickstart","title":"PMM Server"},{"location":"index.html#pmm-client","text":"PMM Client runs on every database host or node you wish to monitor. The client collects server metrics, general system metrics, and query analytics data, and sends it to the server. You must set up and configure PMM clients for each monitored system type: MySQL Percona Server for MySQL MongDB PostgreSQL ProxySQL Amazon RDS Linux External services","title":"PMM Client"},{"location":"index.html#percona-enterprise-platform","text":"Percona Enterprise Platform (in development) provides value-added services for PMM.","title":"Percona Enterprise Platform"},{"location":"index.html#documentation-map","text":"","title":"Documentation map"},{"location":"faq.html","text":"FAQ How can I contact the developers? How can I contact the technical writers? What are the minimum system requirements for PMM? How can I upgrade from PMM version 1? How to control data retention for PMM? How often are NGINX logs in PMM Server rotated? What privileges are required to monitor a MySQL instance? Can I monitor multiple service instances? Can I rename instances? Can I add an AWS RDS MySQL or Aurora MySQL instance from a non-default AWS partition? How do I troubleshoot communication issues between PMM Client and PMM Server? What resolution is used for metrics? How do I set up Alerting in PMM? How do I use a custom Prometheus configuration file inside PMM Server? How to troubleshoot an Update? What are my login credentials when I try to connect to a Prometheus Exporter? How to provision PMM Server with non-default admin password? How can I contact the developers? The best place to discuss PMM with developers and other community members is the community forum . To report a bug, visit the PMM project in JIRA . How can I contact the technical writers? Open a Jira ticket Open a Github issue What are the minimum system requirements for PMM? PMM Server Any system which can run Docker version 1.12.6 or later. It needs roughly 1 GB of storage for each monitored database node with data retention set to one week. Note By default, retention is set to 30 days for Metrics Monitor and for Query Analytics. You can consider disabling table statistics to decrease the VictoriaMetrics database size. The minimum memory requirement is 2 GB for one monitored database node. Note The increase in memory usage is not proportional to the number of nodes. For example, data from 20 nodes should be easily handled with 16 GB. PMM Client Any modern 64-bit Linux distribution. It is tested on the latest versions of Debian, Ubuntu, CentOS, and Red Hat Enterprise Linux. A minimum of 100 MB of storage is required for installing the PMM Client package. With a good connection to PMM Server, additional storage is not required. However, the client needs to store any collected data that it cannot dispatch immediately, so additional storage may be required if the connection is unstable or the throughput is low. (Caching only applies to Query Analytics data; VictoriaMetrics data is never cached on the client side.) How can I upgrade from PMM version 1? Because of the significant architectural changes between PMM1 and PMM2, there is no direct upgrade path. The approach to making the switch from PMM version 1 to 2 is a gradual transition, outlined in this blog post . In short, it involves first standing up a new PMM2 server on a new host and connecting clients to it. As new data is reported to the PMM2 server, old metrics will age out during the course of the retention period (30 days, by default), at which point you\u2019ll be able to shut down your existing PMM1 server. Note Any alerts configured through the Grafana UI will have to be recreated due to the target dashboard id\u2019s not matching between PMM1 and PMM2. In this instance we recommend moving to Alertmanager recipes in PMM2 for alerting which, for the time being, requires a separate Alertmanager instance. However, we are working on integrating this natively into PMM2 Server and expect to support your existing Alertmanager rules. How to control data retention for PMM? By default, PMM stores time-series data for 30 days. Depending on your available disk space and requirements, you may need to adjust the data retention time: Go to PMM > PMM Settings > Advanced Settings . Change the data retention value. Click Apply changes . How often are NGINX logs in PMM Server rotated? PMM Server runs logrotate on a daily basis to rotate NGINX logs and keeps up to ten of the most recent log files. What privileges are required to monitor a MySQL instance? GRANT SELECT , PROCESS , SUPER , REPLICATION CLIENT , RELOAD ON * . * TO 'pmm' @ 'localhost' ; Can I monitor multiple service instances? You can add multiple instances of MySQL or some other service to be monitored from one PMM Client. In this case, you must provide a unique port and IP address, or a socket for each instance, and specify a unique name for each. (If a name is not provided, PMM uses the name of the PMM Client host.) For example, to add complete MySQL monitoring for two local MySQL servers, the commands would be: sudo pmm-admin add mysql --username root --password root instance-01 127 .0.0.1:3001 sudo pmm-admin add mysql --username root --password root instance-02 127 .0.0.1:3002 For more information, run: pmm-admin add mysql --help Can I rename instances? You can remove any monitoring instance and then add it back with a different name. When you remove a monitoring service, previously collected data remains available in Grafana. However, the metrics are tied to the instance name. So if you add the same instance back with a different name, it will be considered a new instance with a new set of metrics. So if you are re-adding an instance and want to keep its previous data, add it with the same name. Can I add an AWS RDS MySQL or Aurora MySQL instance from a non-default AWS partition? By default, the RDS discovery works with the default aws partition. But you can switch to special regions, like the GovCloud one, with the alternative AWS partitions (e.g. aws-us-gov ) adding them to the Settings via the PMM Server API . To specify other than the default value, or to use several, use the JSON Array syntax: [\"aws\", \"aws-cn\"] . How do I troubleshoot communication issues between PMM Client and PMM Server? Broken network connectivity may be due to many reasons. Particularly, when using Docker , the container is constrained by the host-level routing and firewall rules. For example, your hosting provider might have default iptables rules on their hosts that block communication between PMM Server and PMM Client, resulting in DOWN targets in VictoriaMetrics. If this happens, check the firewall and routing settings on the Docker host. PMM is also able to generate diagnostics data which can be examined and/or shared with Percona Support to help quickly solve an issue. You can get collected logs from PMM Client using the pmm-admin summary command. Logs obtained in this way includes PMM Client logs and logs which were received from the PMM Server, stored separately in the client and server folders. The server folder also contains its own client subfolder with the self-monitoring client information collected on the PMM Server. Note Beginning with PMM version 2.4.0, there is an additional flag that enables the fetching of pprof debug profiles and adds them to the diagnostics data. To enable, run pmm-admin summary --pprof . You can get PMM Server logs in two ways: In a browser, visit https://<address-of-your-pmm-server>/logs.zip . Go to PMM > PMM Settings and click Download server diagnostics . (See Diagnostics in PMM Settings .) What resolution is used for metrics? The default values are: Low: 60 seconds Medium: 10 seconds High: 5 seconds (See Metrics resolution .) How do I set up Alerting in PMM? When a monitored service metric reaches a defined threshold, PMM Server can trigger alerts for it either using the Grafana Alerting feature or by using an external alert manager. With these methods you must configure alerting rules that define conditions under which an alert should be triggered, and the channel used to send the alert (e.g. email). Alerting in Grafana allows attaching rules to your dashboard panels. Grafana Alerts are already integrated into PMM Server and may be simpler to get set up. Alertmanager allows the creation of more sophisticated alerting rules and can be easier to manage installations with a large number of hosts. This additional flexibility comes at the expense of simplicity. Note We can only offer support for creating custom rules to Percona customers, so you should already have a working Alertmanager instance prior to using this feature. See also PMM Alerting with Grafana: Working with Templated Dashboards How do I use a custom Prometheus configuration file inside PMM Server? Normally, PMM Server fully manages the Prometheus configuration file . However, some users may want to change the generated configuration to add additional scrape jobs, configure remote storage, etc. From version 2.4.0, when pmm-managed starts the Prometheus file generation process, it tries to load the /srv/prometheus/prometheus.base.yml file first, to use it as a base for the prometheus.yml file. The prometheus.yml file can be regenerated by restarting the PMM Server container, or by using the SetSettings API call with an empty body. See also API Percona blog: Extending PMM\u2019s Prometheus Configuration How to troubleshoot an Update? If PMM server wasn\u2019t updated properly, or if you have concerns about the release, you can force the update process in 2 ways: From the UI - Home panel: click with the Alt key on the reload icon in the Update panel (IMG needed) to make the Update Button visible even if you are on the same version as available for update. Pressing this button will force the system to rerun the update so that any broken or not installed components can be installed. In this case, you\u2019ll go through the usual update process with update logs and successful messages at the end. By API call (if UI not available): You can call the Update API directly with: curl --user admin:admin --request POST 'http://PMM_SERVER/v1/Updates/Start' Replace admin:admin with your username/password, and replace PMM_SERVER with your server address. Note You will not see the logs using this method. Refresh The Home page in 2-5 min and you should see that PMM was updated. What are my login credentials when I try to connect to a Prometheus Exporter? PMM protects an exporter\u2019s output from unauthorized access by adding an authorization layer. To access an exporter you can use \u201c pmm \u201d as a user name and the Agent ID as a password. You can find the Agent ID corresponding to a given exporter by running pmm-admin list . How to provision PMM Server with non-default admin password? Currently there is no API available to change the admin password. If you\u2019re deploying through Docker you can use the following code snippet to change the password after starting the Docker container: PMMPASSWORD = \"mypassword\" echo \"Waiting for PMM to initialize to set password...\" until [ \"`docker inspect -f {{.State.Health.Status}} pmm2-server`\" = \"healthy\" ] ; do sleep 1 ; done docker exec -t pmm2-server bash -c \"ln -s /srv/grafana /usr/share/grafana/data; grafana-cli --homepath /usr/share/grafana admin reset-admin-password $PMMPASSWORD \"","title":"FAQ"},{"location":"faq.html#how-can-i-contact-the-developers","text":"The best place to discuss PMM with developers and other community members is the community forum . To report a bug, visit the PMM project in JIRA .","title":"How can I contact the developers?"},{"location":"faq.html#how-can-i-contact-the-technical-writers","text":"Open a Jira ticket Open a Github issue","title":"How can I contact the technical writers?"},{"location":"faq.html#what-are-the-minimum-system-requirements-for-pmm","text":"PMM Server Any system which can run Docker version 1.12.6 or later. It needs roughly 1 GB of storage for each monitored database node with data retention set to one week. Note By default, retention is set to 30 days for Metrics Monitor and for Query Analytics. You can consider disabling table statistics to decrease the VictoriaMetrics database size. The minimum memory requirement is 2 GB for one monitored database node. Note The increase in memory usage is not proportional to the number of nodes. For example, data from 20 nodes should be easily handled with 16 GB. PMM Client Any modern 64-bit Linux distribution. It is tested on the latest versions of Debian, Ubuntu, CentOS, and Red Hat Enterprise Linux. A minimum of 100 MB of storage is required for installing the PMM Client package. With a good connection to PMM Server, additional storage is not required. However, the client needs to store any collected data that it cannot dispatch immediately, so additional storage may be required if the connection is unstable or the throughput is low. (Caching only applies to Query Analytics data; VictoriaMetrics data is never cached on the client side.)","title":"What are the minimum system requirements for PMM?"},{"location":"faq.html#how-can-i-upgrade-from-pmm-version-1","text":"Because of the significant architectural changes between PMM1 and PMM2, there is no direct upgrade path. The approach to making the switch from PMM version 1 to 2 is a gradual transition, outlined in this blog post . In short, it involves first standing up a new PMM2 server on a new host and connecting clients to it. As new data is reported to the PMM2 server, old metrics will age out during the course of the retention period (30 days, by default), at which point you\u2019ll be able to shut down your existing PMM1 server. Note Any alerts configured through the Grafana UI will have to be recreated due to the target dashboard id\u2019s not matching between PMM1 and PMM2. In this instance we recommend moving to Alertmanager recipes in PMM2 for alerting which, for the time being, requires a separate Alertmanager instance. However, we are working on integrating this natively into PMM2 Server and expect to support your existing Alertmanager rules.","title":"How can I upgrade from PMM version 1?"},{"location":"faq.html#how-to-control-data-retention-for-pmm","text":"By default, PMM stores time-series data for 30 days. Depending on your available disk space and requirements, you may need to adjust the data retention time: Go to PMM > PMM Settings > Advanced Settings . Change the data retention value. Click Apply changes .","title":"How to control data retention for PMM?"},{"location":"faq.html#how-often-are-nginx-logs-in-pmm-server-rotated","text":"PMM Server runs logrotate on a daily basis to rotate NGINX logs and keeps up to ten of the most recent log files.","title":"How often are NGINX logs in PMM Server rotated?"},{"location":"faq.html#what-privileges-are-required-to-monitor-a-mysql-instance","text":"GRANT SELECT , PROCESS , SUPER , REPLICATION CLIENT , RELOAD ON * . * TO 'pmm' @ 'localhost' ;","title":"What privileges are required to monitor a MySQL instance?"},{"location":"faq.html#can-i-monitor-multiple-service-instances","text":"You can add multiple instances of MySQL or some other service to be monitored from one PMM Client. In this case, you must provide a unique port and IP address, or a socket for each instance, and specify a unique name for each. (If a name is not provided, PMM uses the name of the PMM Client host.) For example, to add complete MySQL monitoring for two local MySQL servers, the commands would be: sudo pmm-admin add mysql --username root --password root instance-01 127 .0.0.1:3001 sudo pmm-admin add mysql --username root --password root instance-02 127 .0.0.1:3002 For more information, run: pmm-admin add mysql --help","title":"Can I monitor multiple service instances?"},{"location":"faq.html#can-i-rename-instances","text":"You can remove any monitoring instance and then add it back with a different name. When you remove a monitoring service, previously collected data remains available in Grafana. However, the metrics are tied to the instance name. So if you add the same instance back with a different name, it will be considered a new instance with a new set of metrics. So if you are re-adding an instance and want to keep its previous data, add it with the same name.","title":"Can I rename instances?"},{"location":"faq.html#can-i-add-an-aws-rds-mysql-or-aurora-mysql-instance-from-a-non-default-aws-partition","text":"By default, the RDS discovery works with the default aws partition. But you can switch to special regions, like the GovCloud one, with the alternative AWS partitions (e.g. aws-us-gov ) adding them to the Settings via the PMM Server API . To specify other than the default value, or to use several, use the JSON Array syntax: [\"aws\", \"aws-cn\"] .","title":"Can I add an AWS RDS MySQL or Aurora MySQL instance from a non-default AWS partition?"},{"location":"faq.html#how-do-i-troubleshoot-communication-issues-between-pmm-client-and-pmm-server","text":"Broken network connectivity may be due to many reasons. Particularly, when using Docker , the container is constrained by the host-level routing and firewall rules. For example, your hosting provider might have default iptables rules on their hosts that block communication between PMM Server and PMM Client, resulting in DOWN targets in VictoriaMetrics. If this happens, check the firewall and routing settings on the Docker host. PMM is also able to generate diagnostics data which can be examined and/or shared with Percona Support to help quickly solve an issue. You can get collected logs from PMM Client using the pmm-admin summary command. Logs obtained in this way includes PMM Client logs and logs which were received from the PMM Server, stored separately in the client and server folders. The server folder also contains its own client subfolder with the self-monitoring client information collected on the PMM Server. Note Beginning with PMM version 2.4.0, there is an additional flag that enables the fetching of pprof debug profiles and adds them to the diagnostics data. To enable, run pmm-admin summary --pprof . You can get PMM Server logs in two ways: In a browser, visit https://<address-of-your-pmm-server>/logs.zip . Go to PMM > PMM Settings and click Download server diagnostics . (See Diagnostics in PMM Settings .)","title":"How do I troubleshoot communication issues between PMM Client and PMM Server?"},{"location":"faq.html#what-resolution-is-used-for-metrics","text":"The default values are: Low: 60 seconds Medium: 10 seconds High: 5 seconds (See Metrics resolution .)","title":"What resolution is used for metrics?"},{"location":"faq.html#how-do-i-set-up-alerting-in-pmm","text":"When a monitored service metric reaches a defined threshold, PMM Server can trigger alerts for it either using the Grafana Alerting feature or by using an external alert manager. With these methods you must configure alerting rules that define conditions under which an alert should be triggered, and the channel used to send the alert (e.g. email). Alerting in Grafana allows attaching rules to your dashboard panels. Grafana Alerts are already integrated into PMM Server and may be simpler to get set up. Alertmanager allows the creation of more sophisticated alerting rules and can be easier to manage installations with a large number of hosts. This additional flexibility comes at the expense of simplicity. Note We can only offer support for creating custom rules to Percona customers, so you should already have a working Alertmanager instance prior to using this feature. See also PMM Alerting with Grafana: Working with Templated Dashboards","title":"How do I set up Alerting in PMM?"},{"location":"faq.html#how-do-i-use-a-custom-prometheus-configuration-file-inside-pmm-server","text":"Normally, PMM Server fully manages the Prometheus configuration file . However, some users may want to change the generated configuration to add additional scrape jobs, configure remote storage, etc. From version 2.4.0, when pmm-managed starts the Prometheus file generation process, it tries to load the /srv/prometheus/prometheus.base.yml file first, to use it as a base for the prometheus.yml file. The prometheus.yml file can be regenerated by restarting the PMM Server container, or by using the SetSettings API call with an empty body. See also API Percona blog: Extending PMM\u2019s Prometheus Configuration","title":"How do I use a custom Prometheus configuration file inside PMM Server?"},{"location":"faq.html#how-to-troubleshoot-an-update","text":"If PMM server wasn\u2019t updated properly, or if you have concerns about the release, you can force the update process in 2 ways: From the UI - Home panel: click with the Alt key on the reload icon in the Update panel (IMG needed) to make the Update Button visible even if you are on the same version as available for update. Pressing this button will force the system to rerun the update so that any broken or not installed components can be installed. In this case, you\u2019ll go through the usual update process with update logs and successful messages at the end. By API call (if UI not available): You can call the Update API directly with: curl --user admin:admin --request POST 'http://PMM_SERVER/v1/Updates/Start' Replace admin:admin with your username/password, and replace PMM_SERVER with your server address. Note You will not see the logs using this method. Refresh The Home page in 2-5 min and you should see that PMM was updated.","title":"How to troubleshoot an Update?"},{"location":"faq.html#what-are-my-login-credentials-when-i-try-to-connect-to-a-prometheus-exporter","text":"PMM protects an exporter\u2019s output from unauthorized access by adding an authorization layer. To access an exporter you can use \u201c pmm \u201d as a user name and the Agent ID as a password. You can find the Agent ID corresponding to a given exporter by running pmm-admin list .","title":"What are my login credentials when I try to connect to a Prometheus Exporter?"},{"location":"faq.html#how-to-provision-pmm-server-with-non-default-admin-password","text":"Currently there is no API available to change the admin password. If you\u2019re deploying through Docker you can use the following code snippet to change the password after starting the Docker container: PMMPASSWORD = \"mypassword\" echo \"Waiting for PMM to initialize to set password...\" until [ \"`docker inspect -f {{.State.Health.Status}} pmm2-server`\" = \"healthy\" ] ; do sleep 1 ; done docker exec -t pmm2-server bash -c \"ln -s /srv/grafana /usr/share/grafana/data; grafana-cli --homepath /usr/share/grafana admin reset-admin-password $PMMPASSWORD \"","title":"How to provision PMM Server with non-default admin password?"},{"location":"details/index.html","text":"Details: Overview Architecture : high-level architecture and main components. Dashboards reference : A complete list of dashboards by category, with screenshots Commands pmm-admin : A manual page for the PMM administration tool pmm-agent : A manual page for the PMM Client agent program API : How to access the Swagger API VictoriaMetrics : the third-party monitoring solution and time-series database that replaced Prometheus in PMM 2.12.0 Glossary : A list of obscure terms and definitions","title":"Details: Overview"},{"location":"details/api.html","text":"API PMM Server lets you visually interact with API resources representing all objects within PMM. You can browse the API using the Swagger UI, accessible at the /swagger/ endpoint URL: Clicking an object lets you examine objects and execute requests on them: The objects visible are nodes, services, and agents: A Node represents a bare metal server, a virtual machine, a Docker container, or a more specific type such as an Amazon RDS Node. A node runs zero or more Services and Agents, and has zero or more Agents providing insights for it. A Service represents something useful running on the Node: Amazon Aurora MySQL, MySQL, MongoDB, etc. It runs on zero (Amazon Aurora Serverless), single (MySQL), or several (Percona XtraDB Cluster) Nodes. It also has zero or more Agents providing insights for it. An Agent represents something that runs on the Node which is not useful in itself but instead provides insights (metrics, query performance data, etc) about Nodes and/or Services. An agent always runs on the single Node (except External Exporters), and provides insights for zero or more Services and Nodes. Nodes, Services, and Agents have Types which define specific their properties, and the specific logic they implement. Nodes and Services are external by nature \u2013 we do not manage them (create, destroy), but merely maintain a list of them (add to inventory, remove from inventory) in pmm-managed . Most Agents, however, are started and stopped by pmm-agent . The only exception is the External Exporter Type which is started externally.","title":"API"},{"location":"details/architecture.html","text":"Architecture PMM works on the client/server principle, where a single server instance communicates with one or more clients. Except when monitoring AWS RDS instances, a PMM Client must be running on the host to be monitored. PMM context The PMM Client package provides: Exporters for each database and service type. When an exporter runs, it connects to the database or service instance, runs the metrics collection routines, and sends the results to PMM Server. pmm-agent: Run as a daemon process, it starts and stops exporters when instructed. vmagent: A VictoriaMetrics daemon process that sends metrics data ( pushes ) to PMM Server. The PMM Server package provides: pmm-managed Query Analytics Grafana VictoriaMetrics PMM Server PMM Server includes the following tools: Query Analytics (QAN) enables you to analyze MySQL query performance over periods of time. In addition to the client-side QAN agent, it includes the following: QAN API is the backend for storing and accessing query data collected by the QAN agent running on a PMM Client. QAN Web App is a web application for visualizing collected Query Analytics data. Metrics Monitor provides a historical view of metrics that are critical to a MySQL or MongoDB server instance. It includes the following: VictoriaMetrics , a scalable time-series database. (Replaced Prometheus in PMM 2.12.0 .) ClickHouse is a third-party column-oriented database that facilitates the Query Analytics functionality. Grafana is a third-party dashboard and graph builder for visualizing data aggregated (by VictoriaMetrics or Prometheus) in an intuitive web interface. Percona Dashboards is a set of dashboards for Grafana developed by Percona. PMM Client The PMM Client package consist of the following: pmm-admin is a command-line tool for managing PMM Client, for example, adding and removing database instances that you want to monitor. ( Read more. ). pmm-agent is a client-side component a minimal command-line interface, which is a central entry point in charge for bringing the client functionality: it carries on client\u2019s authentication, gets the client configuration stored on the PMM Server, manages exporters and other agents. node_exporter is an exporter that collects general system metrics. mysqld_exporter is an exporter that collects MySQL server metrics. mongodb_exporter is an exporter that collects MongoDB server metrics. postgres_exporter is an exporter that collects PostgreSQL performance metrics. proxysql_exporter is an exporter that collects ProxySQL performance metrics. rds_exporter is an exporter that collects Amazon RDS performance metrics. To make data transfer from PMM Client to PMM Server secure, all exporters are able to use SSL/TLS encrypted connections, and their communication with the PMM server is protected by the HTTP basic authentication.","title":"Architecture"},{"location":"details/architecture.html#pmm-context","text":"The PMM Client package provides: Exporters for each database and service type. When an exporter runs, it connects to the database or service instance, runs the metrics collection routines, and sends the results to PMM Server. pmm-agent: Run as a daemon process, it starts and stops exporters when instructed. vmagent: A VictoriaMetrics daemon process that sends metrics data ( pushes ) to PMM Server. The PMM Server package provides: pmm-managed Query Analytics Grafana VictoriaMetrics","title":"PMM context"},{"location":"details/architecture.html#pmm-server","text":"PMM Server includes the following tools: Query Analytics (QAN) enables you to analyze MySQL query performance over periods of time. In addition to the client-side QAN agent, it includes the following: QAN API is the backend for storing and accessing query data collected by the QAN agent running on a PMM Client. QAN Web App is a web application for visualizing collected Query Analytics data. Metrics Monitor provides a historical view of metrics that are critical to a MySQL or MongoDB server instance. It includes the following: VictoriaMetrics , a scalable time-series database. (Replaced Prometheus in PMM 2.12.0 .) ClickHouse is a third-party column-oriented database that facilitates the Query Analytics functionality. Grafana is a third-party dashboard and graph builder for visualizing data aggregated (by VictoriaMetrics or Prometheus) in an intuitive web interface. Percona Dashboards is a set of dashboards for Grafana developed by Percona.","title":"PMM Server"},{"location":"details/architecture.html#pmm-client","text":"The PMM Client package consist of the following: pmm-admin is a command-line tool for managing PMM Client, for example, adding and removing database instances that you want to monitor. ( Read more. ). pmm-agent is a client-side component a minimal command-line interface, which is a central entry point in charge for bringing the client functionality: it carries on client\u2019s authentication, gets the client configuration stored on the PMM Server, manages exporters and other agents. node_exporter is an exporter that collects general system metrics. mysqld_exporter is an exporter that collects MySQL server metrics. mongodb_exporter is an exporter that collects MongoDB server metrics. postgres_exporter is an exporter that collects PostgreSQL performance metrics. proxysql_exporter is an exporter that collects ProxySQL performance metrics. rds_exporter is an exporter that collects Amazon RDS performance metrics. To make data transfer from PMM Client to PMM Server secure, all exporters are able to use SSL/TLS encrypted connections, and their communication with the PMM server is protected by the HTTP basic authentication.","title":"PMM Client"},{"location":"details/glossary.html","text":"div.section dl.glossary dt {font-weight: bold; font-size: 1.3em;} div.section dd {margin-top: 10px; margin-bottom: 10px; margin-left: 30px;} Glossary Annotation A way of showing a mark on dashboards signifying an important point in time. Dimension In the Query Analytics dashboard, to help focus on the possible source of performance issues, you can group queries by dimension , one of: Query, Service Name, Database, Schema, User Name, Client Host EBS Amazon\u2019s Elastic Block Store. Fingerprint A normalized statement digest \u2014a query string with values removed that acts as a template or typical example for a query. IAM Identity and Access Management (for Amazon AWS). MM Metrics Monitor. NUMA Non-Uniform Memory Access. PEM Privacy Enhanced Mail. QPS Queries Per Second. A measure of the rate of queries being monitored. Query Analytics Component of PMM Server that enables you to analyze MySQL query performance over periods of time. STT Security Threat Tool. VG Volume Group.","title":"Glossary"},{"location":"details/glossary.html#annotation","text":"A way of showing a mark on dashboards signifying an important point in time.","title":"Annotation"},{"location":"details/glossary.html#dimension","text":"In the Query Analytics dashboard, to help focus on the possible source of performance issues, you can group queries by dimension , one of: Query, Service Name, Database, Schema, User Name, Client Host","title":"Dimension"},{"location":"details/glossary.html#ebs","text":"Amazon\u2019s Elastic Block Store.","title":"EBS"},{"location":"details/glossary.html#fingerprint","text":"A normalized statement digest \u2014a query string with values removed that acts as a template or typical example for a query.","title":"Fingerprint"},{"location":"details/glossary.html#iam","text":"Identity and Access Management (for Amazon AWS).","title":"IAM"},{"location":"details/glossary.html#mm","text":"Metrics Monitor.","title":"MM"},{"location":"details/glossary.html#numa","text":"Non-Uniform Memory Access.","title":"NUMA"},{"location":"details/glossary.html#pem","text":"Privacy Enhanced Mail.","title":"PEM"},{"location":"details/glossary.html#qps","text":"Queries Per Second. A measure of the rate of queries being monitored.","title":"QPS"},{"location":"details/glossary.html#query-analytics","text":"Component of PMM Server that enables you to analyze MySQL query performance over periods of time.","title":"Query Analytics"},{"location":"details/glossary.html#stt","text":"Security Threat Tool.","title":"STT"},{"location":"details/glossary.html#vg","text":"Volume Group.","title":"VG"},{"location":"details/victoria-metrics.html","text":"VictoriaMetrics VictoriaMetrics is a third-party monitoring solution and time-series database that replaced Prometheus in PMM 2.12.0 . Push/Pull modes VictoriaMetrics allows metrics data to be \u2018pushed\u2019 to the server in addition to it being \u2018pulled\u2019 by the server. When setting up services, you can decide which mode to use. Note For PMM 2.12.0 the default mode is \u2018pull\u2019. Later releases will use the \u2018push\u2019 mode by default for newly-added services. The mode (push/pull) is controlled by the --metrics-mode flag for the pmm-admin config and pmm-admin add commands. If you need to change the metrics mode for an existing Service, you must remove it and re-add it with the same name and the required flags. (There is currently no ability to \u201cupdate\u201d a service.) Remapped targets for direct Prometheus paths Direct Prometheus paths return structured information directly from Prometheus, bypassing the PMM application. They are accessed by requesting a URL of the form <PMM SERVER URL>/prometheus/<PATH> . As a result of the move to VictoriaMetrics some direct Prometheus paths are no longer available. Here are their equivalents. /prometheus/alerts \u2192 No change. /prometheus/config \u2192 No equivalent. However, some information is at /prometheus/targets . /prometheus/flags \u2192 The flag metrics at /prometheus/metrics . /prometheus/graph \u2192 /graph/explore (Grafana) or graph/d/prometheus-advanced/advanced-data-exploration (PMM dashboard). /prometheus/rules \u2192 No change. /prometheus/service-discovery \u2192 No equivalent. /prometheus/status \u2192 Some information at /prometheus/metrics . High cardinality metrics information at /prometheus/api/v1/status/tsdb . /prometheus/targets \u2192 /victoriametrics/targets . Troubleshooting To troubleshoot issues, see the VictoriaMetrics troubleshooting documentation . You can also contact the VictoriaMetrics team via: Google Groups Slack Reddit Telegram","title":"VictoriaMetrics"},{"location":"details/victoria-metrics.html#pushpull-modes","text":"VictoriaMetrics allows metrics data to be \u2018pushed\u2019 to the server in addition to it being \u2018pulled\u2019 by the server. When setting up services, you can decide which mode to use. Note For PMM 2.12.0 the default mode is \u2018pull\u2019. Later releases will use the \u2018push\u2019 mode by default for newly-added services. The mode (push/pull) is controlled by the --metrics-mode flag for the pmm-admin config and pmm-admin add commands. If you need to change the metrics mode for an existing Service, you must remove it and re-add it with the same name and the required flags. (There is currently no ability to \u201cupdate\u201d a service.)","title":"Push/Pull modes"},{"location":"details/victoria-metrics.html#remapped-targets-for-direct-prometheus-paths","text":"Direct Prometheus paths return structured information directly from Prometheus, bypassing the PMM application. They are accessed by requesting a URL of the form <PMM SERVER URL>/prometheus/<PATH> . As a result of the move to VictoriaMetrics some direct Prometheus paths are no longer available. Here are their equivalents. /prometheus/alerts \u2192 No change. /prometheus/config \u2192 No equivalent. However, some information is at /prometheus/targets . /prometheus/flags \u2192 The flag metrics at /prometheus/metrics . /prometheus/graph \u2192 /graph/explore (Grafana) or graph/d/prometheus-advanced/advanced-data-exploration (PMM dashboard). /prometheus/rules \u2192 No change. /prometheus/service-discovery \u2192 No equivalent. /prometheus/status \u2192 Some information at /prometheus/metrics . High cardinality metrics information at /prometheus/api/v1/status/tsdb . /prometheus/targets \u2192 /victoriametrics/targets .","title":"Remapped targets for direct Prometheus paths"},{"location":"details/victoria-metrics.html#troubleshooting","text":"To troubleshoot issues, see the VictoriaMetrics troubleshooting documentation . You can also contact the VictoriaMetrics team via: Google Groups Slack Reddit Telegram","title":"Troubleshooting"},{"location":"details/commands/pmm-admin.html","text":"pmm-admin - Administration Tool NAME SYNOPSIS DESCRIPTION COMMON FLAGS COMMANDS GENERAL COMMANDS INFORMATION COMMANDS CONFIGURATION COMMANDS DATABASE COMMANDS OTHER COMMANDS EXAMPLES NAME pmm-admin - Administer PMM SYNOPSIS pmm-admin [FLAGS] pmm-admin config [FLAGS] --server-url=server-url pmm-admin add DATABASE [FLAGS] [NAME] [ADDRESS] pmm-admin add external [FLAGS] [NAME] [ADDRESS] (CAUTION: Technical preview feature) pmm-admin add external-serverless [FLAGS] [NAME] [ADDRESS] (CAUTION: Technical preview feature) pmm-admin remove [FLAGS] service-type [service-name] pmm-admin register [FLAGS] [node-address] [node-type] [node-name] pmm-admin list [FLAGS] [node-address] pmm-admin status [FLAGS] [node-address] pmm-admin summary [FLAGS] [node-address] pmm-admin annotate [--node|--service] [--tags <tags>] [node-name|service-name] pmm-admin help [COMMAND] DESCRIPTION pmm-admin is a command-line tool for administering PMM using a set of COMMAND keywords and associated FLAGS. PMM communicates with the PMM Server via a PMM agent process. COMMON FLAGS -h , --help Show help and exit. --help-long Show extended help and exit. --help-man Generate man page. (Use pmm-admin --help-man | man -l - to view.) --debug Enable debug logging. --trace Enable trace logging (implies debug). --json Enable JSON output. --version Show the application version and exit. --server-url=server-url PMM Server URL in https://username:password@pmm-server-host/ format. --server-insecure-tls Skip PMM Server TLS certificate validation. --group=<group-name> Group name for external services. Default: external COMMANDS GENERAL COMMANDS pmm-admin help [COMMAND] Show help for COMMAND . INFORMATION COMMANDS pmm-admin list --server-url=server-url [FLAGS] Show Services and Agents running on this Node, and the agent mode (push/pull). pmm-admin status --server-url=server-url [FLAGS] Show the following information about a local pmm-agent, and its connected server and clients: Agent: Agent ID, Node ID. PMM Server: URL and version. PMM Client: connection status, time drift, latency, vmagent status, pmm-admin version. Agents: Agent ID path and client name. FLAGS: --wait=<period><unit> Time to wait for a successful response from pmm-agent. period is an integer. unit is one of ms for milliseconds, s for seconds, m for minutes, h for hours. pmm-admin summary --server-url=server-url [FLAGS] Creates an archive file in the current directory with default filename summary_<hostname>_<year>_<month>_<date>_<hour>_<minute>_<second>.zip . The contents are two directories, client and server containing diagnostic text files. FLAGS: --filename=\"filename\" The Summary Archive filename. --skip-server Skip fetching logs.zip from PMM Server. --pprof Include performance profiling data in the summary. CONFIGURATION COMMANDS pmm-admin config pmm-admin config [FLAGS] [node-address] [node-type] [node-name] Configure a local pmm-agent . FLAGS: --node-id=node-id Node ID (default is auto-detected). --node-model=node-model Node model --region=region Node region --az=availability-zone Node availability zone --force Remove Node with that name with all dependent Services and Agents if one exist --metrics-mode=mode Metrics flow mode for agents node-exporter. Allowed values: - auto : chosen by server (default) - push : agent will push metrics - pull : server scrapes metrics from agent pmm-admin register pmm-admin register [FLAGS] [node-address] [node-type] [node-name] Register the current Node with the PMM Server. --server-url=server-url PMM Server URL in https://username:password@pmm-server-host/ format. --machine-id=\"/machine_id/9812826a1c45454a98ba45c56cc4f5b0\" Node machine-id (default is auto-detected). --distro=\"linux\" Node OS distribution (default is auto-detected). --container-id=container-id Container ID. --container-name=container-name Container name. --node-model=node-model Node model. --region=region Node region. --az=availability-zone Node availability zone. --custom-labels=labels Custom user-assigned labels. --force Remove Node with that name with all dependent Services and Agents if one exists. pmm-admin remove pmm-admin remove [FLAGS] service-type [service-name] Remove Service from monitoring. --service-id=service-id Service ID. pmm-admin annotate pmm-admin annotate [--node|--service] <annotation> [--tags <tags>] [--node-name=<node>] [--service-name=<service>] Annotate an event. ( Read more ) <annotation> The annotation string. If it contains spaces, it should be quoted. --node Annotate the current node or that specified by --node-name . --service Annotate all services running on the current node, or that specified by --service-name . --tags A quoted string that defines one or more comma-separated tags for the annotation. Example: \"tag 1,tag 2\" . --node-name The node name being annotated. --service-name The service name being annotated. Combining flags Flags may be combined as shown in the following examples. --node current node --node-name node with name --node --node-name=NODE_NAME node with name --node --service-name current node and service with name --node --node-name --service-name node with name and service with name --node --service current node and all services of current node -node --node-name --service --service-name service with name and node with name --service all services of the current node --service-name service with name --service --service-name service with name --service --node-name all services of current node and node with name --service-name --node-name service with name and node with name --service --service-name -node-name service with name and node with name Note If node or service name is specified, they are used instead of other parameters. DATABASE COMMANDS MongoDB pmm-admin add mongodb [FLAGS] [node-name] [node-address] Add MongoDB to monitoring. FLAGS: --node-id=node-id Node ID (default is auto-detected). --pmm-agent-id=pmm-agent-id The pmm-agent identifier which runs this instance (default is auto-detected). --username=username MongoDB username. --password=password MongoDB password. --query-source=profiler Source of queries, one of: profiler , none (default: profiler ). --environment=environment Environment name. --cluster=cluster Cluster name. --replication-set=replication-set Replication set name. --custom-labels=custom-labels Custom user-assigned labels. --skip-connection-check Skip connection check. --tls Use TLS to connect to the database. --tls-skip-verify Skip TLS certificates validation. --tls-certificate-key-file=PATHTOCERT Path to TLS certificate file. --tls-certificate-key-file=IFPASSWORDTOCERTISSET Password for TLS certificate file. --tls-ca-file=PATHTOCACERT Path to certificate authority file. --metrics-mode=mode Metrics flow mode for agents node-exporter. Allowed values: - auto : chosen by server (default) - push : agent will push metrics - pull : server scrapes metrics from agent MySQL pmm-admin add mysql [FLAGS] node-name node-address | [--name=service-name] --address=address[:port] | --socket Add MySQL to monitoring. FLAGS: --address MySQL address and port (default: 127.0.0.1:3306). --socket=socket Path to MySQL socket. --node-id=node-id Node ID (default is auto-detected). --pmm-agent-id=pmm-agent-id The pmm-agent identifier which runs this instance (default is auto-detected). --username=username MySQL username. --password=password MySQL password. --query-source=slowlog Source of SQL queries, one of: slowlog , perfschema , none (default: slowlog ). --size-slow-logs=N Rotate slow log file at this size (default: server-defined; negative value disables rotation). --disable-queryexamples Disable collection of query examples. --disable-tablestats Disable table statistics collection. --disable-tablestats-limit=disable-tablestats-limit Table statistics collection will be disabled if there are more than specified number of tables (default: server-defined). --environment=environment Environment name. --cluster=cluster Cluster name. --replication-set=replication-set Replication set name. --custom-labels=custom-labels Custom user-assigned labels. --skip-connection-check Skip connection check. --tls Use TLS to connect to the database. --tls-skip-verify Skip TLS certificates validation. --metrics-mode=mode Metrics flow mode for agents node-exporter. Allowed values: - auto : chosen by server (default) - push : agent will push metrics - pull : server scrapes metrics from agent PostgreSQL pmm-admin add postgresql [FLAGS] [node-name] [node-address] Add PostgreSQL to monitoring. FLAGS: --node-id=<node id> Node ID (default is auto-detected). --pmm-agent-id=<pmm agent id> The pmm-agent identifier which runs this instance (default is auto-detected). --username=<username> PostgreSQL username. --password=<password> PostgreSQL password. --query-source=<query source> Source of SQL queries, one of: pgstatements , pgstatmonitor , none (default: pgstatements ). --environment=<environment> Environment name. --cluster=<cluster> Cluster name. --replication-set=<replication set> Replication set name --custom-labels=<custom labels> Custom user-assigned labels. --skip-connection-check Skip connection check. --tls Use TLS to connect to the database. --tls-skip-verify Skip TLS certificates validation. --metrics-mode=mode Metrics flow mode for agents node-exporter. Allowed values: - auto : chosen by server (default) - push : agent will push metrics - pull : server scrapes metrics from agent ProxySQL pmm-admin add proxysql [FLAGS] [node-name] [node-address] Add ProxySQL to monitoring. FLAGS: --node-id=node-id Node ID (default is auto-detected). --pmm-agent-id=pmm-agent-id The pmm-agent identifier which runs this instance (default is auto-detected). --username=username ProxySQL username. --password=password ProxySQL password. --environment=environment Environment name. --cluster=cluster Cluster name. --replication-set=replication-set Replication set name. --custom-labels=custom-labels Custom user-assigned labels. --skip-connection-check Skip connection check. --tls Use TLS to connect to the database. --tls-skip-verify Skip TLS certificates validation. --metrics-mode=mode Metrics flow mode for agents node-exporter. Allowed values: - auto : chosen by server (default) - push : agent will push metrics - pull : server scrapes metrics from agent OTHER COMMANDS pmm-admin add external-serverless [FLAGS] Add External Service on Remote node to monitoring FLAGS: --server-url=SERVER-URL PMM Server URL in https://username:password@pmm-server-host/ format --server-insecure-tls Skip PMM Server TLS certificate validation --external-name=EXTERNAL-NAME Name for external service --listen-port=LISTEN-PORT Listen port of external exporter for scraping metrics --metrics-path=METRICS-PATH Path under which metrics are exposed, used to generate URL --environment=ENVIRONMENT Environment name --cluster=CLUSTER Cluster name --replication-set=REPLICATION-SET Replication set name --custom-labels=CUSTOM-LABELS Custom user-assigned labels --group=\"external\" Group name of external service (default: external) --machine-id=MACHINE-ID Node machine-id --distro=DISTRO Node OS distribution --container-id=CONTAINER-ID Container ID --container-name=CONTAINER-NAME Container name --node-model=NODE-MODEL Node model --region=REGION Node region --az=AZ Node availability zone EXAMPLES pmm-admin add mysql --query-source = slowlog --username = pmm --password = pmm sl-mysql 127 .0.0.1:3306 MySQL Service added. Service ID : /service_id/a89191d4-7d75-44a9-b37f-a528e2c4550f Service name: sl-mysql pmm-admin add mysql --username = pmm --password = pmm --service-name = ps-mysql --host = 127 .0.0.1 --port = 3306 pmm-admin status pmm-admin status --wait = 30s Agent ID: /agent_id/c2a55ac6-a12f-4172-8850-4101237a4236 Node ID : /node_id/29b2cc24-3b90-4892-8d7e-4b44258d9309 PMM Server: URL : https://x.x.x.x:443/ Version: 2.5.0 PMM Client: Connected : true Time drift: 2.152715ms Latency : 465.658\u00b5s pmm-admin version: 2.5.0 pmm-agent version: 2.5.0 Agents: /agent_id/aeb42475-486c-4f48-a906-9546fc7859e8 mysql_slowlog_agent Running","title":"pmm-admin - Administration Tool"},{"location":"details/commands/pmm-admin.html#name","text":"pmm-admin - Administer PMM","title":"NAME"},{"location":"details/commands/pmm-admin.html#synopsis","text":"pmm-admin [FLAGS] pmm-admin config [FLAGS] --server-url=server-url pmm-admin add DATABASE [FLAGS] [NAME] [ADDRESS] pmm-admin add external [FLAGS] [NAME] [ADDRESS] (CAUTION: Technical preview feature) pmm-admin add external-serverless [FLAGS] [NAME] [ADDRESS] (CAUTION: Technical preview feature) pmm-admin remove [FLAGS] service-type [service-name] pmm-admin register [FLAGS] [node-address] [node-type] [node-name] pmm-admin list [FLAGS] [node-address] pmm-admin status [FLAGS] [node-address] pmm-admin summary [FLAGS] [node-address] pmm-admin annotate [--node|--service] [--tags <tags>] [node-name|service-name] pmm-admin help [COMMAND]","title":"SYNOPSIS"},{"location":"details/commands/pmm-admin.html#description","text":"pmm-admin is a command-line tool for administering PMM using a set of COMMAND keywords and associated FLAGS. PMM communicates with the PMM Server via a PMM agent process.","title":"DESCRIPTION"},{"location":"details/commands/pmm-admin.html#common-flags","text":"-h , --help Show help and exit. --help-long Show extended help and exit. --help-man Generate man page. (Use pmm-admin --help-man | man -l - to view.) --debug Enable debug logging. --trace Enable trace logging (implies debug). --json Enable JSON output. --version Show the application version and exit. --server-url=server-url PMM Server URL in https://username:password@pmm-server-host/ format. --server-insecure-tls Skip PMM Server TLS certificate validation. --group=<group-name> Group name for external services. Default: external","title":"COMMON FLAGS"},{"location":"details/commands/pmm-admin.html#commands","text":"","title":"COMMANDS"},{"location":"details/commands/pmm-admin.html#general-commands","text":"pmm-admin help [COMMAND] Show help for COMMAND .","title":"GENERAL COMMANDS"},{"location":"details/commands/pmm-admin.html#information-commands","text":"pmm-admin list --server-url=server-url [FLAGS] Show Services and Agents running on this Node, and the agent mode (push/pull). pmm-admin status --server-url=server-url [FLAGS] Show the following information about a local pmm-agent, and its connected server and clients: Agent: Agent ID, Node ID. PMM Server: URL and version. PMM Client: connection status, time drift, latency, vmagent status, pmm-admin version. Agents: Agent ID path and client name. FLAGS: --wait=<period><unit> Time to wait for a successful response from pmm-agent. period is an integer. unit is one of ms for milliseconds, s for seconds, m for minutes, h for hours. pmm-admin summary --server-url=server-url [FLAGS] Creates an archive file in the current directory with default filename summary_<hostname>_<year>_<month>_<date>_<hour>_<minute>_<second>.zip . The contents are two directories, client and server containing diagnostic text files. FLAGS: --filename=\"filename\" The Summary Archive filename. --skip-server Skip fetching logs.zip from PMM Server. --pprof Include performance profiling data in the summary.","title":"INFORMATION COMMANDS"},{"location":"details/commands/pmm-admin.html#configuration-commands","text":"","title":"CONFIGURATION COMMANDS"},{"location":"details/commands/pmm-admin.html#database-commands","text":"","title":"DATABASE COMMANDS"},{"location":"details/commands/pmm-admin.html#other-commands","text":"pmm-admin add external-serverless [FLAGS] Add External Service on Remote node to monitoring FLAGS: --server-url=SERVER-URL PMM Server URL in https://username:password@pmm-server-host/ format --server-insecure-tls Skip PMM Server TLS certificate validation --external-name=EXTERNAL-NAME Name for external service --listen-port=LISTEN-PORT Listen port of external exporter for scraping metrics --metrics-path=METRICS-PATH Path under which metrics are exposed, used to generate URL --environment=ENVIRONMENT Environment name --cluster=CLUSTER Cluster name --replication-set=REPLICATION-SET Replication set name --custom-labels=CUSTOM-LABELS Custom user-assigned labels --group=\"external\" Group name of external service (default: external) --machine-id=MACHINE-ID Node machine-id --distro=DISTRO Node OS distribution --container-id=CONTAINER-ID Container ID --container-name=CONTAINER-NAME Container name --node-model=NODE-MODEL Node model --region=REGION Node region --az=AZ Node availability zone","title":"OTHER COMMANDS"},{"location":"details/commands/pmm-admin.html#examples","text":"pmm-admin add mysql --query-source = slowlog --username = pmm --password = pmm sl-mysql 127 .0.0.1:3306 MySQL Service added. Service ID : /service_id/a89191d4-7d75-44a9-b37f-a528e2c4550f Service name: sl-mysql pmm-admin add mysql --username = pmm --password = pmm --service-name = ps-mysql --host = 127 .0.0.1 --port = 3306 pmm-admin status pmm-admin status --wait = 30s Agent ID: /agent_id/c2a55ac6-a12f-4172-8850-4101237a4236 Node ID : /node_id/29b2cc24-3b90-4892-8d7e-4b44258d9309 PMM Server: URL : https://x.x.x.x:443/ Version: 2.5.0 PMM Client: Connected : true Time drift: 2.152715ms Latency : 465.658\u00b5s pmm-admin version: 2.5.0 pmm-agent version: 2.5.0 Agents: /agent_id/aeb42475-486c-4f48-a906-9546fc7859e8 mysql_slowlog_agent Running","title":"EXAMPLES"},{"location":"details/commands/pmm-agent.html","text":"pmm-agent - PMM Client agent NAME pmm-agent - The PMM Client daemon program SYNOPSIS pmm-agent [command] [options] DESCRIPTION pmm-agent, part of the PMM Client package, runs as a daemon process on all monitored hosts. COMMANDS pmm-agent run Run pmm-agent (default) pmm-agent setup [node-address] [node-type] [node-name] Configure local pmm-agent pmm-agent help [command] Show help (for command) and exit OPTIONS AND ENVIRONMENT Most options can be set via environment variables (shown in parentheses). Option Environment variable Description --server-password=SERVER-PASSWORD PMM_AGENT_SERVER_PASSWORD Password to connect to PMM Server. --server-username=SERVER-USERNAME PMM_AGENT_SERVER_USERNAME Username to connect to PMM Server. --server-address=host:port PMM_AGENT_SERVER_ADDRESS PMM Server address and port number. --server-insecure-tls PMM_AGENT_SERVER_INSECURE_TLS Skip PMM Server TLS certificate validation. --az=AZ PMM_AGENT_SETUP_AZ Node availability zone. --config-file=path_to/pmm-agent.yaml PMM_AGENT_CONFIG_FILE Configuration file path and name. --container-id=CONTAINER-ID PMM_AGENT_SETUP_CONTAINER_ID Container ID. --container-name=CONTAINER-NAME PMM_AGENT_SETUP_CONTAINER_NAME Container name. --debug PMM_AGENT_DEBUG Enable debug output. --distro=distro PMM_AGENT_SETUP_DISTRO Node OS distribution (default is auto-detected). --force PMM_AGENT_SETUP_FORCE Remove Node with that name and all dependent Services and Agents (if existing). --id=/agent_id/... PMM_AGENT_ID ID of this pmm-agent. --listen-address=LISTEN-ADDRESS PMM_AGENT_LISTEN_ADDRESS Agent local API address. --listen-port=LISTEN-PORT PMM_AGENT_LISTEN_PORT Agent local API port. --machine-id=machine-id PMM_AGENT_SETUP_MACHINE_ID Node machine ID (default is auto-detected). --metrics-mode=auto PMM_AGENT_SETUP_METRICS_MODE Metrics flow mode for agents node-exporter. Can be push (agent will push metrics), pull (server scrapes metrics from agent) or auto (chosen by server). --node-model=NODE-MODEL PMM_AGENT_SETUP_NODE_MODEL Node model. --paths-exporters_base=PATH PMM_AGENT_PATHS_EXPORTERS_BASE Base path for exporters to use. --paths-mongodb_exporter=PATH PMM_AGENT_PATHS_MONGODB_EXPORTER Path to mongodb_exporter . --paths-mysqld_exporter=PATH PMM_AGENT_PATHS_MYSQLD_EXPORTER Path to mysqld_exporter . --paths-node_exporter=PATH PMM_AGENT_PATHS_NODE_EXPORTER Path to node_exporter . --paths-postgres_exporter=PATH PMM_AGENT_PATHS_POSTGRES_EXPORTER Path to postgres_exporter . --paths-proxysql_exporter=PATH PMM_AGENT_PATHS_PROXYSQL_EXPORTER Path to proxysql_exporter . --paths-pt-summary=PATH PMM_AGENT_PATHS_PT_SUMMARY Path to pt-summary . --paths-tempdir=PATH PMM_AGENT_PATHS_TEMPDIR Temporary directory for exporters. --ports-max=PORTS-MAX PMM_AGENT_PORTS_MAX Highest allowed port number for listening sockets. --ports-min=PORTS-MIN PMM_AGENT_PORTS_MIN Lowest allowed port number for listening sockets. --region=REGION PMM_AGENT_SETUP_REGION Node region. --skip-registration PMM_AGENT_SETUP_SKIP_REGISTRATION Skip registration on PMM Server. --trace PMM_AGENT_TRACE Enable trace output (implies --debug ). -h , --help Show help (synonym for pmm-agent help ). --version Show application version, PMM version, timestamp, git commit hash and branch. LOGGING By default, pmm-agent sends messages to stderr and to the system log (syslogd or journald on Linux). To get a separate log file, edit the pmm-agent start-up script. systemd-based systems Script file: /usr/lib/systemd/system/pmm-agent.service Parameter: StandardError Default value: file:/var/log/pmm-agent.log Example: StandardError=file:/var/log/pmm-agent.log initd-based systems Script file: /etc/init.d/pmm-agent Parameter: pmm_log Default value: /var/log/pmm-agent.log Example: pmm_log=\"/var/log/pmm-agent.log\" If you change the default log file name, reflect the change in the log rotation rules file /etc/logrotate.d/pmm-agent-logrotate .","title":"pmm-agent - PMM Client agent"},{"location":"details/commands/pmm-agent.html#name","text":"pmm-agent - The PMM Client daemon program","title":"NAME"},{"location":"details/commands/pmm-agent.html#synopsis","text":"pmm-agent [command] [options]","title":"SYNOPSIS"},{"location":"details/commands/pmm-agent.html#description","text":"pmm-agent, part of the PMM Client package, runs as a daemon process on all monitored hosts.","title":"DESCRIPTION"},{"location":"details/commands/pmm-agent.html#commands","text":"pmm-agent run Run pmm-agent (default) pmm-agent setup [node-address] [node-type] [node-name] Configure local pmm-agent pmm-agent help [command] Show help (for command) and exit","title":"COMMANDS"},{"location":"details/commands/pmm-agent.html#options-and-environment","text":"Most options can be set via environment variables (shown in parentheses). Option Environment variable Description --server-password=SERVER-PASSWORD PMM_AGENT_SERVER_PASSWORD Password to connect to PMM Server. --server-username=SERVER-USERNAME PMM_AGENT_SERVER_USERNAME Username to connect to PMM Server. --server-address=host:port PMM_AGENT_SERVER_ADDRESS PMM Server address and port number. --server-insecure-tls PMM_AGENT_SERVER_INSECURE_TLS Skip PMM Server TLS certificate validation. --az=AZ PMM_AGENT_SETUP_AZ Node availability zone. --config-file=path_to/pmm-agent.yaml PMM_AGENT_CONFIG_FILE Configuration file path and name. --container-id=CONTAINER-ID PMM_AGENT_SETUP_CONTAINER_ID Container ID. --container-name=CONTAINER-NAME PMM_AGENT_SETUP_CONTAINER_NAME Container name. --debug PMM_AGENT_DEBUG Enable debug output. --distro=distro PMM_AGENT_SETUP_DISTRO Node OS distribution (default is auto-detected). --force PMM_AGENT_SETUP_FORCE Remove Node with that name and all dependent Services and Agents (if existing). --id=/agent_id/... PMM_AGENT_ID ID of this pmm-agent. --listen-address=LISTEN-ADDRESS PMM_AGENT_LISTEN_ADDRESS Agent local API address. --listen-port=LISTEN-PORT PMM_AGENT_LISTEN_PORT Agent local API port. --machine-id=machine-id PMM_AGENT_SETUP_MACHINE_ID Node machine ID (default is auto-detected). --metrics-mode=auto PMM_AGENT_SETUP_METRICS_MODE Metrics flow mode for agents node-exporter. Can be push (agent will push metrics), pull (server scrapes metrics from agent) or auto (chosen by server). --node-model=NODE-MODEL PMM_AGENT_SETUP_NODE_MODEL Node model. --paths-exporters_base=PATH PMM_AGENT_PATHS_EXPORTERS_BASE Base path for exporters to use. --paths-mongodb_exporter=PATH PMM_AGENT_PATHS_MONGODB_EXPORTER Path to mongodb_exporter . --paths-mysqld_exporter=PATH PMM_AGENT_PATHS_MYSQLD_EXPORTER Path to mysqld_exporter . --paths-node_exporter=PATH PMM_AGENT_PATHS_NODE_EXPORTER Path to node_exporter . --paths-postgres_exporter=PATH PMM_AGENT_PATHS_POSTGRES_EXPORTER Path to postgres_exporter . --paths-proxysql_exporter=PATH PMM_AGENT_PATHS_PROXYSQL_EXPORTER Path to proxysql_exporter . --paths-pt-summary=PATH PMM_AGENT_PATHS_PT_SUMMARY Path to pt-summary . --paths-tempdir=PATH PMM_AGENT_PATHS_TEMPDIR Temporary directory for exporters. --ports-max=PORTS-MAX PMM_AGENT_PORTS_MAX Highest allowed port number for listening sockets. --ports-min=PORTS-MIN PMM_AGENT_PORTS_MIN Lowest allowed port number for listening sockets. --region=REGION PMM_AGENT_SETUP_REGION Node region. --skip-registration PMM_AGENT_SETUP_SKIP_REGISTRATION Skip registration on PMM Server. --trace PMM_AGENT_TRACE Enable trace output (implies --debug ). -h , --help Show help (synonym for pmm-agent help ). --version Show application version, PMM version, timestamp, git commit hash and branch.","title":"OPTIONS AND ENVIRONMENT"},{"location":"details/commands/pmm-agent.html#logging","text":"By default, pmm-agent sends messages to stderr and to the system log (syslogd or journald on Linux). To get a separate log file, edit the pmm-agent start-up script. systemd-based systems Script file: /usr/lib/systemd/system/pmm-agent.service Parameter: StandardError Default value: file:/var/log/pmm-agent.log Example: StandardError=file:/var/log/pmm-agent.log initd-based systems Script file: /etc/init.d/pmm-agent Parameter: pmm_log Default value: /var/log/pmm-agent.log Example: pmm_log=\"/var/log/pmm-agent.log\" If you change the default log file name, reflect the change in the log rotation rules file /etc/logrotate.d/pmm-agent-logrotate .","title":"LOGGING"},{"location":"details/dashboards/index.html","text":"Dashboards Index Insight PMM OS Dashboards Prometheus Dashboards MySQL Dashboards MongoDB Dashboards PostgreSQL Dashboards ProxySQL Dashboards HA Dashboards Insight Advanced Data Exploration Home Dashboard Prometheus Exporter Status Prometheus Exporters Overview VictoriaMetrics VictoriaMetrics Agents Overview PMM PMM Inventory OS Dashboards CPU Utilization Details Disk Details Network Details Memory Details Node Temperature Details Nodes Compare Nodes Overview Node Summary NUMA Details Processes Details Prometheus Dashboards Prometheus Exporter Status Prometheus Exporters Overview MySQL Dashboards MySQL Amazon Aurora Details MySQL Command/Handler Counters Compare MySQL InnoDB Compression Details MySQL InnoDB Details MySQL MyISAM/Aria Details MySQL MyRocks Details MySQL Instance Summary MySQL Instances Compare MySQL Instances Overview MySQL Wait Event Analyses Details MySQL Performance Schema Details MySQL Query Response Time Details MySQL Replication Summary MySQL Group Replication Summary MySQL Table Details MySQL User Details MySQL TokuDB Details MongoDB Dashboards MongoDB Cluster Summary MongoDB Instance Summary MongoDB Instances Overview MongoDB Instances Compare MongoDB ReplSet Summary MongoDB InMemory Details MongoDB MMAPv1 Details MongoDB WiredTiger Details PostgreSQL Dashboards PostgreSQL Instances Overview PostgreSQL Instance Summary PostgreSQL Instances Compare ProxySQL Dashboards ProxySQL Instance Summary HA Dashboards PXC/Galera Node Summary PXC/Galera Cluster Summary PXC/Galera Nodes Compare","title":"Dashboards Index"},{"location":"details/dashboards/index.html#insight","text":"Advanced Data Exploration Home Dashboard Prometheus Exporter Status Prometheus Exporters Overview VictoriaMetrics VictoriaMetrics Agents Overview","title":"Insight"},{"location":"details/dashboards/index.html#pmm","text":"PMM Inventory","title":"PMM"},{"location":"details/dashboards/index.html#os-dashboards","text":"CPU Utilization Details Disk Details Network Details Memory Details Node Temperature Details Nodes Compare Nodes Overview Node Summary NUMA Details Processes Details","title":"OS Dashboards"},{"location":"details/dashboards/index.html#prometheus-dashboards","text":"Prometheus Exporter Status Prometheus Exporters Overview","title":"Prometheus Dashboards"},{"location":"details/dashboards/index.html#mysql-dashboards","text":"MySQL Amazon Aurora Details MySQL Command/Handler Counters Compare MySQL InnoDB Compression Details MySQL InnoDB Details MySQL MyISAM/Aria Details MySQL MyRocks Details MySQL Instance Summary MySQL Instances Compare MySQL Instances Overview MySQL Wait Event Analyses Details MySQL Performance Schema Details MySQL Query Response Time Details MySQL Replication Summary MySQL Group Replication Summary MySQL Table Details MySQL User Details MySQL TokuDB Details","title":"MySQL Dashboards"},{"location":"details/dashboards/index.html#mongodb-dashboards","text":"MongoDB Cluster Summary MongoDB Instance Summary MongoDB Instances Overview MongoDB Instances Compare MongoDB ReplSet Summary MongoDB InMemory Details MongoDB MMAPv1 Details MongoDB WiredTiger Details","title":"MongoDB Dashboards"},{"location":"details/dashboards/index.html#postgresql-dashboards","text":"PostgreSQL Instances Overview PostgreSQL Instance Summary PostgreSQL Instances Compare","title":"PostgreSQL Dashboards"},{"location":"details/dashboards/index.html#proxysql-dashboards","text":"ProxySQL Instance Summary","title":"ProxySQL Dashboards"},{"location":"details/dashboards/index.html#ha-dashboards","text":"PXC/Galera Node Summary PXC/Galera Cluster Summary PXC/Galera Nodes Compare","title":"HA Dashboards"},{"location":"details/dashboards/dashboard-advanced-data-exploration.html","text":"Advanced Data Exploration The Advanced Data Exploration dashboard provides detailed information about the progress of a single Prometheus metric across one or more hosts. View actual metric values (Gauge) A gauge is a metric that represents a single numerical value that can arbitrarily go up and down. Gauges are typically used for measured values like temperatures or current memory usage, but also \u201ccounts\u201d that can go up and down, like the number of running goroutines. View Metric Rate of Change (Counter) A counter is a cumulative metric that represents a single numerical value that only ever goes up. A counter is typically used to count requests served, tasks completed, errors occurred, etc. Counters should not be used to expose current counts of items whose number can also go down, e.g. the number of currently running goroutines. Use gauges for this use case. Metric Rates Shows the number of samples Per second stored for a given interval in the time series. Note This dashboard supports metrics related to NUMA. The names of all these metrics start with node_memory_numa .","title":"Advanced Data Exploration"},{"location":"details/dashboards/dashboard-advanced-data-exploration.html#view-actual-metric-values-gauge","text":"A gauge is a metric that represents a single numerical value that can arbitrarily go up and down. Gauges are typically used for measured values like temperatures or current memory usage, but also \u201ccounts\u201d that can go up and down, like the number of running goroutines.","title":"View actual metric values (Gauge)"},{"location":"details/dashboards/dashboard-advanced-data-exploration.html#view-metric-rate-of-change-counter","text":"A counter is a cumulative metric that represents a single numerical value that only ever goes up. A counter is typically used to count requests served, tasks completed, errors occurred, etc. Counters should not be used to expose current counts of items whose number can also go down, e.g. the number of currently running goroutines. Use gauges for this use case.","title":"View Metric Rate of Change (Counter)"},{"location":"details/dashboards/dashboard-advanced-data-exploration.html#metric-rates","text":"Shows the number of samples Per second stored for a given interval in the time series. Note This dashboard supports metrics related to NUMA. The names of all these metrics start with node_memory_numa .","title":"Metric Rates"},{"location":"details/dashboards/dashboard-cpu-utilization-details.html","text":"CPU Utilization Details Overall CPU Utilization The Overall CPU Utilization metric shows how much of the overall CPU time is used by the server. It has these components: Max Core Utilization No description System This component the proportion of time the CPUs spent inside the Linux kernel for operations like context switching, memory allocation and queue handling. User This component is the time spent in the user space. Normally, most of the MySQL CPU time is in user space. A high value of user time indicates a CPU bound workload. Softirq This component is the portion of time the CPU spent servicing software interrupts generated by the device drivers. A high value of softirq may indicates a poorly configured device. The network devices are generally the main source of high softirq values. Steal When multiple virtual machines share the same physical host, some virtual machines may be allowed to use more of their share of CPU and that CPU time is accounted as Steal by the virtual machine from which the time is taken. Iowait This component is the time the CPU spent waiting for disk IO requests to complete. A high value of iowait indicates a disk bound load. Nice No description In addition, sampling of the Max utilization of a single core is shown. Note This metric presents global values: while there may be a lot of unused CPU, a single core may be saturated. Look at the Max Core Utilization to see if any core is reaching close to 100%. Current CPU Threads Utilization This shows the total utilization of each CPU core along with the average utilization of all CPU cores. Watch for any core close to 100% utilization and investigate the root cause. CPU Threads Frequency No description Current CPU Cores Temperature No description Overall CPU Threads Utilization Details No description","title":"CPU Utilization Details"},{"location":"details/dashboards/dashboard-cpu-utilization-details.html#overall-cpu-utilization","text":"The Overall CPU Utilization metric shows how much of the overall CPU time is used by the server. It has these components: Max Core Utilization No description System This component the proportion of time the CPUs spent inside the Linux kernel for operations like context switching, memory allocation and queue handling. User This component is the time spent in the user space. Normally, most of the MySQL CPU time is in user space. A high value of user time indicates a CPU bound workload. Softirq This component is the portion of time the CPU spent servicing software interrupts generated by the device drivers. A high value of softirq may indicates a poorly configured device. The network devices are generally the main source of high softirq values. Steal When multiple virtual machines share the same physical host, some virtual machines may be allowed to use more of their share of CPU and that CPU time is accounted as Steal by the virtual machine from which the time is taken. Iowait This component is the time the CPU spent waiting for disk IO requests to complete. A high value of iowait indicates a disk bound load. Nice No description In addition, sampling of the Max utilization of a single core is shown. Note This metric presents global values: while there may be a lot of unused CPU, a single core may be saturated. Look at the Max Core Utilization to see if any core is reaching close to 100%.","title":"Overall CPU Utilization"},{"location":"details/dashboards/dashboard-cpu-utilization-details.html#current-cpu-threads-utilization","text":"This shows the total utilization of each CPU core along with the average utilization of all CPU cores. Watch for any core close to 100% utilization and investigate the root cause.","title":"Current CPU Threads Utilization"},{"location":"details/dashboards/dashboard-cpu-utilization-details.html#cpu-threads-frequency","text":"No description","title":"CPU Threads Frequency"},{"location":"details/dashboards/dashboard-cpu-utilization-details.html#current-cpu-cores-temperature","text":"No description","title":"Current CPU Cores Temperature"},{"location":"details/dashboards/dashboard-cpu-utilization-details.html#overall-cpu-threads-utilization-details","text":"No description","title":"Overall CPU Threads Utilization Details"},{"location":"details/dashboards/dashboard-disk-details.html","text":"Disk Details Mountpoint Usage Shows the percentage of disk space utilization for every mountpoint defined on the system. Having some of the mountpoints close to 100% space utilization is not good because of the risk of a \u201cdisk full\u201d error that can block one of the services or even cause a crash of the entire system. In cases where the mountpoint is close to 100% consider removing unused files or expanding the space allocated to the mountpoint. Mountpoint Shows information about the disk space usage of the specified mountpoint. Used is the amount of space used. Free is the amount of space not in use. Used+Free is the total disk space allocated to the mountpoint. Having Free close to 0 B is not good because of the risk of a \u201cdisk full\u201d error that can block one of the services or even cause a crash of the entire system. In cases where Free is close to 0 B consider removing unused files or expanding the space allocated to the mountpoint. Disk Latency Shows average latency for Reads and Writes IO Devices. Higher than typical latency for highly loaded storage indicates saturation (overload) and is frequent cause of performance problems. Higher than normal latency also can indicate internal storage problems. Disk Operations Shows amount of physical IOs (reads and writes) different devices are serving. Spikes in number of IOs served often corresponds to performance problems due to IO subsystem overload. Disk Bandwidth Shows volume of reads and writes the storage is handling. This can be better measure of IO capacity usage for network attached and SSD storage as it is often bandwidth limited. Amount of data being written to the disk can be used to estimate Flash storage life time. Disk Load Shows how much disk was loaded for reads or writes as average number of outstanding requests at different period of time. High disk load is a good measure of actual storage utilization. Different storage types handle load differently - some will show latency increases on low loads others can handle higher load with no problems. Disk IO Utilization Shows disk Utilization as percent of the time when there was at least one IO request in flight. It is designed to match utilization available in iostat tool. It is not very good measure of true IO Capacity Utilization. Consider looking at IO latency and Disk Load Graphs instead. Avg Disks Operations Merge Ratio Shows how effectively Operating System is able to merge logical IO requests into physical requests. This is a good measure of the IO locality which can be used for workload characterization. Disk IO Size Shows average size of a single disk operation.","title":"Disk Details"},{"location":"details/dashboards/dashboard-disk-details.html#mountpoint-usage","text":"Shows the percentage of disk space utilization for every mountpoint defined on the system. Having some of the mountpoints close to 100% space utilization is not good because of the risk of a \u201cdisk full\u201d error that can block one of the services or even cause a crash of the entire system. In cases where the mountpoint is close to 100% consider removing unused files or expanding the space allocated to the mountpoint.","title":"Mountpoint Usage"},{"location":"details/dashboards/dashboard-disk-details.html#mountpoint","text":"Shows information about the disk space usage of the specified mountpoint. Used is the amount of space used. Free is the amount of space not in use. Used+Free is the total disk space allocated to the mountpoint. Having Free close to 0 B is not good because of the risk of a \u201cdisk full\u201d error that can block one of the services or even cause a crash of the entire system. In cases where Free is close to 0 B consider removing unused files or expanding the space allocated to the mountpoint.","title":"Mountpoint"},{"location":"details/dashboards/dashboard-disk-details.html#disk-latency","text":"Shows average latency for Reads and Writes IO Devices. Higher than typical latency for highly loaded storage indicates saturation (overload) and is frequent cause of performance problems. Higher than normal latency also can indicate internal storage problems.","title":"Disk Latency"},{"location":"details/dashboards/dashboard-disk-details.html#disk-operations","text":"Shows amount of physical IOs (reads and writes) different devices are serving. Spikes in number of IOs served often corresponds to performance problems due to IO subsystem overload.","title":"Disk Operations"},{"location":"details/dashboards/dashboard-disk-details.html#disk-bandwidth","text":"Shows volume of reads and writes the storage is handling. This can be better measure of IO capacity usage for network attached and SSD storage as it is often bandwidth limited. Amount of data being written to the disk can be used to estimate Flash storage life time.","title":"Disk Bandwidth"},{"location":"details/dashboards/dashboard-disk-details.html#disk-load","text":"Shows how much disk was loaded for reads or writes as average number of outstanding requests at different period of time. High disk load is a good measure of actual storage utilization. Different storage types handle load differently - some will show latency increases on low loads others can handle higher load with no problems.","title":"Disk Load"},{"location":"details/dashboards/dashboard-disk-details.html#disk-io-utilization","text":"Shows disk Utilization as percent of the time when there was at least one IO request in flight. It is designed to match utilization available in iostat tool. It is not very good measure of true IO Capacity Utilization. Consider looking at IO latency and Disk Load Graphs instead.","title":"Disk IO Utilization"},{"location":"details/dashboards/dashboard-disk-details.html#avg-disks-operations-merge-ratio","text":"Shows how effectively Operating System is able to merge logical IO requests into physical requests. This is a good measure of the IO locality which can be used for workload characterization.","title":"Avg Disks Operations Merge Ratio"},{"location":"details/dashboards/dashboard-disk-details.html#disk-io-size","text":"Shows average size of a single disk operation.","title":"Disk IO Size"},{"location":"details/dashboards/dashboard-home.html","text":"Home Dashboard The Home Dashboard is a high-level overview of your environment, the starting page of the PMM portal from which you can open the tools of PMM, and browse to online resources. On the PMM home page, you can also find the version number and a button to update your PMM Server. General Information This section contains links to online resources, such as PMM documentation, releases notes, and blogs. Shared and Recently Used Dashboards This section is automatically updated to show the most recent dashboards that you worked with. It also contains the dashboards that you have bookmarked. Statistics This section shows the total number of hosts added to PMM and the total number of database instanced being monitored. This section also current the version number. Use the Check for Updates Manually button to see if you are using the most recent version of PMM. Environment Overview This section lists all added hosts along with essential information about their performance. For each host, you can find the current values of the following metrics: CPU Busy Memory Available Disk Reads Disk Writes Network IO DB Connections DB QPS Virtual CPUs RAM Host Uptime DB Uptime","title":"Home Dashboard"},{"location":"details/dashboards/dashboard-home.html#general-information","text":"This section contains links to online resources, such as PMM documentation, releases notes, and blogs.","title":"General Information"},{"location":"details/dashboards/dashboard-home.html#shared-and-recently-used-dashboards","text":"This section is automatically updated to show the most recent dashboards that you worked with. It also contains the dashboards that you have bookmarked.","title":"Shared and Recently Used Dashboards"},{"location":"details/dashboards/dashboard-home.html#statistics","text":"This section shows the total number of hosts added to PMM and the total number of database instanced being monitored. This section also current the version number. Use the Check for Updates Manually button to see if you are using the most recent version of PMM.","title":"Statistics"},{"location":"details/dashboards/dashboard-home.html#environment-overview","text":"This section lists all added hosts along with essential information about their performance. For each host, you can find the current values of the following metrics: CPU Busy Memory Available Disk Reads Disk Writes Network IO DB Connections DB QPS Virtual CPUs RAM Host Uptime DB Uptime","title":"Environment Overview"},{"location":"details/dashboards/dashboard-inventory.html","text":"PMM Inventory The Inventory dashboard is a high level overview of all objects PMM \u201cknows\u201d about. It contains three tabs ( services , agents , and nodes ) with lists of the correspondent objects and details about them, so that users are better able to understand which objects are registered against PMM Server. These objects are composing a hierarchy with Node at the top, then Service and Agents assigned to a Node. Nodes \u2013 Where the service and agents will run. Assigned a node_id , associated with a machine_id (from /etc/machine-id ). Few examples are bare metal, virtualized, container. Services \u2013 Individual service names and where they run, against which agents will be assigned. Each instance of a service gets a service_id value that is related to a node_id . Examples are MySQL, Amazon Aurora MySQL. This feature also allows to support multiple mysqld instances on a single node, with different service names, e.g. mysql1-3306, and mysql1-3307. Agents \u2013 Each binary (exporter, agent) running on a client will get an agent_id value. pmm-agent one is the top of the tree, assigned to a node_id node_exporter is assigned to pmm-agent agent_id mysqld_exporter & QAN MySQL Perfschema are assigned to a service_id. Examples are pmm-agent, node_exporter, mysqld_exporter, QAN MySQL Perfschema. Removing items from the inventory You can remove items from the inventory. Open Home Dashboard > PMM Inventory In the first column, select the items to be removed. Click Delete . The interface will ask you to confirm the operation:","title":"PMM Inventory"},{"location":"details/dashboards/dashboard-inventory.html#removing-items-from-the-inventory","text":"You can remove items from the inventory. Open Home Dashboard > PMM Inventory In the first column, select the items to be removed. Click Delete . The interface will ask you to confirm the operation:","title":"Removing items from the inventory"},{"location":"details/dashboards/dashboard-memory-details.html","text":"Memory Details Memory Usage No description","title":"Memory Details"},{"location":"details/dashboards/dashboard-memory-details.html#memory-usage","text":"No description","title":"Memory Usage"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html","text":"MongoDB Cluster Summary Current Connections Per Shard TCP connections (Incoming) in mongod processes. Total Connections Incoming connections to mongos nodes. Cursors Per Shard The Cursor is a MongoDB Collection of the document which is returned upon the find method execution. Mongos Cursors The Cursor is a MongoDB Collection of the document which is returned upon the find method execution. Operations Per Shard Ops/sec, classified by legacy wire protocol type (query, insert, update, delete, getmore). Total Mongos Operations Ops/sec, classified by legacy wire protocol type (query, insert, update, delete, getmore). Change Log Events Count, over last 10 minutes, of all types of config db changelog events. Oplog Range by Set Timespan \u2018window\u2019 between oldest and newest ops in the Oplog collection.","title":"MongoDB Cluster Summary"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#current-connections-per-shard","text":"TCP connections (Incoming) in mongod processes.","title":"Current Connections Per Shard"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#total-connections","text":"Incoming connections to mongos nodes.","title":"Total Connections"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#cursors-per-shard","text":"The Cursor is a MongoDB Collection of the document which is returned upon the find method execution.","title":"Cursors Per Shard"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#mongos-cursors","text":"The Cursor is a MongoDB Collection of the document which is returned upon the find method execution.","title":"Mongos Cursors"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#operations-per-shard","text":"Ops/sec, classified by legacy wire protocol type (query, insert, update, delete, getmore).","title":"Operations Per Shard"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#total-mongos-operations","text":"Ops/sec, classified by legacy wire protocol type (query, insert, update, delete, getmore).","title":"Total Mongos Operations"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#change-log-events","text":"Count, over last 10 minutes, of all types of config db changelog events.","title":"Change Log Events"},{"location":"details/dashboards/dashboard-mongodb-cluster-summary.html#oplog-range-by-set","text":"Timespan \u2018window\u2019 between oldest and newest ops in the Oplog collection.","title":"Oplog Range by Set"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html","text":"MongoDB InMemory Details InMemory Transactions WiredTiger internal transactions InMemory Capacity Configured max and current size of the WiredTiger cache. InMemory Sessions Internal WiredTiger storage engine cursors and sessions currently open. InMemory Pages Pages in the WiredTiger cache InMemory Concurrency Tickets A WT \u2018ticket\u2019 is assigned out for every operation running simultaneously in the WT storage engine. \u201cTickets available\u201d = hardcoded high value - \u201cTickets Out\u201d. Queued Operations Operations queued due to a lock Document Changes Mixed metrics: Docs per second inserted, updated, deleted or returned on any type of node (primary or secondary); + replicated write Ops/sec; + TTL deletes per second. InMemory Cache Eviction This panel shows the number of pages that have been evicted from the WiredTiger cache for the given time period. The InMemory storage engine only evicts modified pages which signals a compaction of the data and removal of the dirty pages. Scanned and Moved Objects This panel shows the number of objects (both data (scanned_objects) and index (scanned)) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine. Page Faults Unix or Window memory page faults. Not necessarily from mongodb.","title":"MongoDB InMemory Details"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#inmemory-transactions","text":"WiredTiger internal transactions","title":"InMemory Transactions"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#inmemory-capacity","text":"Configured max and current size of the WiredTiger cache.","title":"InMemory Capacity"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#inmemory-sessions","text":"Internal WiredTiger storage engine cursors and sessions currently open.","title":"InMemory Sessions"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#inmemory-pages","text":"Pages in the WiredTiger cache","title":"InMemory Pages"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#inmemory-concurrency-tickets","text":"A WT \u2018ticket\u2019 is assigned out for every operation running simultaneously in the WT storage engine. \u201cTickets available\u201d = hardcoded high value - \u201cTickets Out\u201d.","title":"InMemory Concurrency Tickets"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#queued-operations","text":"Operations queued due to a lock","title":"Queued Operations"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#document-changes","text":"Mixed metrics: Docs per second inserted, updated, deleted or returned on any type of node (primary or secondary); + replicated write Ops/sec; + TTL deletes per second.","title":"Document Changes"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#inmemory-cache-eviction","text":"This panel shows the number of pages that have been evicted from the WiredTiger cache for the given time period. The InMemory storage engine only evicts modified pages which signals a compaction of the data and removal of the dirty pages.","title":"InMemory Cache Eviction"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#scanned-and-moved-objects","text":"This panel shows the number of objects (both data (scanned_objects) and index (scanned)) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine.","title":"Scanned and Moved Objects"},{"location":"details/dashboards/dashboard-mongodb-inmemory-details.html#page-faults","text":"Unix or Window memory page faults. Not necessarily from mongodb.","title":"Page Faults"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html","text":"MongoDB Instance Summary Command Operations Ops or Replicated Ops/sec classified by legacy wire protocol type (query, insert, update, delete, getmore). And (from the internal TTL threads) the docs deletes/sec by TTL indexes. Latency Detail Average latency of operations (classified by read, write, or (other) command) Connections TCP connections (Incoming) Cursors Open cursors. Includes idle cursors. Document Operations Docs per second inserted, updated, deleted or returned. (N.b. not 1-to-1 with operation counts.) Queued Operations Operations queued due to a lock. Query Efficiency Ratio of Documents returned or Index entries scanned / full documents scanned Scanned and Moved Objects This panel shows the number of objects (both data (scanned_objects) and index (scanned)) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine. getLastError Write Time Legacy driver operation: Number of, and Sum of time spent, per second executing getLastError commands to confirm write concern. getLastError Write Operations Legacy driver operation: Number of getLastError commands that timed out trying to confirm write concern. Assert Events This panel shows the number of assert events per second on average over the given time period. In most cases assertions are trivial, but you would want to check your log files if this counter spikes or is consistently high. Page Faults Unix or Window memory page faults. Not necessarily from mongodb.","title":"MongoDB Instance Summary"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#command-operations","text":"Ops or Replicated Ops/sec classified by legacy wire protocol type (query, insert, update, delete, getmore). And (from the internal TTL threads) the docs deletes/sec by TTL indexes.","title":"Command Operations"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#latency-detail","text":"Average latency of operations (classified by read, write, or (other) command)","title":"Latency Detail"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#connections","text":"TCP connections (Incoming)","title":"Connections"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#cursors","text":"Open cursors. Includes idle cursors.","title":"Cursors"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#document-operations","text":"Docs per second inserted, updated, deleted or returned. (N.b. not 1-to-1 with operation counts.)","title":"Document Operations"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#queued-operations","text":"Operations queued due to a lock.","title":"Queued Operations"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#query-efficiency","text":"Ratio of Documents returned or Index entries scanned / full documents scanned","title":"Query Efficiency"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#scanned-and-moved-objects","text":"This panel shows the number of objects (both data (scanned_objects) and index (scanned)) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine.","title":"Scanned and Moved Objects"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#getlasterror-write-time","text":"Legacy driver operation: Number of, and Sum of time spent, per second executing getLastError commands to confirm write concern.","title":"getLastError Write Time"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#getlasterror-write-operations","text":"Legacy driver operation: Number of getLastError commands that timed out trying to confirm write concern.","title":"getLastError Write Operations"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#assert-events","text":"This panel shows the number of assert events per second on average over the given time period. In most cases assertions are trivial, but you would want to check your log files if this counter spikes or is consistently high.","title":"Assert Events"},{"location":"details/dashboards/dashboard-mongodb-instance-summary.html#page-faults","text":"Unix or Window memory page faults. Not necessarily from mongodb.","title":"Page Faults"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html","text":"MongoDB Instances Compare Connections No description Cursors No description Latency Average latency of operations (classified by read, write, or (other) command) Scan Ratios Ratio of index entries scanned or whole docs scanned / number of documents returned Index Filtering Effectiveness No description Requests Ops/sec (classified by (legacy) wire protocol request type) Document Operations Documents inserted/updated/deleted or returned per sec Queued Operations The number of operations that are currently queued and waiting for a lock Used Memory No description","title":"MongoDB Instances Compare"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#connections","text":"No description","title":"Connections"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#cursors","text":"No description","title":"Cursors"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#latency","text":"Average latency of operations (classified by read, write, or (other) command)","title":"Latency"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#scan-ratios","text":"Ratio of index entries scanned or whole docs scanned / number of documents returned","title":"Scan Ratios"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#index-filtering-effectiveness","text":"No description","title":"Index Filtering Effectiveness"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#requests","text":"Ops/sec (classified by (legacy) wire protocol request type)","title":"Requests"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#document-operations","text":"Documents inserted/updated/deleted or returned per sec","title":"Document Operations"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#queued-operations","text":"The number of operations that are currently queued and waiting for a lock","title":"Queued Operations"},{"location":"details/dashboards/dashboard-mongodb-instances-compare.html#used-memory","text":"No description","title":"Used Memory"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html","text":"MongoDB Instances Overview This dashboard provides basic information about MongoDB instances. Command Operations Shows how many times a command is executed per second on average during the selected interval. Look for peaks and drops and correlate them with other graphs. Connections Keep in mind the hard limit on the maximum number of connections set by your distribution. Anything over 5,000 should be a concern, because the application may not close connections correctly. Cursors Helps identify why connections are increasing. Shows active cursors compared to cursors being automatically killed after 10 minutes due to an application not closing the connection. Document Operations When used in combination with Command Operations , this graph can help identify write aplification . For example, when one insert or update command actually inserts or updates hundreds, thousands, or even millions of documents. Queued Operations Any number of queued operations for long periods of time is an indication of possible issues. Find the cause and fix it before requests get stuck in the queue. getLastError Write Time, getLastError Write Operations This is useful for write-heavy workloads to understand how long it takes to verify writes and how many concurrent writes are occurring. Asserts Asserts are not important by themselves, but you can correlate spikes with other graphs. Memory Faults Memory faults indicate that requests are processed from disk either because an index is missing or there is not enough memory for the data set. Consider increasing memory or sharding out.","title":"MongoDB Instances Overview"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#command-operations","text":"Shows how many times a command is executed per second on average during the selected interval. Look for peaks and drops and correlate them with other graphs.","title":"Command Operations"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#connections","text":"Keep in mind the hard limit on the maximum number of connections set by your distribution. Anything over 5,000 should be a concern, because the application may not close connections correctly.","title":"Connections"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#cursors","text":"Helps identify why connections are increasing. Shows active cursors compared to cursors being automatically killed after 10 minutes due to an application not closing the connection.","title":"Cursors"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#document-operations","text":"When used in combination with Command Operations , this graph can help identify write aplification . For example, when one insert or update command actually inserts or updates hundreds, thousands, or even millions of documents.","title":"Document Operations"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#queued-operations","text":"Any number of queued operations for long periods of time is an indication of possible issues. Find the cause and fix it before requests get stuck in the queue.","title":"Queued Operations"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#getlasterror-write-time-getlasterror-write-operations","text":"This is useful for write-heavy workloads to understand how long it takes to verify writes and how many concurrent writes are occurring.","title":"getLastError Write Time, getLastError Write Operations"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#asserts","text":"Asserts are not important by themselves, but you can correlate spikes with other graphs.","title":"Asserts"},{"location":"details/dashboards/dashboard-mongodb-instances-overview.html#memory-faults","text":"Memory faults indicate that requests are processed from disk either because an index is missing or there is not enough memory for the data set. Consider increasing memory or sharding out.","title":"Memory Faults"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html","text":"MongoDB MMAPv1 Details Document Activity Docs per second inserted, updated, deleted or returned. Also showing replicated write ops and internal TTL index deletes. MMAPv1 Lock Wait Time Time spent per second waiting to acquire locks. MMAPv1 Page Faults Unix or Window memory page faults. Not necessarily from mongodb. MMAPv1 Journal Write Activity MB processed through the journal in memory. MMAPv1 Journal Commit Activity MB committed to disk for the journal. MMAPv1 Background Flushing Time Average time in ms, over full uptime of mongod process, the MMAP background flushes have taken. Queued Operations Queue size of ops waiting to be submitted to storage engine layer. (N.b. see WiredTiger concurrency tickets for number of ops being processed simultaneously in storage engine layer.) Client Operations Ops and Replicated Ops/sec, classified by legacy wire protocol type (query, insert, update, delete, getmore). Scanned and Moved Objects This panel shows the number of objects (both data (scanned_objects) and index (scanned)) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine.","title":"MongoDB MMAPv1 Details"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#document-activity","text":"Docs per second inserted, updated, deleted or returned. Also showing replicated write ops and internal TTL index deletes.","title":"Document Activity"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#mmapv1-lock-wait-time","text":"Time spent per second waiting to acquire locks.","title":"MMAPv1 Lock Wait Time"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#mmapv1-page-faults","text":"Unix or Window memory page faults. Not necessarily from mongodb.","title":"MMAPv1 Page Faults"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#mmapv1-journal-write-activity","text":"MB processed through the journal in memory.","title":"MMAPv1 Journal Write Activity"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#mmapv1-journal-commit-activity","text":"MB committed to disk for the journal.","title":"MMAPv1 Journal Commit Activity"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#mmapv1-background-flushing-time","text":"Average time in ms, over full uptime of mongod process, the MMAP background flushes have taken.","title":"MMAPv1 Background Flushing Time"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#queued-operations","text":"Queue size of ops waiting to be submitted to storage engine layer. (N.b. see WiredTiger concurrency tickets for number of ops being processed simultaneously in storage engine layer.)","title":"Queued Operations"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#client-operations","text":"Ops and Replicated Ops/sec, classified by legacy wire protocol type (query, insert, update, delete, getmore).","title":"Client Operations"},{"location":"details/dashboards/dashboard-mongodb-mmapv1-details.html#scanned-and-moved-objects","text":"This panel shows the number of objects (both data (scanned_objects) and index (scanned)) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine.","title":"Scanned and Moved Objects"},{"location":"details/dashboards/dashboard-mongodb-replset-summary.html","text":"MongoDB ReplSet Summary Replication Lag MongoDB replication lag occurs when the secondary node cannot replicate data fast enough to keep up with the rate that data is being written to the primary node. It could be caused by something as simple as network latency, packet loss within your network, or a routing issue. Operations - by service name Operations are classified by legacy wire protocol type (insert, update, and delete only). Max Member Ping Time - by service name This metric can show a correlation with the replication lag value. Max Heartbeat Time Time span between now and last heartbeat from replicaset members. Elections Count of elections. Usually zero; 1 count by each healthy node will appear in each election. Happens when the primary role changes due to either normal maintenance or trouble events. Oplog Recovery Window - by service name Timespan \u2018window\u2019 between newest and the oldest op in the Oplog collection.","title":"MongoDB ReplSet Summary"},{"location":"details/dashboards/dashboard-mongodb-replset-summary.html#replication-lag","text":"MongoDB replication lag occurs when the secondary node cannot replicate data fast enough to keep up with the rate that data is being written to the primary node. It could be caused by something as simple as network latency, packet loss within your network, or a routing issue.","title":"Replication Lag"},{"location":"details/dashboards/dashboard-mongodb-replset-summary.html#operations-by-service-name","text":"Operations are classified by legacy wire protocol type (insert, update, and delete only).","title":"Operations - by service name"},{"location":"details/dashboards/dashboard-mongodb-replset-summary.html#max-member-ping-time-by-service-name","text":"This metric can show a correlation with the replication lag value.","title":"Max Member Ping Time - by service name"},{"location":"details/dashboards/dashboard-mongodb-replset-summary.html#max-heartbeat-time","text":"Time span between now and last heartbeat from replicaset members.","title":"Max Heartbeat Time"},{"location":"details/dashboards/dashboard-mongodb-replset-summary.html#elections","text":"Count of elections. Usually zero; 1 count by each healthy node will appear in each election. Happens when the primary role changes due to either normal maintenance or trouble events.","title":"Elections"},{"location":"details/dashboards/dashboard-mongodb-replset-summary.html#oplog-recovery-window-by-service-name","text":"Timespan \u2018window\u2019 between newest and the oldest op in the Oplog collection.","title":"Oplog Recovery Window - by service name"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html","text":"MongoDB WiredTiger Details WiredTiger Transactions WiredTiger internal transactions WiredTiger Cache Activity Data volume transfered per second between the WT cache and data files. Writes out always imply disk; Reads are often from OS filebuffer cache already in RAM, but disk if not. WiredTiger Block Activity Data volume handled by the WT block manager per second WiredTiger Sessions Internal WT storage engine cursors and sessions currently open WiredTiger Concurrency Tickets Available A WT \u2018ticket\u2019 is assigned out for every operation running simultaneously in the WT storage engine. \u201cAvailable\u201d = hardcoded high value - \u201cOut\u201d. Queued Operations Operations queued due to a lock. WiredTiger Checkpoint Time The time spent in WT checkpoint phase. Warning: This calculation averages the cyclical event (default: 1 min) execution to a per-second value. WiredTiger Cache Eviction Least-recently used pages being evicted due to WT cache becoming full. WiredTiger Cache Capacity Configured max and current size of the WT cache. WiredTiger Cache Pages WiredTiger Log Operations WT internal write-ahead log operations. WiredTiger Log Activity Data volume moved per second in WT internal write-ahead log. WiredTiger Log Records Number of records appended per second in WT internal log. Document Changes Mixed metrics: Docs per second inserted, updated, deleted or returned on any type of node (primary or secondary); + replicated write Ops/sec; + TTL deletes per second. Scanned and Moved Objects This panel shows the number of objects (both data (scanned_objects) and index (scanned)) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine. Page Faults Unix or Window memory page faults. Not necessarily from mongodb.","title":"MongoDB WiredTiger Details"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-transactions","text":"WiredTiger internal transactions","title":"WiredTiger Transactions"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-cache-activity","text":"Data volume transfered per second between the WT cache and data files. Writes out always imply disk; Reads are often from OS filebuffer cache already in RAM, but disk if not.","title":"WiredTiger Cache Activity"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-block-activity","text":"Data volume handled by the WT block manager per second","title":"WiredTiger Block Activity"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-sessions","text":"Internal WT storage engine cursors and sessions currently open","title":"WiredTiger Sessions"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-concurrency-tickets-available","text":"A WT \u2018ticket\u2019 is assigned out for every operation running simultaneously in the WT storage engine. \u201cAvailable\u201d = hardcoded high value - \u201cOut\u201d.","title":"WiredTiger Concurrency Tickets Available"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#queued-operations","text":"Operations queued due to a lock.","title":"Queued Operations"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-checkpoint-time","text":"The time spent in WT checkpoint phase. Warning: This calculation averages the cyclical event (default: 1 min) execution to a per-second value.","title":"WiredTiger Checkpoint Time"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-cache-eviction","text":"Least-recently used pages being evicted due to WT cache becoming full.","title":"WiredTiger Cache Eviction"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-cache-capacity","text":"Configured max and current size of the WT cache.","title":"WiredTiger Cache Capacity"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-cache-pages","text":"","title":"WiredTiger Cache Pages"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-log-operations","text":"WT internal write-ahead log operations.","title":"WiredTiger Log Operations"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-log-activity","text":"Data volume moved per second in WT internal write-ahead log.","title":"WiredTiger Log Activity"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#wiredtiger-log-records","text":"Number of records appended per second in WT internal log.","title":"WiredTiger Log Records"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#document-changes","text":"Mixed metrics: Docs per second inserted, updated, deleted or returned on any type of node (primary or secondary); + replicated write Ops/sec; + TTL deletes per second.","title":"Document Changes"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#scanned-and-moved-objects","text":"This panel shows the number of objects (both data (scanned_objects) and index (scanned)) as well as the number of documents that were moved to a new location due to the size of the document growing. Moved documents only apply to the MMAPv1 storage engine.","title":"Scanned and Moved Objects"},{"location":"details/dashboards/dashboard-mongodb-wiredtiger-details.html#page-faults","text":"Unix or Window memory page faults. Not necessarily from mongodb.","title":"Page Faults"},{"location":"details/dashboards/dashboard-mysql-amazon-aurora-details.html","text":"MySQL Amazon Aurora Details Amazon Aurora Transaction Commits This graph shows the number of Commits which Amazon Aurora engine performed as well as average commit latency. Graph Latency does not always correlate with the number of performed commits and can be quite high in certain situations. Number of Amazon Aurora Commits : The average number of commit operations per second. Amazon Aurora Commit avg Latency : The average amount of latency for commit operations Amazon Aurora Load This graph shows us what statements contribute most load on the system as well as what load corresponds to Amazon Aurora transaction commit. Write Transaction Commit Load : Load in Average Active Sessions per second for COMMIT operations UPDATE load : Load in Average Active Sessions per second for UPDATE queries SELECT load : Load in Average Active Sessions per second for SELECT queries DELETE load : Load in Average Active Sessions per second for DELETE queries INSERT load : Load in Average Active Sessions per second for INSERT queries An active session is a connection that has submitted work to the database engine and is waiting for a response from it. For example, if you submit an SQL query to the database engine, the database session is active while the database engine is processing that query. Aurora Memory Used This graph shows how much memory is used by Amazon Aurora lock manager as well as amount of memory used by Amazon Aurora to store Data Dictionary. Aurora Lock Manager Memory : the amount of memory used by the Lock Manager, the module responsible for handling row lock requests for concurrent transactions. Aurora Dictionary Memory : the amount of memory used by the Dictionary, the space that contains metadata used to keep track of database objects, such as tables and indexes. Amazon Aurora Statement Latency This graph shows average latency for the most important types of statements. Latency spikes are often indicative of the instance overload. DDL Latency: Average time to execute DDL queries DELETE Latency : Average time to execute DELETE queries UPDATE Latency : Average time to execute UPDATE queries SELECT Latency : Average time to execute SELECT queries INSERT Latency : Average time to execute INSERT queries Amazon Aurora Special Command Counters Amazon Aurora MySQL allows a number of commands which are not available in standard MySQL. This graph shows usage of such commands. Regular \u201cunit_test\u201d calls can be seen in default Amazon Aurora install, the rest will depend on your workload. show_volume_status : The number of executions per second of the command SHOW VOLUME STATUS. The SHOW VOLUME STATUS query returns two server status variables, Disks and Nodes. These variables represent the total number of logical blocks of data and storage nodes, respectively, for the DB cluster volume. awslambda : The number of AWS Lambda calls per second. AWS Lambda is an event-drive, server-less computing platform provided by AWS. It is a compute service that run codes in response to an event. You can run any kind of code from Aurora invoking Lambda from a stored procedure or a trigger. alter_system : The number of executions per second of the special query ALTER SYSTEM, that is a special query to simulate an instance crash, a disk failure, a disk congestion or a replica failure. It\u2019s a useful query for testing the system. Amazon Aurora Problems This graph shows different kinds of Internal Amazon Aurora MySQL Problems which general should be zero in normal operation. Anything non-zero is worth examining in greater depth.","title":"MySQL Amazon Aurora Details"},{"location":"details/dashboards/dashboard-mysql-amazon-aurora-details.html#amazon-aurora-transaction-commits","text":"This graph shows the number of Commits which Amazon Aurora engine performed as well as average commit latency. Graph Latency does not always correlate with the number of performed commits and can be quite high in certain situations. Number of Amazon Aurora Commits : The average number of commit operations per second. Amazon Aurora Commit avg Latency : The average amount of latency for commit operations","title":"Amazon Aurora Transaction Commits"},{"location":"details/dashboards/dashboard-mysql-amazon-aurora-details.html#amazon-aurora-load","text":"This graph shows us what statements contribute most load on the system as well as what load corresponds to Amazon Aurora transaction commit. Write Transaction Commit Load : Load in Average Active Sessions per second for COMMIT operations UPDATE load : Load in Average Active Sessions per second for UPDATE queries SELECT load : Load in Average Active Sessions per second for SELECT queries DELETE load : Load in Average Active Sessions per second for DELETE queries INSERT load : Load in Average Active Sessions per second for INSERT queries An active session is a connection that has submitted work to the database engine and is waiting for a response from it. For example, if you submit an SQL query to the database engine, the database session is active while the database engine is processing that query.","title":"Amazon Aurora Load"},{"location":"details/dashboards/dashboard-mysql-amazon-aurora-details.html#aurora-memory-used","text":"This graph shows how much memory is used by Amazon Aurora lock manager as well as amount of memory used by Amazon Aurora to store Data Dictionary. Aurora Lock Manager Memory : the amount of memory used by the Lock Manager, the module responsible for handling row lock requests for concurrent transactions. Aurora Dictionary Memory : the amount of memory used by the Dictionary, the space that contains metadata used to keep track of database objects, such as tables and indexes.","title":"Aurora Memory Used"},{"location":"details/dashboards/dashboard-mysql-amazon-aurora-details.html#amazon-aurora-statement-latency","text":"This graph shows average latency for the most important types of statements. Latency spikes are often indicative of the instance overload. DDL Latency: Average time to execute DDL queries DELETE Latency : Average time to execute DELETE queries UPDATE Latency : Average time to execute UPDATE queries SELECT Latency : Average time to execute SELECT queries INSERT Latency : Average time to execute INSERT queries","title":"Amazon Aurora Statement Latency"},{"location":"details/dashboards/dashboard-mysql-amazon-aurora-details.html#amazon-aurora-special-command-counters","text":"Amazon Aurora MySQL allows a number of commands which are not available in standard MySQL. This graph shows usage of such commands. Regular \u201cunit_test\u201d calls can be seen in default Amazon Aurora install, the rest will depend on your workload. show_volume_status : The number of executions per second of the command SHOW VOLUME STATUS. The SHOW VOLUME STATUS query returns two server status variables, Disks and Nodes. These variables represent the total number of logical blocks of data and storage nodes, respectively, for the DB cluster volume. awslambda : The number of AWS Lambda calls per second. AWS Lambda is an event-drive, server-less computing platform provided by AWS. It is a compute service that run codes in response to an event. You can run any kind of code from Aurora invoking Lambda from a stored procedure or a trigger. alter_system : The number of executions per second of the special query ALTER SYSTEM, that is a special query to simulate an instance crash, a disk failure, a disk congestion or a replica failure. It\u2019s a useful query for testing the system.","title":"Amazon Aurora Special Command Counters"},{"location":"details/dashboards/dashboard-mysql-amazon-aurora-details.html#amazon-aurora-problems","text":"This graph shows different kinds of Internal Amazon Aurora MySQL Problems which general should be zero in normal operation. Anything non-zero is worth examining in greater depth.","title":"Amazon Aurora Problems"},{"location":"details/dashboards/dashboard-mysql-command-handler-counters-compare.html","text":"MySQL Command/Handler Counters Compare This dashboard shows server status variables. On this dashboard, you may select multiple servers and compare their counters simultaneously. Server status variables appear in two sections: Commands and Handlers . Choose one or more variables in the Command and Handler fields in the top menu to select the variables which will appear in the COMMANDS or HANDLERS section for each host. Your comparison may include from one up to three hosts. By default or if no item is selected in the menu, PMM displays each command or handler respectively.","title":"MySQL Command/Handler Counters Compare"},{"location":"details/dashboards/dashboard-mysql-group-replication-summary.html","text":"MySQL Group Replication Summary Overview PRIMARY Service Group Replication Service States Replication Group Members Replication Lag Replication Delay Transport Time Transactions Transaction Details Applied Transactions Sent Transactions Checked Transactions Rolled Back Transactions Transactions Row Validating Transactions in the Queue for Checking Received Transactions Queue Conflicts Detected Conflicts","title":"MySQL Group Replication Summary"},{"location":"details/dashboards/dashboard-mysql-group-replication-summary.html#overview","text":"PRIMARY Service Group Replication Service States Replication Group Members Replication Lag Replication Delay Transport Time","title":"Overview"},{"location":"details/dashboards/dashboard-mysql-group-replication-summary.html#transactions","text":"Transaction Details Applied Transactions Sent Transactions Checked Transactions Rolled Back Transactions Transactions Row Validating Transactions in the Queue for Checking Received Transactions Queue","title":"Transactions"},{"location":"details/dashboards/dashboard-mysql-group-replication-summary.html#conflicts","text":"Detected Conflicts","title":"Conflicts"},{"location":"details/dashboards/dashboard-mysql-innodb-compression-details.html","text":"MySQL InnoDB Compression Details This dashboard helps you analyze the efficiency of InnoDB compression. Compression level and failure rate threshold InnoDB Compression Level The level of zlib compression to use for InnoDB compressed tables and indexes. InnoDB Compression Failure Threshold The compression failure rate threshold for a table. Compression Failure Rate Threshold The maximum percentage that can be reserved as free space within each compressed page, allowing room to reorganize the data and modification log within the page when a compressed table or index is updated and the data might be recompressed. Write Pages to the Redo Log Specifies whether images of re-compressed pages are written to the redo log. Re-compression may occur when changes are made to compressed data. Statistic of compression operations Compress Attempts Number of compression operations attempted. Pages are compressed whenever an empty page is created or the space for the uncompressed modification log runs out. Uncompressed Attempts Number of uncompression operations performed. Compressed InnoDB pages are uncompressed whenever compression fails, or the first time a compressed page is accessed in the buffer pool and the uncompressed page does not exist. CPU Core Usage CPU Core Usage for Compression Shows the time in seconds spent by InnoDB Compression operations. CPU Core Usage for Uncompression Shows the time in seconds spent by InnoDB Uncompression operations. Buffer Pool Total Total Used Pages Shows the total amount of used compressed pages into the InnoDB Buffer Pool split by page size. Total Free Pages Shows the total amount of free compressed pages into the InnoDB Buffer Pool split by page size.","title":"MySQL InnoDB Compression Details"},{"location":"details/dashboards/dashboard-mysql-innodb-compression-details.html#compression-level-and-failure-rate-threshold","text":"InnoDB Compression Level The level of zlib compression to use for InnoDB compressed tables and indexes. InnoDB Compression Failure Threshold The compression failure rate threshold for a table. Compression Failure Rate Threshold The maximum percentage that can be reserved as free space within each compressed page, allowing room to reorganize the data and modification log within the page when a compressed table or index is updated and the data might be recompressed. Write Pages to the Redo Log Specifies whether images of re-compressed pages are written to the redo log. Re-compression may occur when changes are made to compressed data.","title":"Compression level and failure rate threshold"},{"location":"details/dashboards/dashboard-mysql-innodb-compression-details.html#statistic-of-compression-operations","text":"Compress Attempts Number of compression operations attempted. Pages are compressed whenever an empty page is created or the space for the uncompressed modification log runs out. Uncompressed Attempts Number of uncompression operations performed. Compressed InnoDB pages are uncompressed whenever compression fails, or the first time a compressed page is accessed in the buffer pool and the uncompressed page does not exist.","title":"Statistic of compression operations"},{"location":"details/dashboards/dashboard-mysql-innodb-compression-details.html#cpu-core-usage","text":"CPU Core Usage for Compression Shows the time in seconds spent by InnoDB Compression operations. CPU Core Usage for Uncompression Shows the time in seconds spent by InnoDB Uncompression operations.","title":"CPU Core Usage"},{"location":"details/dashboards/dashboard-mysql-innodb-compression-details.html#buffer-pool-total","text":"Total Used Pages Shows the total amount of used compressed pages into the InnoDB Buffer Pool split by page size. Total Free Pages Shows the total amount of free compressed pages into the InnoDB Buffer Pool split by page size.","title":"Buffer Pool Total"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html","text":"MySQL InnoDB Details InnoDB Activity Writes (Rows) Writes (Transactions) Row Writes per Trx Rows Written Per Transactions which modify rows. This is better indicator of transaction write size than looking at all transactions which did not do any writes as well. Rows Read Per Trx Log Space per Trx Rollbacks Percent of Transaction Rollbacks (as portion of read-write transactions). BP Reqs Per Row Number of Buffer Pool requests per Row Access. High numbers here indicate going through long undo chains, deep trees and other inefficient data access. It can be less than zero due to several rows being read from single page. Log Fsync Per Trx Log Fsync Per Transaction. InnoDB Row Reads InnoDB Row Operations This graph allows you to see which operations occur and the number of rows affected per operation. A graph like Queries Per Second will give you an idea of queries, but one query could effect millions of rows. InnoDB Row Writes InnoDB Row Operations This graph allows you to see which operations occur and the number of rows affected per operation. A graph like Queries Per Second will give you an idea of queries, but one query could effect millions of rows. InnoDB Read-Only Transactions InnoDB Read-Write Transactions InnoDB Transactions Information (RW) The InnoDB Transactions Information graph shows details about the recent transactions. Transaction IDs Assigned represents the total number of transactions initiated by InnoDB. RW Transaction Commits are the number of transactions not read-only. Insert-Update Transactions Commits are transactions on the Undo entries. Non Locking RO Transaction Commits are transactions commit from select statement in auto-commit mode or transactions explicitly started with \u201cstart transaction read only\u201d. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client. Misc InnoDB Transactions Information Additional Innodb Transaction Information InnoDB Storage Summary Innodb Tables Current Number of Innodb Tables in database Data Buffer Pool Fit Buffer Pool Size as Portion of the Data Avg Row Size Amount of Data Per Row Index Size Per Row Index Size Per Row shows how much space we\u2019re using for indexes on per row basics InnoDB Data Summary Space Allocated Total Amount of Space Allocated. May not exactly match amount of space used on file system but provided great guidance. Space Used Space used in All Innodb Tables. Reported Allocated Space Less Free Space. Data Length Space Used by Data (Including Primary Key). Index Length Space Used by Secondary Indexes. Estimated Rows Estimated number of Rows in Innodb Storage Engine. It is not exact value and it can change abruptly as information is updated. Indexing Overhead How Much Indexes Take Compared to Data. Free Space Percent How Much Space is Free. Too high value wastes space on disk. Free Allocated Space not currently used by Data or Indexes. InnoDB File Per Table If Enabled, By Default every Table will have its own Tablespace represented as its own .idb file rather than all tables stored in single system tablespace. InnoDB Disk IO InnoDB Page Size Avg Data Read Rq Size Avg Data Write Rq Size Avg Log Write Rq Size Data Written Per Fsync Log Written Per Fsync Data Read Per Row Read Data Written Per Row Written Note: Due to difference in timing of Row Write and Data Write the value may be misleading on short intervals. InnoDB Data I/O InnoDB I/O Data Writes - The total number of InnoDB data writes. Data Reads - The total number of InnoDB data reads (OS file reads). Log Writes - The number of physical writes to the InnoDB redo log file. Data Fsyncs - The number of fsync() operations. The frequency of fsync() calls is influenced by the setting of the innodb_flush_method configuration option. InnoDB Data Bandwitdh InnoDB Log IO InnoDB I/O Data Writes - The total number of InnoDB data writes. Data Reads - The total number of InnoDB data reads (OS file reads). Log Writes - The number of physical writes to the InnoDB redo log file. Data Fsyncs - The number of fsync() operations. The frequency of fsync() calls is influenced by the setting of the innodb_flush_method configuration option. InnoDB FSyncs InnoDB Pending IO InnoDB Pending Fsyncs InnoDB Auto Extend Increment When Growing Innodb System Tablespace extend it by this size at the time. InnoDB Double Write Whether InnoDB Double Write Buffer is enabled. Doing so doubles amount of writes InnoDB has to do to storage but is required to avoid potential data corruption during the crash on most storage subsystems. InnoDB Fast Shutdown Fast Shutdown means InnoDB will not perform complete Undo Space and Change Buffer cleanup on shutdown, which is faster but may interfere with certain major upgrade operations. InnoDB Open Files Maximum Number of Files InnoDB is Allowed to use. InnoDB File Use Portion of Allowed InnoDB Open Files Use. InnoDB IO Objects InnoDB IO Targets Write Load Write Load Includes both Write and fsync (refered as misc). InnoDB Buffer Pool Buffer Pool Size InnoDB Buffer Pool Size InnoDB maintains a storage area called the buffer pool for caching data and indexes in memory. Knowing how the InnoDB buffer pool works, and taking advantage of it to keep frequently accessed data in memory, is one of the most important aspects of MySQL tuning. The goal is to keep the working set in memory. In most cases, this should be between 60%-90% of available memory on a dedicated database host, but depends on many factors. Buffer Pool Size of Total RAM InnoDB Buffer Pool Size % of Total RAM InnoDB maintains a storage area called the buffer pool for caching data and indexes in memory. Knowing how the InnoDB buffer pool works, and taking advantage of it to keep frequently accessed data in memory, is one of the most important aspects of MySQL tuning. The goal is to keep the working set in memory. In most cases, this should be between 60%-90% of available memory on a dedicated database host, but depends on many factors. NUMA Interleave Interleave Buffer Pool between NUMA zones to better support NUMA systems. Buffer Pool Activity Combined value of Buffer Pool Read and Write requests. BP Data Percent of Buffer Pool Occupied by Cached Data. BP Data Dirty Percent of Data which is Dirty. BP Miss Ratio How often buffer pool read requests have to do read from the disk. Keep this percent low for good performance. BP Write Buffering Number of Logical Writes to Buffer Pool Per logical Write. InnoDB Buffer Pool LRU Sub-Chain Churn Buffer Pool Chunk Size Size of the \u201cChunk\u201d for buffer pool allocation. Allocation of buffer pool will be rounded by this number. It also affects the performance impact of online buffer pool resize. Buffer Pool Instances Number of Buffer Pool Instances. Higher values allow to reduce contention but also increase overhead. Read Ahead IO Percent Percent of Reads Caused by Innodb Read Ahead. Read Ahead Wasted Percent of Pages Fetched by Read Ahead Evicted Without Access. Dump Buffer Pool on Shutdown Load Buffer Pool at Startup Portion of Buffer Pool To Dump/Load Larger Portion increases dump/load time but get more of original buffer pool content and hence may reduce warmup time. Include Buffer Pool in Core Dump Whenever to Include Buffer Pool in Crash Core Dumps. Doing so may dramatically increase core dump file slow down restart. Only makes a difference if core dumping on crash is enabled. InnoDB Old Blocks Percent of The Buffer Pool To be Reserved for \u201cOld Blocks\u201d - which has been touched repeatedly over period of time. InnoDB Old Blocks Time The Time which has to pass between multiple touches for the block for it to qualify as old block. InnoDB Random Read Ahead Is InnoDB Random ReadAhead Enabled. InnoDB Random Read Ahead The Threshold (in Pages) to trigger Linear Read Ahead. InnoDB Read IO Threads Number of Threads used to Schedule Reads. InnoDB Write IO Threads Number of Threads used to Schedule Writes. InnoDB Native AIO Enabled Whether Native Asynchronous IO is enabled. Strongly recommended for optimal performance. InnoDB Buffer Pool - Replacement Management LRU Scan Depth Innodb LRU Scan Depth This variable defines Innodb Free Page Target per buffer pool. When number of free pages falls below this number this number page cleaner will make required amount of pages free, flushing or evicting pages from the tail of LRU as needed. LRU Clean Page Searches When Page is being read (or created) the Page need to be allocated in Buffer Pool. Free List Miss Rate The most efficient way to get a clean page is to grab one from free list. However if no pages are available in Free List the LRU scan needs to be performed. LRU Get Free Loops If Free List was empty LRU Get Free Loop will be performed. It may perform LRU scan or may use some other heuristics and shortcuts to get free page. LRU Scans If Page could not be find any Free list and other shortcuts did not work, free page will be searched by scanning LRU chain which is not efficient. Pages Scanned in LRU Scans Pages Scanned Per Second while doing LRU scans. If this value is large (thousands) it means a lot of resources are wasted. Pages scanned per LRU Scan Number of pages scanned per LRU scan in Average. Large number of scans can consume a lot of resources and also introduce significant addition latency to queries. LRU Get Free Waits If Innodb could not find a free page in LRU list and had to sleep. Should be zero. InnoDB Checkpointing and Flushing Pages Flushed from Flush List Number of Pages Flushed from \u201cFlush List\u201d This combines Pages Flushed through Adaptive Flush and Background Flush. Page Flush Batches Executed Innodb Flush Cycle typically Runs on 1 second intervals. If it is too far off from this number it can indicate an issue. Pages Flushed Per Batch How many pages are flushed per Batch. Large Batches can \u201cchoke\u201d IO subsystem and starve other IO which needs to happen. Neighbor Flushing Enabled Neighbor Flushing is Optimized for Rotational Media and unless you\u2019re Running spinning disks you should disable it. InnoDB Checkpoint Age InnoDB Checkpoint Age The maximum checkpoint age is determined by the total length of all transaction log files ( innodb_log_file_size ). When the checkpoint age reaches the maximum checkpoint age, blocks are flushed syncronously. The rules of the thumb is to keep one hour of traffic in those logs and let the checkpointing perform its work as smooth as possible. If you don\u2019t do this, InnoDB will do synchronous flushing at the worst possible time, ie when you are busiest. Pages Flushed (Adaptive) Adaptive Flush Flushes pages from Flush List based on the need to advance Checkpoint (driven by Redo Generation Rate) and by maintaining number of dirty pages within set limit. Adaptive Flush Batches Executed Pages Per Batch (Adaptive) Pages Flushed Per Adaptive Batch. Neighbor Flushing To optimize IO for rotational Media InnoDB may flush neighbor pages. It can cause significant wasted IO for flash storage. Generally for flash you should run with innodb_flush_neighbors=0 but otherwise this shows how much IO you\u2019re wasting. Pages Flushed (LRU) Flushing from the tail of LRU list needs to happen when data does not fit in buffer pool in order to maintain free pages readily available for new data to be read. LRU Flush Batches Executed Pages Per Batch (LRU) Pages Flushed Per Neighbor. LSN Age Flush Batch Target Target for Pages to Flush due to LSN Age. Pages Flushed (Neighbor) Number of Neighbor pages flushed (If neighbor flushing is enabled) from Flush List and LRU List Combined. Neighbor Flush Batches Executed Pages Per Batch (Neighbor) Pages Flushed Per Neighbor. Sync Flush Waits If Innodb could not keep up with Checkpoint Flushing and had to trigger Sync flush. This should never happen. Pages Flushed (Background) Pages Flushed by Background Flush which is activated when server is considered to be idle. Background Flush Batches Executed Pages Per Batch (Background) Pages Flushed Per Background Batch. Redo Generation Rate Rate at which LSN (Redo) is Created. It may not match how much data is written to log files due to block size rounding. Innodb Flushing by Type Pages Evicted (LRU) This correspond to number of clean pages which were evicted (made free) from the tail of LRU buffer. Page Eviction Batches Pages Evicted per Batch Max Log Space Used Single Page Flushes Single Page flushes happen in rare case, then clean page could not be found in LRU list. It should be zero for most workloads. Single Page Flush Pages Scanned Pages Scanned Per Single Page Flush Innodb IO Capacity Estimated number of IOPS storage system can provide. Is used to scale background activities. Do not set it to actual storage capacity. Innodb IO Capacity Max InnoDB IO Capacity to use when falling behind and need to catch up with Flushing. InnoDB Logging Total Log Space Number of Innodb Log Files Multiplied by Their Size. Log Buffer Size InnoDB Log Buffer Size The size of buffer InnoDB uses for buffering writes to log files. At Transaction Commit What to do with Log file At Transaction Commit. Do nothing and wait for timeout to flush the data from Log Buffer, Flush it to OS Cache but not FSYNC or Flush only. Flush Transaction Log Every Every Specified Number of Seconds Flush Transaction Log. InnoDB Write Ahead Block Size This variable can be seen as minimum IO alignment InnoDB will use for Redo log file. High Values cause waste, low values can make IO less efficient. Log Write Amplification How much Writes to Log Are Amplified compared to how much Redo is Generated. Log Fsync Rate Redo Generated per Trx Amount of Redo Generated Per Write Transaction. This is a good indicator of transaction size. InnoDB Log File Usage Hourly InnoDB Log File Usage Hourly Along with the buffer pool size, innodb_log_file_size is the most important setting when we are working with InnoDB. This graph shows how much data was written to InnoDB\u2019s redo logs over each hour. When the InnoDB log files are full, InnoDB needs to flush the modified pages from memory to disk. The rules of the thumb is to keep one hour of traffic in those logs and let the checkpointing perform its work as smooth as possible. If you don\u2019t do this, InnoDB will do synchronous flushing at the worst possible time, ie when you are busiest. This graph can help guide you in setting the correct innodb_log_file_size . Log Padding Written Amount of Log Padding Written. InnoDB Log File Size InnoDB Log Files Number of InnoDB Redo Log Files. Log Bandwidth Redo Generation Rate Rate at which LSN (Redo) is Created. It may not match how much data is written to log files due to block size rounding. InnoDB Group Commit Batch Size The InnoDB Group Commit Batch Size graph shows how many bytes were written to the InnoDB log files per attempt to write. If many threads are committing at the same time, one of them will write the log entries of all the waiting threads and flush the file. Such process reduces the number of disk operations needed and enlarge the batch size. InnoDB Locking Lock Wait Timeout InnoDB Lock Wait Timeout How long to wait for row lock before timing out. InnoDB Deadlock Detection If Disabled Innodb Will not detect deadlocks but rely on timeouts. InnoDB Auto Increment Lock Mode Will Define How much locking will come from working with Auto Increment Columns. Rollback on Timeout Whenever to rollback all transaction on timeout or just last statement. Row Lock Blocking Percent of Active Sections which are blocked due to waiting on Innodb Row Locks. Row Writes per Trx Rows Written Per Transactions which modify rows. This is better indicator of transaction write size than looking at all transactions which did not do any writes as well. Rollbacks Percent of Transaction Rollbacks (as portion of read-write transactions). InnoDB Row Lock Wait Activity Innodb Row Lock Wait Time Innodb Row Lock Wait Load Average Number of Sessions blocked from proceeding due to waiting on row level lock. Innodb Row Locks Activity Innodb Table Lock Activity Current Locks InnoDB Undo Space and Purging Undo Tablespaces Max Undo Log Size Innodb Undo Log Truncate Purge Threads Max Purge Lag Maximum number of Unpurged Transactions, if this number exceeded delay will be introduced to incoming DDL statements. Max Purge Lag Delay Current Purge Delay The Delay Injected due to Purge Thread(s) unable to keep up with purge progress. Rollback Segments InnoDB Purge Activity The InnoDB Purge Performance graph shows metrics about the page purging process. The purge process removed the undo entries from the history list and cleanup the pages of the old versions of modified rows and effectively remove deleted rows. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client. Transactions and Undo Records InnoDB Undo Space Usage The InnoDB Undo Space Usage graph shows the amount of space used by the Undo segment. If the amount of space grows too much, look for long running transactions holding read views opened in the InnoDB status. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client. Transaction History InnoDB Purge Throttling Records Per Undo Log Page How Many Undo Operations Are Handled Per Each Undo Log Page. Purge Invoked How Frequently Purge Operation is Invoked. Ops Per Purge Home Many Purge Actions are done Per invocation. Undo Slots Used Number of Undo Slots Used. Max Transaction History Length Purge Batch Size Rseg Truncate Frequency InnoDB Page Operations InnoDB Page Splits and Merges The InnoDB Page Splits graph shows the InnoDB page maintenance activity related to splitting and merging pages. When an InnoDB page, other than the top most leaf page, has too much data to accept a row update or a row insert, it has to be split in two. Similarly, if an InnoDB page, after a row update or delete operation, ends up being less than half full, an attempt is made to merge the page with a neighbor page. If the resulting page size is larger than the InnoDB page size, the operation fails. If your workload causes a large number of page splits, try lowering the innodb_fill_factor variable (5.7+). Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client. Page Merge Success Ratio InnoDB Page Reorg Attempts The InnoDB Page Reorgs graph shows information about the page reorganization operations. When a page receives an update or an insert that affect the offset of other rows in the page, a reorganization is needed. If the reorganization process finds out there is not enough room in the page, the page will be split. Page reorganization can only fail for compressed pages. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client. InnoDB Page Reorgs Failures The InnoDB Page Reorgs graph shows information about the page reorganization operations. When a page receives an update or an insert that affect the offset of other rows in the page, a reorganization is needed. If the reorganization process finds out there is not enough room in the page, the page will be split. Page reorganization can only fail for compressed pages. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client. InnoDB Fill Factor The portion of the page to fill then doing sorted Index Build. Lowering this value will worsen space utilization but will reduce need to split pages when new data is inserted in the index. InnoDB Adaptive Hash Index Adaptive Hash Index Enabled Adaptive Hash Index Helps to Optimize Index Lookups but can be severe hotspot for some workloads. Adaptive Hash Index Partitions How many Partitions Used for Adaptive Hash Index (to reduce contention). Percent of Pages Hashed Number of Pages Added to AHI vs Number of Pages Added to Buffer Pool. AHI Miss Ratio Percent of Searches which could not be resolved through AHI. Rows Added Per Page Number of Rows \u201cHashed\u201d Per Each Page which needs to be added to AHI. AHI ROI How Many Successful Searches using AHI are performed per each row maintenance operation. InnoDB AHI Usage The InnoDB AHI Usage graph shows the search operations on the InnoDB adaptive hash index and its efficiency. The adaptive hash index is a search hash designed to speed access to InnoDB pages in memory. If the Hit Ratio is small, the working data set is larger than the buffer pool, the AHI should likely be disabled. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client. InnoDB AHI Miss Ratio InnoDB AHI Churn - Rows InnoDB AHI Churn - Pages InnoDB Change Buffer Change Buffer Max Size The Maximum Size of Change Buffer (as Percent of Buffer Pool Size). Change Buffer Max Size The Maximum Size of Change Buffer (Bytes). InnoDB Change Buffer Merge Load Number of Average of Active Merge Buffer Operations in Process. InnoDB Contention InnoDB Thread Concurrency If Enabled limits number of Threads allowed inside Innodb Kernel at the same time. InnoDB Commit Concurrency If Enabled limits number of Threads allowed inside InnoDB Kernel at the same time during Commit Stage. InnoDB Thread Sleep Delay The Time the thread will Sleep before Re-Entering Innodb Kernel if high contention. InnoDB Adaptive Max Sleep Delay If Set to Non-Zero Value InnoDB Thread Sleep Delay will be adjusted automatically depending on the load up to the value specified by this variable. InnoDB Concurrency Tickets Number of low level operations InnoDB can do after it entered InnoDB kernel before it is forced to exit and yield to another thread waiting. InnoDB Spin Wait Delay InnoDB Spin Wait Pause Multiplier InnoDB Sync Spin Loops InnoDB Contention - OS Waits The InnoDB Contention - OS Waits graph shows the number of time an OS wait operation was required while waiting to get the lock. This happens once the spin rounds are exhausted. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client. InnoDB Contention - Spin Rounds The InnoDB Contention - Spin Rounds graph shows the number of spin rounds executed in order to get a lock. A spin round is a fast retry to get the lock in a loop. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client. InnoDB Misc InnoDB Main Thread Utilization The InnoDB Main Thread Utilization graph shows the portion of time the InnoDB main thread spent at various task. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client. InnoDB Activity The InnoDB Activity graph shows a measure of the activity of the InnoDB threads. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client. InnoDB Dedicated Server InnoDB automatically optimized for Dedicated Server Environment (auto scaling cache and some other variables). InnoDB Sort Buffer Size This Buffer is used for Building InnoDB Indexes using Sort algorithm. InnoDB Stats Auto Recalc Update Stats when Metadata Queried Refresh InnoDB Statistics when meta-data queries by SHOW TABLE STATUS or INFORMATION_SCHEMA queries. If Enabled can cause severe performance issues. Index Condition Pushdown (ICP) Index Condition Pushdown (ICP) is an optimization for the case where MySQL retrieves rows from a table using an index. Without ICP, the storage engine traverses the index to locate rows in the base table and returns them to the MySQL server which evaluates the WHERE condition for the rows. With ICP enabled, and if parts of the WHERE condition can be evaluated by using only columns from the index, the MySQL server pushes this part of the WHERE condition down to the storage engine. The storage engine then evaluates the pushed index condition by using the index entry and only if this is satisfied is the row read from the table. ICP can reduce the number of times the storage engine must access the base table and the number of times the MySQL server must access the storage engine. InnoDB Persistent Statistics InnoDB Persistent Sample Pages Number of Pages To Sample if Persistent Statistics are Enabled. InnoDB Transient Sample Pages Number of Pages To Sample if Persistent Statistics are Disabled. InnoDB Online Operations (MariaDB) InnoDB Defragmentation The InnoDB Defragmentation graph shows the status information related to the InnoDB online defragmentation feature of MariaDB for the optimize table command. To enable this feature, the variable innodb-defragment must be set to 1 in the configuration file. Note: Currently available only on a MariaDB server. InnoDB Online DDL The InnoDB Online DDL graph shows the state of the online DDL (alter table) operations in InnoDB. The progress metric is estimate of the percentage of the rows processed by the online DDL. Note: Currently available only on a MariaDB server. MySQL Summary MySQL Uptime MySQL Uptime The amount of time since the last restart of the MySQL server process. Current QPS Current QPS Based on the queries reported by MySQL\u2019s SHOW STATUS command, it is the number of statements executed by the server within the last second. This variable includes statements executed within stored programs, unlike the Questions variable. It does not count COM_PING or COM_STATISTICS commands. File Handlers Used Table Open Cache Miss Ratio Table Open Cache Size Table Definition Cache Size MySQL Connections Max Connections Max Connections is the maximum permitted number of simultaneous client connections. By default, this is 151. Increasing this value increases the number of file descriptors that mysqld requires. If the required number of descriptors are not available, the server reduces the value of Max Connections. mysqld actually permits Max Connections + 1 clients to connect. The extra connection is reserved for use by accounts that have the SUPER privilege, such as root. Max Used Connections is the maximum number of connections that have been in use simultaneously since the server started. Connections is the number of connection attempts (successful or not) to the MySQL server. MySQL Client Thread Activity MySQL Active Threads Threads Connected is the number of open connections, while Threads Running is the number of threads not sleeping. MySQL Handlers MySQL Handlers Handler statistics are internal statistics on how MySQL is selecting, updating, inserting, and modifying rows, tables, and indexes. This is in fact the layer between the Storage Engine and MySQL. read_rnd_next is incremented when the server performs a full table scan and this is a counter you don\u2019t really want to see with a high value. read_key is incremented when a read is done with an index. read_next is incremented when the storage engine is asked to \u2018read the next index entry\u2019. A high value means a lot of index scans are being done. Top Command Counters Top Command Counters The Com_ statement counter variables indicate the number of times each xxx statement has been executed. There is one status variable for each type of statement. For example, Com_delete and Com_update count DELETE and UPDATE statements, respectively. Com_delete_multi and Com_update_multi are similar but apply to DELETE and UPDATE statements that use multiple-table syntax. MySQL Network Traffic MySQL Network Traffic Here we can see how much network traffic is generated by MySQL. Outbound is network traffic sent from MySQL and Inbound is network traffic MySQL has received. Node Summary System Uptime The parameter shows how long a system has been \u201cup\u201d and running without a shut down or restart. Load Average The system load is a measurement of the computational work the system is performing. Each running process either using or waiting for CPU resources adds 1 to the load. RAM RAM (Random Access Memory) is the hardware in a computing device where the operating system, application programs and data in current use are kept so they can be quickly reached by the device\u2019s processor. Memory Available Percent of Memory Available Note: on Modern Linux Kernels amount of Memory Available for application is not the same as Free+Cached+Buffers. Virtual Memory RAM + SWAP Disk Space Sum of disk space on all partitions. Note it can be significantly over-reported in some installations. Min Space Available Lowest percent of the disk space available. CPU Usage The CPU time is measured in clock ticks or seconds. It is useful to measure CPU time as a percentage of the CPU\u2019s capacity, which is called the CPU usage. CPU Saturation and Max Core Usage When a system is running with maximum CPU utilization, the transmitting and receiving threads must all share the available CPU. This will cause data to be queued more frequently to cope with the lack of CPU. CPU Saturation may be measured as the length of a wait queue, or the time spent waiting on the queue. Disk I/O and Swap Activity Disk I/O includes read or write or input/output operations involving a physical disk. It is the speed with which the data transfer takes place between the hard disk drive and RAM. Swap Activity is memory management that involves swapping sections of memory to and from physical storage. Network Traffic Network traffic refers to the amount of data moving across a network at a given point in time.","title":"MySQL InnoDB Details"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-activity","text":"","title":"InnoDB Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#writes-rows","text":"","title":"Writes (Rows)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#writes-transactions","text":"","title":"Writes (Transactions)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#row-writes-per-trx","text":"Rows Written Per Transactions which modify rows. This is better indicator of transaction write size than looking at all transactions which did not do any writes as well.","title":"Row Writes per Trx"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#rows-read-per-trx","text":"","title":"Rows Read Per Trx"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-space-per-trx","text":"","title":"Log Space per Trx"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#rollbacks","text":"Percent of Transaction Rollbacks (as portion of read-write transactions).","title":"Rollbacks"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#bp-reqs-per-row","text":"Number of Buffer Pool requests per Row Access. High numbers here indicate going through long undo chains, deep trees and other inefficient data access. It can be less than zero due to several rows being read from single page.","title":"BP Reqs Per Row"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-fsync-per-trx","text":"Log Fsync Per Transaction.","title":"Log Fsync Per Trx"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-row-reads","text":"InnoDB Row Operations This graph allows you to see which operations occur and the number of rows affected per operation. A graph like Queries Per Second will give you an idea of queries, but one query could effect millions of rows.","title":"InnoDB Row Reads"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-row-writes","text":"InnoDB Row Operations This graph allows you to see which operations occur and the number of rows affected per operation. A graph like Queries Per Second will give you an idea of queries, but one query could effect millions of rows.","title":"InnoDB Row Writes"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-read-only-transactions","text":"","title":"InnoDB Read-Only Transactions"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-read-write-transactions","text":"","title":"InnoDB Read-Write Transactions"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-transactions-information-rw","text":"The InnoDB Transactions Information graph shows details about the recent transactions. Transaction IDs Assigned represents the total number of transactions initiated by InnoDB. RW Transaction Commits are the number of transactions not read-only. Insert-Update Transactions Commits are transactions on the Undo entries. Non Locking RO Transaction Commits are transactions commit from select statement in auto-commit mode or transactions explicitly started with \u201cstart transaction read only\u201d. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client.","title":"InnoDB Transactions Information (RW)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#misc-innodb-transactions-information","text":"Additional Innodb Transaction Information","title":"Misc InnoDB Transactions Information"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-storage-summary","text":"","title":"InnoDB Storage Summary"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-tables","text":"Current Number of Innodb Tables in database","title":"Innodb Tables"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#data-buffer-pool-fit","text":"Buffer Pool Size as Portion of the Data","title":"Data Buffer Pool Fit"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#avg-row-size","text":"Amount of Data Per Row","title":"Avg Row Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#index-size-per-row","text":"Index Size Per Row shows how much space we\u2019re using for indexes on per row basics","title":"Index Size Per Row"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-data-summary","text":"","title":"InnoDB Data Summary"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#space-allocated","text":"Total Amount of Space Allocated. May not exactly match amount of space used on file system but provided great guidance.","title":"Space Allocated"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#space-used","text":"Space used in All Innodb Tables. Reported Allocated Space Less Free Space.","title":"Space Used"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#data-length","text":"Space Used by Data (Including Primary Key).","title":"Data Length"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#index-length","text":"Space Used by Secondary Indexes.","title":"Index Length"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#estimated-rows","text":"Estimated number of Rows in Innodb Storage Engine. It is not exact value and it can change abruptly as information is updated.","title":"Estimated Rows"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#indexing-overhead","text":"How Much Indexes Take Compared to Data.","title":"Indexing Overhead"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#free-space-percent","text":"How Much Space is Free. Too high value wastes space on disk.","title":"Free Space Percent"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#free","text":"Allocated Space not currently used by Data or Indexes.","title":"Free"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-file-per-table","text":"If Enabled, By Default every Table will have its own Tablespace represented as its own .idb file rather than all tables stored in single system tablespace.","title":"InnoDB File Per Table"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-disk-io","text":"","title":"InnoDB Disk IO"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-page-size","text":"","title":"InnoDB Page Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#avg-data-read-rq-size","text":"","title":"Avg Data Read Rq Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#avg-data-write-rq-size","text":"","title":"Avg Data Write Rq Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#avg-log-write-rq-size","text":"","title":"Avg Log Write Rq Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#data-written-per-fsync","text":"","title":"Data Written Per Fsync"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-written-per-fsync","text":"","title":"Log Written Per Fsync"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#data-read-per-row-read","text":"","title":"Data Read Per Row Read"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#data-written-per-row-written","text":"Note: Due to difference in timing of Row Write and Data Write the value may be misleading on short intervals.","title":"Data Written Per Row Written"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-data-io","text":"InnoDB I/O Data Writes - The total number of InnoDB data writes. Data Reads - The total number of InnoDB data reads (OS file reads). Log Writes - The number of physical writes to the InnoDB redo log file. Data Fsyncs - The number of fsync() operations. The frequency of fsync() calls is influenced by the setting of the innodb_flush_method configuration option.","title":"InnoDB Data I/O"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-data-bandwitdh","text":"","title":"InnoDB Data Bandwitdh"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-log-io","text":"InnoDB I/O Data Writes - The total number of InnoDB data writes. Data Reads - The total number of InnoDB data reads (OS file reads). Log Writes - The number of physical writes to the InnoDB redo log file. Data Fsyncs - The number of fsync() operations. The frequency of fsync() calls is influenced by the setting of the innodb_flush_method configuration option.","title":"InnoDB Log IO"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-fsyncs","text":"","title":"InnoDB FSyncs"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-pending-io","text":"","title":"InnoDB Pending IO"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-pending-fsyncs","text":"","title":"InnoDB Pending Fsyncs"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-auto-extend-increment","text":"When Growing Innodb System Tablespace extend it by this size at the time.","title":"InnoDB Auto Extend Increment"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-double-write","text":"Whether InnoDB Double Write Buffer is enabled. Doing so doubles amount of writes InnoDB has to do to storage but is required to avoid potential data corruption during the crash on most storage subsystems.","title":"InnoDB Double Write"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-fast-shutdown","text":"Fast Shutdown means InnoDB will not perform complete Undo Space and Change Buffer cleanup on shutdown, which is faster but may interfere with certain major upgrade operations.","title":"InnoDB Fast Shutdown"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-open-files","text":"Maximum Number of Files InnoDB is Allowed to use.","title":"InnoDB Open Files"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-file-use","text":"Portion of Allowed InnoDB Open Files Use.","title":"InnoDB File Use"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-io-objects","text":"","title":"InnoDB IO Objects"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-io-targets-write-load","text":"Write Load Includes both Write and fsync (refered as misc).","title":"InnoDB IO Targets Write Load"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-buffer-pool","text":"","title":"InnoDB Buffer Pool"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#buffer-pool-size","text":"InnoDB Buffer Pool Size InnoDB maintains a storage area called the buffer pool for caching data and indexes in memory. Knowing how the InnoDB buffer pool works, and taking advantage of it to keep frequently accessed data in memory, is one of the most important aspects of MySQL tuning. The goal is to keep the working set in memory. In most cases, this should be between 60%-90% of available memory on a dedicated database host, but depends on many factors.","title":"Buffer Pool Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#buffer-pool-size-of-total-ram","text":"InnoDB Buffer Pool Size % of Total RAM InnoDB maintains a storage area called the buffer pool for caching data and indexes in memory. Knowing how the InnoDB buffer pool works, and taking advantage of it to keep frequently accessed data in memory, is one of the most important aspects of MySQL tuning. The goal is to keep the working set in memory. In most cases, this should be between 60%-90% of available memory on a dedicated database host, but depends on many factors.","title":"Buffer Pool Size of Total RAM"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#numa-interleave","text":"Interleave Buffer Pool between NUMA zones to better support NUMA systems.","title":"NUMA Interleave"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#buffer-pool-activity","text":"Combined value of Buffer Pool Read and Write requests.","title":"Buffer Pool Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#bp-data","text":"Percent of Buffer Pool Occupied by Cached Data.","title":"BP Data"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#bp-data-dirty","text":"Percent of Data which is Dirty.","title":"BP Data Dirty"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#bp-miss-ratio","text":"How often buffer pool read requests have to do read from the disk. Keep this percent low for good performance.","title":"BP Miss Ratio"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#bp-write-buffering","text":"Number of Logical Writes to Buffer Pool Per logical Write.","title":"BP Write Buffering"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-buffer-pool-lru-sub-chain-churn","text":"","title":"InnoDB Buffer Pool LRU Sub-Chain Churn"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#buffer-pool-chunk-size","text":"Size of the \u201cChunk\u201d for buffer pool allocation. Allocation of buffer pool will be rounded by this number. It also affects the performance impact of online buffer pool resize.","title":"Buffer Pool Chunk Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#buffer-pool-instances","text":"Number of Buffer Pool Instances. Higher values allow to reduce contention but also increase overhead.","title":"Buffer Pool Instances"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#read-ahead-io-percent","text":"Percent of Reads Caused by Innodb Read Ahead.","title":"Read Ahead IO Percent"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#read-ahead-wasted","text":"Percent of Pages Fetched by Read Ahead Evicted Without Access.","title":"Read Ahead Wasted"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#dump-buffer-pool-on-shutdown","text":"","title":"Dump Buffer Pool on Shutdown"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#load-buffer-pool-at-startup","text":"","title":"Load Buffer Pool at Startup"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#portion-of-buffer-pool-to-dumpload","text":"Larger Portion increases dump/load time but get more of original buffer pool content and hence may reduce warmup time.","title":"Portion of Buffer Pool To Dump/Load"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#include-buffer-pool-in-core-dump","text":"Whenever to Include Buffer Pool in Crash Core Dumps. Doing so may dramatically increase core dump file slow down restart. Only makes a difference if core dumping on crash is enabled.","title":"Include Buffer Pool in Core Dump"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-old-blocks","text":"Percent of The Buffer Pool To be Reserved for \u201cOld Blocks\u201d - which has been touched repeatedly over period of time.","title":"InnoDB Old Blocks"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-old-blocks-time","text":"The Time which has to pass between multiple touches for the block for it to qualify as old block.","title":"InnoDB Old Blocks Time"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-random-read-ahead","text":"Is InnoDB Random ReadAhead Enabled.","title":"InnoDB Random Read Ahead"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-random-read-ahead_1","text":"The Threshold (in Pages) to trigger Linear Read Ahead.","title":"InnoDB Random Read Ahead"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-read-io-threads","text":"Number of Threads used to Schedule Reads.","title":"InnoDB Read IO Threads"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-write-io-threads","text":"Number of Threads used to Schedule Writes.","title":"InnoDB Write IO Threads"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-native-aio-enabled","text":"Whether Native Asynchronous IO is enabled. Strongly recommended for optimal performance.","title":"InnoDB Native AIO Enabled"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-buffer-pool-replacement-management","text":"","title":"InnoDB Buffer Pool - Replacement Management"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lru-scan-depth","text":"Innodb LRU Scan Depth This variable defines Innodb Free Page Target per buffer pool. When number of free pages falls below this number this number page cleaner will make required amount of pages free, flushing or evicting pages from the tail of LRU as needed.","title":"LRU Scan Depth"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lru-clean-page-searches","text":"When Page is being read (or created) the Page need to be allocated in Buffer Pool.","title":"LRU Clean Page Searches"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#free-list-miss-rate","text":"The most efficient way to get a clean page is to grab one from free list. However if no pages are available in Free List the LRU scan needs to be performed.","title":"Free List Miss Rate"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lru-get-free-loops","text":"If Free List was empty LRU Get Free Loop will be performed. It may perform LRU scan or may use some other heuristics and shortcuts to get free page.","title":"LRU Get Free Loops"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lru-scans","text":"If Page could not be find any Free list and other shortcuts did not work, free page will be searched by scanning LRU chain which is not efficient.","title":"LRU Scans"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-scanned-in-lru-scans","text":"Pages Scanned Per Second while doing LRU scans. If this value is large (thousands) it means a lot of resources are wasted.","title":"Pages Scanned in LRU Scans"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-scanned-per-lru-scan","text":"Number of pages scanned per LRU scan in Average. Large number of scans can consume a lot of resources and also introduce significant addition latency to queries.","title":"Pages scanned per LRU Scan"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lru-get-free-waits","text":"If Innodb could not find a free page in LRU list and had to sleep. Should be zero.","title":"LRU Get Free Waits"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-checkpointing-and-flushing","text":"","title":"InnoDB Checkpointing and Flushing"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-flushed-from-flush-list","text":"Number of Pages Flushed from \u201cFlush List\u201d This combines Pages Flushed through Adaptive Flush and Background Flush.","title":"Pages Flushed from Flush List"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#page-flush-batches-executed","text":"Innodb Flush Cycle typically Runs on 1 second intervals. If it is too far off from this number it can indicate an issue.","title":"Page Flush Batches Executed"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-flushed-per-batch","text":"How many pages are flushed per Batch. Large Batches can \u201cchoke\u201d IO subsystem and starve other IO which needs to happen.","title":"Pages Flushed Per Batch"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#neighbor-flushing-enabled","text":"Neighbor Flushing is Optimized for Rotational Media and unless you\u2019re Running spinning disks you should disable it.","title":"Neighbor Flushing Enabled"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-checkpoint-age","text":"InnoDB Checkpoint Age The maximum checkpoint age is determined by the total length of all transaction log files ( innodb_log_file_size ). When the checkpoint age reaches the maximum checkpoint age, blocks are flushed syncronously. The rules of the thumb is to keep one hour of traffic in those logs and let the checkpointing perform its work as smooth as possible. If you don\u2019t do this, InnoDB will do synchronous flushing at the worst possible time, ie when you are busiest.","title":"InnoDB Checkpoint Age"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-flushed-adaptive","text":"Adaptive Flush Flushes pages from Flush List based on the need to advance Checkpoint (driven by Redo Generation Rate) and by maintaining number of dirty pages within set limit.","title":"Pages Flushed (Adaptive)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#adaptive-flush-batches-executed","text":"","title":"Adaptive Flush Batches Executed"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-per-batch-adaptive","text":"Pages Flushed Per Adaptive Batch.","title":"Pages Per Batch (Adaptive)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#neighbor-flushing","text":"To optimize IO for rotational Media InnoDB may flush neighbor pages. It can cause significant wasted IO for flash storage. Generally for flash you should run with innodb_flush_neighbors=0 but otherwise this shows how much IO you\u2019re wasting.","title":"Neighbor Flushing"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-flushed-lru","text":"Flushing from the tail of LRU list needs to happen when data does not fit in buffer pool in order to maintain free pages readily available for new data to be read.","title":"Pages Flushed (LRU)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lru-flush-batches-executed","text":"","title":"LRU Flush Batches Executed"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-per-batch-lru","text":"Pages Flushed Per Neighbor.","title":"Pages Per Batch (LRU)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lsn-age-flush-batch-target","text":"Target for Pages to Flush due to LSN Age.","title":"LSN Age Flush Batch Target"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-flushed-neighbor","text":"Number of Neighbor pages flushed (If neighbor flushing is enabled) from Flush List and LRU List Combined.","title":"Pages Flushed (Neighbor)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#neighbor-flush-batches-executed","text":"","title":"Neighbor Flush Batches Executed"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-per-batch-neighbor","text":"Pages Flushed Per Neighbor.","title":"Pages Per Batch (Neighbor)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#sync-flush-waits","text":"If Innodb could not keep up with Checkpoint Flushing and had to trigger Sync flush. This should never happen.","title":"Sync Flush Waits"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-flushed-background","text":"Pages Flushed by Background Flush which is activated when server is considered to be idle.","title":"Pages Flushed (Background)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#background-flush-batches-executed","text":"","title":"Background Flush Batches Executed"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-per-batch-background","text":"Pages Flushed Per Background Batch.","title":"Pages Per Batch (Background)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#redo-generation-rate","text":"Rate at which LSN (Redo) is Created. It may not match how much data is written to log files due to block size rounding.","title":"Redo Generation Rate"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-flushing-by-type","text":"","title":"Innodb Flushing by Type"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-evicted-lru","text":"This correspond to number of clean pages which were evicted (made free) from the tail of LRU buffer.","title":"Pages Evicted (LRU)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#page-eviction-batches","text":"","title":"Page Eviction Batches"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-evicted-per-batch","text":"","title":"Pages Evicted per Batch"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#max-log-space-used","text":"","title":"Max Log Space Used"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#single-page-flushes","text":"Single Page flushes happen in rare case, then clean page could not be found in LRU list. It should be zero for most workloads.","title":"Single Page Flushes"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#single-page-flush-pages-scanned","text":"","title":"Single Page Flush Pages Scanned"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#pages-scanned-per-single-page-flush","text":"","title":"Pages Scanned Per Single Page Flush"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-io-capacity","text":"Estimated number of IOPS storage system can provide. Is used to scale background activities. Do not set it to actual storage capacity.","title":"Innodb IO Capacity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-io-capacity-max","text":"InnoDB IO Capacity to use when falling behind and need to catch up with Flushing.","title":"Innodb IO Capacity Max"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-logging","text":"","title":"InnoDB Logging"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#total-log-space","text":"Number of Innodb Log Files Multiplied by Their Size.","title":"Total Log Space"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-buffer-size","text":"InnoDB Log Buffer Size The size of buffer InnoDB uses for buffering writes to log files.","title":"Log Buffer Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#at-transaction-commit","text":"What to do with Log file At Transaction Commit. Do nothing and wait for timeout to flush the data from Log Buffer, Flush it to OS Cache but not FSYNC or Flush only.","title":"At Transaction Commit"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#flush-transaction-log-every","text":"Every Specified Number of Seconds Flush Transaction Log.","title":"Flush Transaction Log Every"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-write-ahead-block-size","text":"This variable can be seen as minimum IO alignment InnoDB will use for Redo log file. High Values cause waste, low values can make IO less efficient.","title":"InnoDB Write Ahead Block Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-write-amplification","text":"How much Writes to Log Are Amplified compared to how much Redo is Generated.","title":"Log Write Amplification"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-fsync-rate","text":"","title":"Log Fsync Rate"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#redo-generated-per-trx","text":"Amount of Redo Generated Per Write Transaction. This is a good indicator of transaction size.","title":"Redo Generated per Trx"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-log-file-usage-hourly","text":"InnoDB Log File Usage Hourly Along with the buffer pool size, innodb_log_file_size is the most important setting when we are working with InnoDB. This graph shows how much data was written to InnoDB\u2019s redo logs over each hour. When the InnoDB log files are full, InnoDB needs to flush the modified pages from memory to disk. The rules of the thumb is to keep one hour of traffic in those logs and let the checkpointing perform its work as smooth as possible. If you don\u2019t do this, InnoDB will do synchronous flushing at the worst possible time, ie when you are busiest. This graph can help guide you in setting the correct innodb_log_file_size .","title":"InnoDB Log File Usage Hourly"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-padding-written","text":"Amount of Log Padding Written.","title":"Log Padding Written"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-log-file-size","text":"","title":"InnoDB Log File Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-log-files","text":"Number of InnoDB Redo Log Files.","title":"InnoDB Log Files"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#log-bandwidth","text":"","title":"Log Bandwidth"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#redo-generation-rate_1","text":"Rate at which LSN (Redo) is Created. It may not match how much data is written to log files due to block size rounding.","title":"Redo Generation Rate"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-group-commit-batch-size","text":"The InnoDB Group Commit Batch Size graph shows how many bytes were written to the InnoDB log files per attempt to write. If many threads are committing at the same time, one of them will write the log entries of all the waiting threads and flush the file. Such process reduces the number of disk operations needed and enlarge the batch size.","title":"InnoDB Group Commit Batch Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-locking","text":"","title":"InnoDB Locking"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#lock-wait-timeout","text":"InnoDB Lock Wait Timeout How long to wait for row lock before timing out.","title":"Lock Wait Timeout"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-deadlock-detection","text":"If Disabled Innodb Will not detect deadlocks but rely on timeouts.","title":"InnoDB Deadlock Detection"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-auto-increment-lock-mode","text":"Will Define How much locking will come from working with Auto Increment Columns.","title":"InnoDB Auto Increment Lock Mode"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#rollback-on-timeout","text":"Whenever to rollback all transaction on timeout or just last statement.","title":"Rollback on Timeout"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#row-lock-blocking","text":"Percent of Active Sections which are blocked due to waiting on Innodb Row Locks.","title":"Row Lock Blocking"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#row-writes-per-trx_1","text":"Rows Written Per Transactions which modify rows. This is better indicator of transaction write size than looking at all transactions which did not do any writes as well.","title":"Row Writes per Trx"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#rollbacks_1","text":"Percent of Transaction Rollbacks (as portion of read-write transactions).","title":"Rollbacks"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-row-lock-wait-activity","text":"","title":"InnoDB Row Lock Wait Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-row-lock-wait-time","text":"","title":"Innodb Row Lock Wait Time"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-row-lock-wait-load","text":"Average Number of Sessions blocked from proceeding due to waiting on row level lock.","title":"Innodb Row Lock Wait Load"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-row-locks-activity","text":"","title":"Innodb Row Locks Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-table-lock-activity","text":"","title":"Innodb Table Lock Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#current-locks","text":"","title":"Current Locks"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-undo-space-and-purging","text":"","title":"InnoDB Undo Space and Purging"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#undo-tablespaces","text":"","title":"Undo Tablespaces"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#max-undo-log-size","text":"","title":"Max Undo Log Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-undo-log-truncate","text":"","title":"Innodb Undo Log Truncate"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#purge-threads","text":"","title":"Purge Threads"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#max-purge-lag","text":"Maximum number of Unpurged Transactions, if this number exceeded delay will be introduced to incoming DDL statements.","title":"Max Purge Lag"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#max-purge-lag-delay","text":"","title":"Max Purge Lag Delay"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#current-purge-delay","text":"The Delay Injected due to Purge Thread(s) unable to keep up with purge progress.","title":"Current Purge Delay"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#rollback-segments","text":"","title":"Rollback Segments"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-purge-activity","text":"The InnoDB Purge Performance graph shows metrics about the page purging process. The purge process removed the undo entries from the history list and cleanup the pages of the old versions of modified rows and effectively remove deleted rows. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client.","title":"InnoDB Purge Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#transactions-and-undo-records","text":"","title":"Transactions and Undo Records"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-undo-space-usage","text":"The InnoDB Undo Space Usage graph shows the amount of space used by the Undo segment. If the amount of space grows too much, look for long running transactions holding read views opened in the InnoDB status. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client.","title":"InnoDB Undo Space Usage"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#transaction-history","text":"","title":"Transaction History"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-purge-throttling","text":"","title":"InnoDB Purge Throttling"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#records-per-undo-log-page","text":"How Many Undo Operations Are Handled Per Each Undo Log Page.","title":"Records Per Undo Log Page"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#purge-invoked","text":"How Frequently Purge Operation is Invoked.","title":"Purge Invoked"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#ops-per-purge","text":"Home Many Purge Actions are done Per invocation.","title":"Ops Per Purge"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#undo-slots-used","text":"Number of Undo Slots Used.","title":"Undo Slots Used"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#max-transaction-history-length","text":"","title":"Max Transaction History Length"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#purge-batch-size","text":"","title":"Purge Batch Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#rseg-truncate-frequency","text":"","title":"Rseg Truncate Frequency"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-page-operations","text":"","title":"InnoDB Page Operations"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-page-splits-and-merges","text":"The InnoDB Page Splits graph shows the InnoDB page maintenance activity related to splitting and merging pages. When an InnoDB page, other than the top most leaf page, has too much data to accept a row update or a row insert, it has to be split in two. Similarly, if an InnoDB page, after a row update or delete operation, ends up being less than half full, an attempt is made to merge the page with a neighbor page. If the resulting page size is larger than the InnoDB page size, the operation fails. If your workload causes a large number of page splits, try lowering the innodb_fill_factor variable (5.7+). Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client.","title":"InnoDB Page Splits and Merges"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#page-merge-success-ratio","text":"","title":"Page Merge Success Ratio"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-page-reorg-attempts","text":"The InnoDB Page Reorgs graph shows information about the page reorganization operations. When a page receives an update or an insert that affect the offset of other rows in the page, a reorganization is needed. If the reorganization process finds out there is not enough room in the page, the page will be split. Page reorganization can only fail for compressed pages. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client.","title":"InnoDB Page Reorg Attempts"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-page-reorgs-failures","text":"The InnoDB Page Reorgs graph shows information about the page reorganization operations. When a page receives an update or an insert that affect the offset of other rows in the page, a reorganization is needed. If the reorganization process finds out there is not enough room in the page, the page will be split. Page reorganization can only fail for compressed pages. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client.","title":"InnoDB Page Reorgs Failures"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-fill-factor","text":"The portion of the page to fill then doing sorted Index Build. Lowering this value will worsen space utilization but will reduce need to split pages when new data is inserted in the index.","title":"InnoDB Fill Factor"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-adaptive-hash-index","text":"","title":"InnoDB Adaptive Hash Index"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#adaptive-hash-index-enabled","text":"Adaptive Hash Index Helps to Optimize Index Lookups but can be severe hotspot for some workloads.","title":"Adaptive Hash Index Enabled"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#adaptive-hash-index-partitions","text":"How many Partitions Used for Adaptive Hash Index (to reduce contention).","title":"Adaptive Hash Index Partitions"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#percent-of-pages-hashed","text":"Number of Pages Added to AHI vs Number of Pages Added to Buffer Pool.","title":"Percent of Pages Hashed"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#ahi-miss-ratio","text":"Percent of Searches which could not be resolved through AHI.","title":"AHI Miss Ratio"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#rows-added-per-page","text":"Number of Rows \u201cHashed\u201d Per Each Page which needs to be added to AHI.","title":"Rows Added Per Page"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#ahi-roi","text":"How Many Successful Searches using AHI are performed per each row maintenance operation.","title":"AHI ROI"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-ahi-usage","text":"The InnoDB AHI Usage graph shows the search operations on the InnoDB adaptive hash index and its efficiency. The adaptive hash index is a search hash designed to speed access to InnoDB pages in memory. If the Hit Ratio is small, the working data set is larger than the buffer pool, the AHI should likely be disabled. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client.","title":"InnoDB AHI Usage"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-ahi-miss-ratio","text":"","title":"InnoDB AHI Miss Ratio"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-ahi-churn-rows","text":"","title":"InnoDB AHI Churn - Rows"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-ahi-churn-pages","text":"","title":"InnoDB AHI Churn - Pages"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-change-buffer","text":"","title":"InnoDB Change Buffer"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#change-buffer-max-size","text":"The Maximum Size of Change Buffer (as Percent of Buffer Pool Size).","title":"Change Buffer Max Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#change-buffer-max-size_1","text":"The Maximum Size of Change Buffer (Bytes).","title":"Change Buffer Max Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-change-buffer-merge-load","text":"Number of Average of Active Merge Buffer Operations in Process.","title":"InnoDB Change Buffer Merge Load"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-contention","text":"","title":"InnoDB Contention"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-thread-concurrency","text":"If Enabled limits number of Threads allowed inside Innodb Kernel at the same time.","title":"InnoDB Thread Concurrency"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-commit-concurrency","text":"If Enabled limits number of Threads allowed inside InnoDB Kernel at the same time during Commit Stage.","title":"InnoDB Commit Concurrency"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-thread-sleep-delay","text":"The Time the thread will Sleep before Re-Entering Innodb Kernel if high contention.","title":"InnoDB Thread Sleep Delay"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-adaptive-max-sleep-delay","text":"If Set to Non-Zero Value InnoDB Thread Sleep Delay will be adjusted automatically depending on the load up to the value specified by this variable.","title":"InnoDB Adaptive Max Sleep Delay"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-concurrency-tickets","text":"Number of low level operations InnoDB can do after it entered InnoDB kernel before it is forced to exit and yield to another thread waiting.","title":"InnoDB Concurrency Tickets"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-spin-wait-delay","text":"","title":"InnoDB Spin Wait Delay"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-spin-wait-pause-multiplier","text":"","title":"InnoDB Spin Wait Pause Multiplier"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-sync-spin-loops","text":"","title":"InnoDB Sync Spin Loops"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-contention-os-waits","text":"The InnoDB Contention - OS Waits graph shows the number of time an OS wait operation was required while waiting to get the lock. This happens once the spin rounds are exhausted. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client.","title":"InnoDB Contention - OS Waits"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-contention-spin-rounds","text":"The InnoDB Contention - Spin Rounds graph shows the number of spin rounds executed in order to get a lock. A spin round is a fast retry to get the lock in a loop. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client.","title":"InnoDB Contention - Spin Rounds"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-misc","text":"","title":"InnoDB Misc"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-main-thread-utilization","text":"The InnoDB Main Thread Utilization graph shows the portion of time the InnoDB main thread spent at various task. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client.","title":"InnoDB Main Thread Utilization"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-activity_1","text":"The InnoDB Activity graph shows a measure of the activity of the InnoDB threads. Note: If you do not see any metric, try running: SET GLOBAL innodb_monitor_enable=all; in the MySQL client.","title":"InnoDB Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-dedicated-server","text":"InnoDB automatically optimized for Dedicated Server Environment (auto scaling cache and some other variables).","title":"InnoDB Dedicated Server"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-sort-buffer-size","text":"This Buffer is used for Building InnoDB Indexes using Sort algorithm.","title":"InnoDB Sort Buffer Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-stats-auto-recalc","text":"","title":"InnoDB Stats Auto Recalc"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#update-stats-when-metadata-queried","text":"Refresh InnoDB Statistics when meta-data queries by SHOW TABLE STATUS or INFORMATION_SCHEMA queries. If Enabled can cause severe performance issues.","title":"Update Stats when Metadata Queried"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#index-condition-pushdown-icp","text":"Index Condition Pushdown (ICP) is an optimization for the case where MySQL retrieves rows from a table using an index. Without ICP, the storage engine traverses the index to locate rows in the base table and returns them to the MySQL server which evaluates the WHERE condition for the rows. With ICP enabled, and if parts of the WHERE condition can be evaluated by using only columns from the index, the MySQL server pushes this part of the WHERE condition down to the storage engine. The storage engine then evaluates the pushed index condition by using the index entry and only if this is satisfied is the row read from the table. ICP can reduce the number of times the storage engine must access the base table and the number of times the MySQL server must access the storage engine.","title":"Index Condition Pushdown (ICP)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-persistent-statistics","text":"","title":"InnoDB Persistent Statistics"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-persistent-sample-pages","text":"Number of Pages To Sample if Persistent Statistics are Enabled.","title":"InnoDB Persistent Sample Pages"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-transient-sample-pages","text":"Number of Pages To Sample if Persistent Statistics are Disabled.","title":"InnoDB Transient Sample Pages"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-online-operations-mariadb","text":"","title":"InnoDB Online Operations (MariaDB)"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-defragmentation","text":"The InnoDB Defragmentation graph shows the status information related to the InnoDB online defragmentation feature of MariaDB for the optimize table command. To enable this feature, the variable innodb-defragment must be set to 1 in the configuration file. Note: Currently available only on a MariaDB server.","title":"InnoDB Defragmentation"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#innodb-online-ddl","text":"The InnoDB Online DDL graph shows the state of the online DDL (alter table) operations in InnoDB. The progress metric is estimate of the percentage of the rows processed by the online DDL. Note: Currently available only on a MariaDB server.","title":"InnoDB Online DDL"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#mysql-summary","text":"","title":"MySQL Summary"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#mysql-uptime","text":"MySQL Uptime The amount of time since the last restart of the MySQL server process.","title":"MySQL Uptime"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#current-qps","text":"Current QPS Based on the queries reported by MySQL\u2019s SHOW STATUS command, it is the number of statements executed by the server within the last second. This variable includes statements executed within stored programs, unlike the Questions variable. It does not count COM_PING or COM_STATISTICS commands.","title":"Current QPS"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#file-handlers-used","text":"","title":"File Handlers Used"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#table-open-cache-miss-ratio","text":"","title":"Table Open Cache Miss Ratio"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#table-open-cache-size","text":"","title":"Table Open Cache Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#table-definition-cache-size","text":"","title":"Table Definition Cache Size"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#mysql-connections","text":"Max Connections Max Connections is the maximum permitted number of simultaneous client connections. By default, this is 151. Increasing this value increases the number of file descriptors that mysqld requires. If the required number of descriptors are not available, the server reduces the value of Max Connections. mysqld actually permits Max Connections + 1 clients to connect. The extra connection is reserved for use by accounts that have the SUPER privilege, such as root. Max Used Connections is the maximum number of connections that have been in use simultaneously since the server started. Connections is the number of connection attempts (successful or not) to the MySQL server.","title":"MySQL Connections"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#mysql-client-thread-activity","text":"MySQL Active Threads Threads Connected is the number of open connections, while Threads Running is the number of threads not sleeping.","title":"MySQL Client Thread Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#mysql-handlers","text":"MySQL Handlers Handler statistics are internal statistics on how MySQL is selecting, updating, inserting, and modifying rows, tables, and indexes. This is in fact the layer between the Storage Engine and MySQL. read_rnd_next is incremented when the server performs a full table scan and this is a counter you don\u2019t really want to see with a high value. read_key is incremented when a read is done with an index. read_next is incremented when the storage engine is asked to \u2018read the next index entry\u2019. A high value means a lot of index scans are being done.","title":"MySQL Handlers"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#top-command-counters","text":"Top Command Counters The Com_ statement counter variables indicate the number of times each xxx statement has been executed. There is one status variable for each type of statement. For example, Com_delete and Com_update count DELETE and UPDATE statements, respectively. Com_delete_multi and Com_update_multi are similar but apply to DELETE and UPDATE statements that use multiple-table syntax.","title":"Top Command Counters"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#mysql-network-traffic","text":"MySQL Network Traffic Here we can see how much network traffic is generated by MySQL. Outbound is network traffic sent from MySQL and Inbound is network traffic MySQL has received.","title":"MySQL Network Traffic"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#node-summary","text":"","title":"Node Summary"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#system-uptime","text":"The parameter shows how long a system has been \u201cup\u201d and running without a shut down or restart.","title":"System Uptime"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#load-average","text":"The system load is a measurement of the computational work the system is performing. Each running process either using or waiting for CPU resources adds 1 to the load.","title":"Load Average"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#ram","text":"RAM (Random Access Memory) is the hardware in a computing device where the operating system, application programs and data in current use are kept so they can be quickly reached by the device\u2019s processor.","title":"RAM"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#memory-available","text":"Percent of Memory Available Note: on Modern Linux Kernels amount of Memory Available for application is not the same as Free+Cached+Buffers.","title":"Memory Available"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#virtual-memory","text":"RAM + SWAP","title":"Virtual Memory"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#disk-space","text":"Sum of disk space on all partitions. Note it can be significantly over-reported in some installations.","title":"Disk Space"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#min-space-available","text":"Lowest percent of the disk space available.","title":"Min Space Available"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#cpu-usage","text":"The CPU time is measured in clock ticks or seconds. It is useful to measure CPU time as a percentage of the CPU\u2019s capacity, which is called the CPU usage.","title":"CPU Usage"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#cpu-saturation-and-max-core-usage","text":"When a system is running with maximum CPU utilization, the transmitting and receiving threads must all share the available CPU. This will cause data to be queued more frequently to cope with the lack of CPU. CPU Saturation may be measured as the length of a wait queue, or the time spent waiting on the queue.","title":"CPU Saturation and Max Core Usage"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#disk-io-and-swap-activity","text":"Disk I/O includes read or write or input/output operations involving a physical disk. It is the speed with which the data transfer takes place between the hard disk drive and RAM. Swap Activity is memory management that involves swapping sections of memory to and from physical storage.","title":"Disk I/O and Swap Activity"},{"location":"details/dashboards/dashboard-mysql-innodb-details.html#network-traffic","text":"Network traffic refers to the amount of data moving across a network at a given point in time.","title":"Network Traffic"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html","text":"MySQL Instance Summary MySQL Connections Max Connections Max Connections is the maximum permitted number of simultaneous client connections. By default, this is 151. Increasing this value increases the number of file descriptors that mysqld requires. If the required number of descriptors are not available, the server reduces the value of Max Connections. mysqld actually permits Max Connections + 1 clients to connect. The extra connection is reserved for use by accounts that have the SUPER privilege, such as root. Max Used Connections is the maximum number of connections that have been in use simultaneously since the server started. Connections is the number of connection attempts (successful or not) to the MySQL server. MySQL Aborted Connections Aborted Connections When a given host connects to MySQL and the connection is interrupted in the middle (for example due to bad credentials), MySQL keeps that info in a system table (since 5.6 this table is exposed in performance_schema). If the amount of failed requests without a successful connection reaches the value of max_connect_errors, mysqld assumes that something is wrong and blocks the host from further connection. To allow connections from that host again, you need to issue the FLUSH HOSTS statement. MySQL Client Thread Activity MySQL Active Threads Threads Connected is the number of open connections, while Threads Running is the number of threads not sleeping. MySQL Thread Cache MySQL Thread Cache The thread_cache_size variable sets how many threads the server should cache to reuse. When a client disconnects, the client\u2019s threads are put in the cache if the cache is not full. It is autosized in MySQL 5.6.8 and above (capped to 100). Requests for threads are satisfied by reusing threads taken from the cache if possible, and only when the cache is empty is a new thread created. Threads_created : The number of threads created to handle connections. Threads_cached : The number of threads in the thread cache. MySQL Slow Queries MySQL Slow Queries Slow queries are defined as queries being slower than the long_query_time setting. For example, if you have long_query_time set to 3, all queries that take longer than 3 seconds to complete will show on this graph. MySQL Select Types MySQL Select Types As with most relational databases, selecting based on indexes is more efficient than scanning an entire table\u2019s data. Here we see the counters for selects not done with indexes. Select Scan is how many queries caused full table scans, in which all the data in the table had to be read and either discarded or returned. Select Range is how many queries used a range scan, which means MySQL scanned all rows in a given range. Select Full Join is the number of joins that are not joined on an index, this is usually a huge performance hit. MySQL Sorts MySQL Sorts Due to a query\u2019s structure, order, or other requirements, MySQL sorts the rows before returning them. For example, if a table is ordered 1 to 10 but you want the results reversed, MySQL then has to sort the rows to return 10 to 1. This graph also shows when sorts had to scan a whole table or a given range of a table in order to return the results and which could not have been sorted via an index. MySQL Table Locks Table Locks MySQL takes a number of different locks for varying reasons. In this graph we see how many Table level locks MySQL has requested from the storage engine. In the case of InnoDB, many times the locks could actually be row locks as it only takes table level locks in a few specific cases. It is most useful to compare Locks Immediate and Locks Waited. If Locks waited is rising, it means you have lock contention. Otherwise, Locks Immediate rising and falling is normal activity. MySQL Questions MySQL Questions The number of statements executed by the server. This includes only statements sent to the server by clients and not statements executed within stored programs, unlike the Queries used in the QPS calculation. This variable does not count the following commands: COM_PING COM_STATISTICS COM_STMT_PREPARE COM_STMT_CLOSE COM_STMT_RESET MySQL Network Traffic MySQL Network Traffic Here we can see how much network traffic is generated by MySQL. Outbound is network traffic sent from MySQL and Inbound is network traffic MySQL has received. MySQL Network Usage Hourly MySQL Network Usage Hourly Here we can see how much network traffic is generated by MySQL per hour. You can use the bar graph to compare data sent by MySQL and data received by MySQL. MySQL Internal Memory Overview System Memory : Total Memory for the system. InnoDB Buffer Pool Data : InnoDB maintains a storage area called the buffer pool for caching data and indexes in memory. TokuDB Cache Size : Similar in function to the InnoDB Buffer Pool, TokuDB will allocate 50% of the installed RAM for its own cache. Key Buffer Size : Index blocks for MYISAM tables are buffered and are shared by all threads. key_buffer_size is the size of the buffer used for index blocks. Adaptive Hash Index Size : When InnoDB notices that some index values are being accessed very frequently, it builds a hash index for them in memory on top of B-Tree indexes. Query Cache Size : The query cache stores the text of a SELECT statement together with the corresponding result that was sent to the client. The query cache has huge scalability problems in that only one thread can do an operation in the query cache at the same time. InnoDB Dictionary Size : The data dictionary is InnoDB\u2019s internal catalog of tables. InnoDB stores the data dictionary on disk, and loads entries into memory while the server is running. InnoDB Log Buffer Size : The MySQL InnoDB log buffer allows transactions to run without having to write the log to disk before the transactions commit. Top Command Counters Top Command Counters The Com_ statement counter variables indicate the number of times each xxx statement has been executed. There is one status variable for each type of statement. For example, Com_delete and Com_update count DELETE and UPDATE statements, respectively. Com_delete_multi and Com_update_multi are similar but apply to DELETE and UPDATE statements that use multiple-table syntax. Top Command Counters Hourly Top Command Counters Hourly The Com_ statement counter variables indicate the number of times each xxx statement has been executed. There is one status variable for each type of statement. For example, Com_delete and Com_update count DELETE and UPDATE statements, respectively. Com_delete_multi and Com_update_multi are similar but apply to DELETE and UPDATE statements that use multiple-table syntax. MySQL Handlers MySQL Handlers Handler statistics are internal statistics on how MySQL is selecting, updating, inserting, and modifying rows, tables, and indexes. This is in fact the layer between the Storage Engine and MySQL. read_rnd_next is incremented when the server performs a full table scan and this is a counter you don\u2019t really want to see with a high value. read_key is incremented when a read is done with an index. read_next is incremented when the storage engine is asked to \u2018read the next index entry\u2019. A high value means a lot of index scans are being done. MySQL Query Cache Memory MySQL Query Cache Memory The query cache has huge scalability problems in that only one thread can do an operation in the query cache at the same time. This serialization is true not only for SELECTs, but also for INSERT/UPDATE/DELETE. This also means that the larger the query_cache_size is set to, the slower those operations become. In concurrent environments, the MySQL Query Cache quickly becomes a contention point, decreasing performance. MariaDB and AWS Aurora have done work to try and eliminate the query cache contention in their flavors of MySQL, while MySQL 8.0 has eliminated the query cache feature. The recommended settings for most environments is to set: query_cache_type=0 query_cache_size=0 Note that while you can dynamically change these values, to completely remove the contention point you have to restart the database. MySQL Query Cache Activity MySQL Query Cache Activity The query cache has huge scalability problems in that only one thread can do an operation in the query cache at the same time. This serialization is true not only for SELECTs, but also for INSERT/UPDATE/DELETE. This also means that the larger the query_cache_size is set to, the slower those operations become. In concurrent environments, the MySQL Query Cache quickly becomes a contention point, decreasing performance. MariaDB and AWS Aurora have done work to try and eliminate the query cache contention in their flavors of MySQL, while MySQL 8.0 has eliminated the query cache feature. The recommended settings for most environments is to set: query_cache_type=0 query_cache_size=0 Note that while you can dynamically change these values, to completely remove the contention point you have to restart the database. MySQL Table Open Cache Status MySQL Table Open Cache Status The recommendation is to set the table_open_cache_instances to a loose correlation to virtual CPUs, keeping in mind that more instances means the cache is split more times. If you have a cache set to 500 but it has 10 instances, each cache will only have 50 cached. The table_definition_cache and table_open_cache can be left as default as they are auto-sized MySQL 5.6 and above (ie: do not set them to any value). MySQL Open Tables MySQL Open Tables The recommendation is to set the table_open_cache_instances to a loose correlation to virtual CPUs, keeping in mind that more instances means the cache is split more times. If you have a cache set to 500 but it has 10 instances, each cache will only have 50 cached. The table_definition_cache and table_open_cache can be left as default as they are auto-sized MySQL 5.6 and above (ie: do not set them to any value). MySQL Table Definition Cache MySQL Table Definition Cache The recommendation is to set the table_open_cache_instances to a loose correlation to virtual CPUs, keeping in mind that more instances means the cache is split more times. If you have a cache set to 500 but it has 10 instances, each cache will only have 50 cached. The table_definition_cache and table_open_cache can be left as default as they are auto-sized MySQL 5.6 and above (ie: do not set them to any value).","title":"MySQL Instance Summary"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-connections","text":"Max Connections Max Connections is the maximum permitted number of simultaneous client connections. By default, this is 151. Increasing this value increases the number of file descriptors that mysqld requires. If the required number of descriptors are not available, the server reduces the value of Max Connections. mysqld actually permits Max Connections + 1 clients to connect. The extra connection is reserved for use by accounts that have the SUPER privilege, such as root. Max Used Connections is the maximum number of connections that have been in use simultaneously since the server started. Connections is the number of connection attempts (successful or not) to the MySQL server.","title":"MySQL Connections"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-aborted-connections","text":"Aborted Connections When a given host connects to MySQL and the connection is interrupted in the middle (for example due to bad credentials), MySQL keeps that info in a system table (since 5.6 this table is exposed in performance_schema). If the amount of failed requests without a successful connection reaches the value of max_connect_errors, mysqld assumes that something is wrong and blocks the host from further connection. To allow connections from that host again, you need to issue the FLUSH HOSTS statement.","title":"MySQL Aborted Connections"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-client-thread-activity","text":"MySQL Active Threads Threads Connected is the number of open connections, while Threads Running is the number of threads not sleeping.","title":"MySQL Client Thread Activity"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-thread-cache","text":"MySQL Thread Cache The thread_cache_size variable sets how many threads the server should cache to reuse. When a client disconnects, the client\u2019s threads are put in the cache if the cache is not full. It is autosized in MySQL 5.6.8 and above (capped to 100). Requests for threads are satisfied by reusing threads taken from the cache if possible, and only when the cache is empty is a new thread created. Threads_created : The number of threads created to handle connections. Threads_cached : The number of threads in the thread cache.","title":"MySQL Thread Cache"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-slow-queries","text":"MySQL Slow Queries Slow queries are defined as queries being slower than the long_query_time setting. For example, if you have long_query_time set to 3, all queries that take longer than 3 seconds to complete will show on this graph.","title":"MySQL Slow Queries"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-select-types","text":"MySQL Select Types As with most relational databases, selecting based on indexes is more efficient than scanning an entire table\u2019s data. Here we see the counters for selects not done with indexes. Select Scan is how many queries caused full table scans, in which all the data in the table had to be read and either discarded or returned. Select Range is how many queries used a range scan, which means MySQL scanned all rows in a given range. Select Full Join is the number of joins that are not joined on an index, this is usually a huge performance hit.","title":"MySQL Select Types"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-sorts","text":"MySQL Sorts Due to a query\u2019s structure, order, or other requirements, MySQL sorts the rows before returning them. For example, if a table is ordered 1 to 10 but you want the results reversed, MySQL then has to sort the rows to return 10 to 1. This graph also shows when sorts had to scan a whole table or a given range of a table in order to return the results and which could not have been sorted via an index.","title":"MySQL Sorts"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-table-locks","text":"Table Locks MySQL takes a number of different locks for varying reasons. In this graph we see how many Table level locks MySQL has requested from the storage engine. In the case of InnoDB, many times the locks could actually be row locks as it only takes table level locks in a few specific cases. It is most useful to compare Locks Immediate and Locks Waited. If Locks waited is rising, it means you have lock contention. Otherwise, Locks Immediate rising and falling is normal activity.","title":"MySQL Table Locks"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-questions","text":"MySQL Questions The number of statements executed by the server. This includes only statements sent to the server by clients and not statements executed within stored programs, unlike the Queries used in the QPS calculation. This variable does not count the following commands: COM_PING COM_STATISTICS COM_STMT_PREPARE COM_STMT_CLOSE COM_STMT_RESET","title":"MySQL Questions"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-network-traffic","text":"MySQL Network Traffic Here we can see how much network traffic is generated by MySQL. Outbound is network traffic sent from MySQL and Inbound is network traffic MySQL has received.","title":"MySQL Network Traffic"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-network-usage-hourly","text":"MySQL Network Usage Hourly Here we can see how much network traffic is generated by MySQL per hour. You can use the bar graph to compare data sent by MySQL and data received by MySQL.","title":"MySQL Network Usage Hourly"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-internal-memory-overview","text":"System Memory : Total Memory for the system. InnoDB Buffer Pool Data : InnoDB maintains a storage area called the buffer pool for caching data and indexes in memory. TokuDB Cache Size : Similar in function to the InnoDB Buffer Pool, TokuDB will allocate 50% of the installed RAM for its own cache. Key Buffer Size : Index blocks for MYISAM tables are buffered and are shared by all threads. key_buffer_size is the size of the buffer used for index blocks. Adaptive Hash Index Size : When InnoDB notices that some index values are being accessed very frequently, it builds a hash index for them in memory on top of B-Tree indexes. Query Cache Size : The query cache stores the text of a SELECT statement together with the corresponding result that was sent to the client. The query cache has huge scalability problems in that only one thread can do an operation in the query cache at the same time. InnoDB Dictionary Size : The data dictionary is InnoDB\u2019s internal catalog of tables. InnoDB stores the data dictionary on disk, and loads entries into memory while the server is running. InnoDB Log Buffer Size : The MySQL InnoDB log buffer allows transactions to run without having to write the log to disk before the transactions commit.","title":"MySQL Internal Memory Overview"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#top-command-counters","text":"Top Command Counters The Com_ statement counter variables indicate the number of times each xxx statement has been executed. There is one status variable for each type of statement. For example, Com_delete and Com_update count DELETE and UPDATE statements, respectively. Com_delete_multi and Com_update_multi are similar but apply to DELETE and UPDATE statements that use multiple-table syntax.","title":"Top Command Counters"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#top-command-counters-hourly","text":"Top Command Counters Hourly The Com_ statement counter variables indicate the number of times each xxx statement has been executed. There is one status variable for each type of statement. For example, Com_delete and Com_update count DELETE and UPDATE statements, respectively. Com_delete_multi and Com_update_multi are similar but apply to DELETE and UPDATE statements that use multiple-table syntax.","title":"Top Command Counters Hourly"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-handlers","text":"MySQL Handlers Handler statistics are internal statistics on how MySQL is selecting, updating, inserting, and modifying rows, tables, and indexes. This is in fact the layer between the Storage Engine and MySQL. read_rnd_next is incremented when the server performs a full table scan and this is a counter you don\u2019t really want to see with a high value. read_key is incremented when a read is done with an index. read_next is incremented when the storage engine is asked to \u2018read the next index entry\u2019. A high value means a lot of index scans are being done.","title":"MySQL Handlers"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-query-cache-memory","text":"MySQL Query Cache Memory The query cache has huge scalability problems in that only one thread can do an operation in the query cache at the same time. This serialization is true not only for SELECTs, but also for INSERT/UPDATE/DELETE. This also means that the larger the query_cache_size is set to, the slower those operations become. In concurrent environments, the MySQL Query Cache quickly becomes a contention point, decreasing performance. MariaDB and AWS Aurora have done work to try and eliminate the query cache contention in their flavors of MySQL, while MySQL 8.0 has eliminated the query cache feature. The recommended settings for most environments is to set: query_cache_type=0 query_cache_size=0 Note that while you can dynamically change these values, to completely remove the contention point you have to restart the database.","title":"MySQL Query Cache Memory"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-query-cache-activity","text":"MySQL Query Cache Activity The query cache has huge scalability problems in that only one thread can do an operation in the query cache at the same time. This serialization is true not only for SELECTs, but also for INSERT/UPDATE/DELETE. This also means that the larger the query_cache_size is set to, the slower those operations become. In concurrent environments, the MySQL Query Cache quickly becomes a contention point, decreasing performance. MariaDB and AWS Aurora have done work to try and eliminate the query cache contention in their flavors of MySQL, while MySQL 8.0 has eliminated the query cache feature. The recommended settings for most environments is to set: query_cache_type=0 query_cache_size=0 Note that while you can dynamically change these values, to completely remove the contention point you have to restart the database.","title":"MySQL Query Cache Activity"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-table-open-cache-status","text":"MySQL Table Open Cache Status The recommendation is to set the table_open_cache_instances to a loose correlation to virtual CPUs, keeping in mind that more instances means the cache is split more times. If you have a cache set to 500 but it has 10 instances, each cache will only have 50 cached. The table_definition_cache and table_open_cache can be left as default as they are auto-sized MySQL 5.6 and above (ie: do not set them to any value).","title":"MySQL Table Open Cache Status"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-open-tables","text":"MySQL Open Tables The recommendation is to set the table_open_cache_instances to a loose correlation to virtual CPUs, keeping in mind that more instances means the cache is split more times. If you have a cache set to 500 but it has 10 instances, each cache will only have 50 cached. The table_definition_cache and table_open_cache can be left as default as they are auto-sized MySQL 5.6 and above (ie: do not set them to any value).","title":"MySQL Open Tables"},{"location":"details/dashboards/dashboard-mysql-instance-summary.html#mysql-table-definition-cache","text":"MySQL Table Definition Cache The recommendation is to set the table_open_cache_instances to a loose correlation to virtual CPUs, keeping in mind that more instances means the cache is split more times. If you have a cache set to 500 but it has 10 instances, each cache will only have 50 cached. The table_definition_cache and table_open_cache can be left as default as they are auto-sized MySQL 5.6 and above (ie: do not set them to any value).","title":"MySQL Table Definition Cache"},{"location":"details/dashboards/dashboard-mysql-instances-compare.html","text":"MySQL Instances Compare No description","title":"MySQL Instances Compare"},{"location":"details/dashboards/dashboard-mysql-instances-overview.html","text":"MySQL Instances Overview No description","title":"MySQL Instances Overview"},{"location":"details/dashboards/dashboard-mysql-myisam-aria-details.html","text":"MySQL MyISAM/Aria Details MyISAM Key Buffer Performance The Key Read Ratio (Key_reads/Key_read_requests) ratio should normally be less than 0.01. The Key Write Ratio (Key_writes/Key_write_requests) ratio is usually near 1 if you are using mostly updates and deletes, but might be much smaller if you tend to do updates that affect many rows at the same time or if you are using the DELAY_KEY_WRITE table option. Aria Pagecache Reads/Writes This graph is similar to InnoDB buffer pool reads/writes. aria-pagecache-buffer-size is the main cache for the Aria storage engine. If you see high reads/writes (physical IO), i.e. reads are close to read requests and/or writes are close to write requests you may need to increase the aria-pagecache-buffer-size (may need to decrease other buffers: key_buffer_size , innodb_buffer_pool_size , etc.) Aria Transaction Log Syncs This is similar to InnoDB log file syncs. If you see lots of log syncs and want to relax the durability settings you can change aria_checkpoint_interval (in seconds) from 30 (default) to a higher number. It is good to look at the disk IO dashboard as well. Aria Pagecache Blocks This graph shows the utilization for the Aria pagecache. This is similar to InnDB buffer pool graph. If you see all blocks are used you may consider increasing aria-pagecache-buffer-size (may need to decrease other buffers: key_buffer_size , innodb_buffer_pool_size , etc.)","title":"MySQL MyISAM/Aria Details"},{"location":"details/dashboards/dashboard-mysql-myisam-aria-details.html#myisam-key-buffer-performance","text":"The Key Read Ratio (Key_reads/Key_read_requests) ratio should normally be less than 0.01. The Key Write Ratio (Key_writes/Key_write_requests) ratio is usually near 1 if you are using mostly updates and deletes, but might be much smaller if you tend to do updates that affect many rows at the same time or if you are using the DELAY_KEY_WRITE table option.","title":"MyISAM Key Buffer Performance"},{"location":"details/dashboards/dashboard-mysql-myisam-aria-details.html#aria-pagecache-readswrites","text":"This graph is similar to InnoDB buffer pool reads/writes. aria-pagecache-buffer-size is the main cache for the Aria storage engine. If you see high reads/writes (physical IO), i.e. reads are close to read requests and/or writes are close to write requests you may need to increase the aria-pagecache-buffer-size (may need to decrease other buffers: key_buffer_size , innodb_buffer_pool_size , etc.)","title":"Aria Pagecache Reads/Writes"},{"location":"details/dashboards/dashboard-mysql-myisam-aria-details.html#aria-transaction-log-syncs","text":"This is similar to InnoDB log file syncs. If you see lots of log syncs and want to relax the durability settings you can change aria_checkpoint_interval (in seconds) from 30 (default) to a higher number. It is good to look at the disk IO dashboard as well.","title":"Aria Transaction Log Syncs"},{"location":"details/dashboards/dashboard-mysql-myisam-aria-details.html#aria-pagecache-blocks","text":"This graph shows the utilization for the Aria pagecache. This is similar to InnDB buffer pool graph. If you see all blocks are used you may consider increasing aria-pagecache-buffer-size (may need to decrease other buffers: key_buffer_size , innodb_buffer_pool_size , etc.)","title":"Aria Pagecache Blocks"},{"location":"details/dashboards/dashboard-mysql-myrocks-details.html","text":"MySQL MyRocks Details The MyRocks storage engine developed by Facebook based on the RocksDB storage engine is applicable to systems which primarily interact with the database by writing data to it rather than reading from it. RocksDB also features a good level of compression, higher than that of the InnoDB storage engine, which makes it especially valuable when optimizing the usage of hard drives. PMM collects statistics on the MyRocks storage engine for MySQL in the Metrics Monitor information for this dashboard comes from the Information Schema tables. Metrics MyRocks cache MyRocks cache data bytes R/W MyRocks cache index hit rate MyRocks cache index MyRocks cache filter hit rate MyRocks cache filter MyRocks cache data byltes inserted MyRocks bloom filter MyRocks memtable MyRocks memtable size MyRocks number of keys MyRocks cache L0/L1 MyRocks number of DB ops MyRocks R/W MyRocks bytes read by iterations MyRocks write ops MyRocks WAL MyRocks number reseeks in iterations RocksDB row operations MyRocks file operations RocksDB stalls RocksDB stops/slowdowns","title":"MySQL MyRocks Details"},{"location":"details/dashboards/dashboard-mysql-performance-schema-details.html","text":"MySQL Performance Schema Details The MySQL Performance Schema dashboard helps determine the efficiency of communicating with Performance Schema. This dashboard contains the following metrics: Performance Schema file IO (events) Performance Schema file IO (load) Performance Schema file IO (Bytes) Performance Schema waits (events) Performance Schema waits (load) Index access operations (load) Table access operations (load) Performance Schema SQL and external locks (events) Performance Schema SQL and external locks (seconds)","title":"MySQL Performance Schema Details"},{"location":"details/dashboards/dashboard-mysql-query-response-time-details.html","text":"MySQL Query Response Time Details Average Query Response Time The Average Query Response Time graph shows information collected using the Response Time Distribution plugin sourced from table INFORMATION_SCHEMA. QUERY_RESPONSE_TIME . It computes this value across all queries by taking the sum of seconds divided by the count of queries. Query Response Time Distribution Query response time counts (operations) are grouped into three buckets: 100ms - 1s 1s - 10s > 10s Average Query Response Time Available only in Percona Server for MySQL , provides visibility of the split of READ vs WRITE query response time. Read Query Response Time Distribution Available only in Percona Server for MySQL, illustrates READ query response time counts (operations) grouped into three buckets: 100ms - 1s 1s - 10s > 10s Write Query Response Time Distribution Available only in Percona Server for MySQL, illustrates WRITE query response time counts (operations) grouped into three buckets: 100ms - 1s 1s - 10s > 10s","title":"MySQL Query Response Time Details"},{"location":"details/dashboards/dashboard-mysql-query-response-time-details.html#average-query-response-time","text":"The Average Query Response Time graph shows information collected using the Response Time Distribution plugin sourced from table INFORMATION_SCHEMA. QUERY_RESPONSE_TIME . It computes this value across all queries by taking the sum of seconds divided by the count of queries.","title":"Average Query Response Time"},{"location":"details/dashboards/dashboard-mysql-query-response-time-details.html#query-response-time-distribution","text":"Query response time counts (operations) are grouped into three buckets: 100ms - 1s 1s - 10s > 10s","title":"Query Response Time Distribution"},{"location":"details/dashboards/dashboard-mysql-query-response-time-details.html#average-query-response-time_1","text":"Available only in Percona Server for MySQL , provides visibility of the split of READ vs WRITE query response time.","title":"Average Query Response Time"},{"location":"details/dashboards/dashboard-mysql-query-response-time-details.html#read-query-response-time-distribution","text":"Available only in Percona Server for MySQL, illustrates READ query response time counts (operations) grouped into three buckets: 100ms - 1s 1s - 10s > 10s","title":"Read Query Response Time Distribution"},{"location":"details/dashboards/dashboard-mysql-query-response-time-details.html#write-query-response-time-distribution","text":"Available only in Percona Server for MySQL, illustrates WRITE query response time counts (operations) grouped into three buckets: 100ms - 1s 1s - 10s > 10s","title":"Write Query Response Time Distribution"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html","text":"MySQL Replication Summary IO Thread Running This metric shows if the IO Thread is running or not. It only applies to a secondary host. SQL Thread is a process that runs on a secondary host in the replication environment. It reads the events from the local relay log file and applies them to the secondary server. Depending on the format of the binary log it can read query statements in plain text and re-execute them or it can read raw data and apply them to the local host. Possible values Yes The thread is running and is connected to a replication primary No The thread is not running because it is not launched yet or because an error has occurred connecting to the primary host Connecting The thread is running but is not connected to a replication primary No value The host is not configured to be a replication secondary IO Thread Running is one of the parameters that the command SHOW SLAVE STATUS returns. SQL Thread Running This metric shows if the SQL thread is running or not. It only applies to a secondary host. Possible values Yes SQL Thread is running and is applying events from the relay log to the local secondary host No SQL Thread is not running because it is not launched yet or because of an error occurred while applying an event to the local secondary host Replication Error No This metric shows the number of the last error in the SQL Thread encountered which caused replication to stop. One of the more common errors is Error: 1022 Duplicate Key Entry . In such a case replication is attempting to update a row that already exists on the secondary. The SQL Thread will stop replication in order to avoid data corruption. Read only This metric indicates whether the host is configured to be in Read Only mode or not. Possible values Yes The secondary host permits no client updates except from users who have the SUPER privilege or the REPLICATION SLAVE privilege. This kind of configuration is typically used for secondary hosts in a replication environment to avoid a user can inadvertently or voluntarily modify data causing inconsistencies and stopping the replication process. No The secondary host is not configured in Read Only mode. MySQL Replication Delay This metric shows the number of seconds the secondary host is delayed in replication applying events compared to when the primary host applied them, denoted by the Seconds_Behind_Master value, and only applies to a secondary host. Since the replication process applies the data modifications on the secondary asyncronously, it could happen that the secondary replicates events after some time. The main reasons are: Network round trip time - high latency links will lead to non-zero replication lag values. Single threaded nature of replication channels - primary servers have the advantage of applying changes in parallel, whereas secondary ones are only able to apply changes in serial, thus limiting their throughput. In some cases Group Commit can help but is not always applicable. High number of changed rows or computationally expensive SQL - depending on the replication format ( ROW vs STATEMENT ), significant changes to the database through high volume of rows modified, or expensive CPU will all contribute to secondary servers lagging behind the primary. Generally adding more CPU or Disk resources can alleviate replication lag issues, up to a point. Binlog Size This metric shows the overall size of the binary log files, which can exist on both primary and secondary servers. The binary log (also known as the binlog) contains events that describe database changes: CREATE TABLE , ALTER TABLE , updates, inserts, deletes and other statements or database changes. The binlog is the file that is read by secondaries via their IO Thread process in order to replicate database changes modification on the data and on the table structures. There can be more than one binlog file present depending on the binlog rotation policy adopted (for example using the configuration variables max_binlog_size and expire_logs_days ). Note There can be more binlog files depending on the rotation policy adopted (for example using the configuration variables max_binlog_size and expire_logs_days ) or even because of server reboots. When planning the disk space, take care of the overall dimension of binlog files and adopt a good rotation policy or think about having a separate mount point or disk to store the binlog data. Binlog Data Written Hourly This metric shows the amount of data written hourly to the binlog files during the last 24 hours. This metric can give you an idea of how big is your application in terms of data writes (creation, modification, deletion). Binlog Count This metric shows the overall count of binary log files, on both primary and secondary servers. Binlogs Created Hourly This metric shows the number of binlog files created hourly during the last 24 hours. Relay Log Space This metric shows the overall size of the relay log files. It only applies to a secondary host. The relay log consists of a set of numbered files containing the events to be executed on the secondary host in order to replicate database changes. The relay log has the same format as the binlog. There can be multiple relay log files depending on the rotation policy adopted (using the configuration variable max_relay_log_size ). As soon as the SQL thread completes to execute all events in the relay log file, the file is deleted. If this metric contains a high value, the variable max_relay_log_file is high too. Generally, this not a serious issue. If the value of this metric is constantly increased, the secondary is delaying too much in applying the events. Treat this metric in the same way as the MySQL Replication Delay metric. Relay Log Written Hourly This metric shows the amount of data written hourly into relay log files during the last 24 hours.","title":"MySQL Replication Summary"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#io-thread-running","text":"This metric shows if the IO Thread is running or not. It only applies to a secondary host. SQL Thread is a process that runs on a secondary host in the replication environment. It reads the events from the local relay log file and applies them to the secondary server. Depending on the format of the binary log it can read query statements in plain text and re-execute them or it can read raw data and apply them to the local host. Possible values Yes The thread is running and is connected to a replication primary No The thread is not running because it is not launched yet or because an error has occurred connecting to the primary host Connecting The thread is running but is not connected to a replication primary No value The host is not configured to be a replication secondary IO Thread Running is one of the parameters that the command SHOW SLAVE STATUS returns.","title":"IO Thread Running"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#sql-thread-running","text":"This metric shows if the SQL thread is running or not. It only applies to a secondary host. Possible values Yes SQL Thread is running and is applying events from the relay log to the local secondary host No SQL Thread is not running because it is not launched yet or because of an error occurred while applying an event to the local secondary host","title":"SQL Thread Running"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#replication-error-no","text":"This metric shows the number of the last error in the SQL Thread encountered which caused replication to stop. One of the more common errors is Error: 1022 Duplicate Key Entry . In such a case replication is attempting to update a row that already exists on the secondary. The SQL Thread will stop replication in order to avoid data corruption.","title":"Replication Error No"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#read-only","text":"This metric indicates whether the host is configured to be in Read Only mode or not. Possible values Yes The secondary host permits no client updates except from users who have the SUPER privilege or the REPLICATION SLAVE privilege. This kind of configuration is typically used for secondary hosts in a replication environment to avoid a user can inadvertently or voluntarily modify data causing inconsistencies and stopping the replication process. No The secondary host is not configured in Read Only mode.","title":"Read only"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#mysql-replication-delay","text":"This metric shows the number of seconds the secondary host is delayed in replication applying events compared to when the primary host applied them, denoted by the Seconds_Behind_Master value, and only applies to a secondary host. Since the replication process applies the data modifications on the secondary asyncronously, it could happen that the secondary replicates events after some time. The main reasons are: Network round trip time - high latency links will lead to non-zero replication lag values. Single threaded nature of replication channels - primary servers have the advantage of applying changes in parallel, whereas secondary ones are only able to apply changes in serial, thus limiting their throughput. In some cases Group Commit can help but is not always applicable. High number of changed rows or computationally expensive SQL - depending on the replication format ( ROW vs STATEMENT ), significant changes to the database through high volume of rows modified, or expensive CPU will all contribute to secondary servers lagging behind the primary. Generally adding more CPU or Disk resources can alleviate replication lag issues, up to a point.","title":"MySQL Replication Delay"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#binlog-size","text":"This metric shows the overall size of the binary log files, which can exist on both primary and secondary servers. The binary log (also known as the binlog) contains events that describe database changes: CREATE TABLE , ALTER TABLE , updates, inserts, deletes and other statements or database changes. The binlog is the file that is read by secondaries via their IO Thread process in order to replicate database changes modification on the data and on the table structures. There can be more than one binlog file present depending on the binlog rotation policy adopted (for example using the configuration variables max_binlog_size and expire_logs_days ). Note There can be more binlog files depending on the rotation policy adopted (for example using the configuration variables max_binlog_size and expire_logs_days ) or even because of server reboots. When planning the disk space, take care of the overall dimension of binlog files and adopt a good rotation policy or think about having a separate mount point or disk to store the binlog data.","title":"Binlog Size"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#binlog-data-written-hourly","text":"This metric shows the amount of data written hourly to the binlog files during the last 24 hours. This metric can give you an idea of how big is your application in terms of data writes (creation, modification, deletion).","title":"Binlog Data Written Hourly"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#binlog-count","text":"This metric shows the overall count of binary log files, on both primary and secondary servers.","title":"Binlog Count"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#binlogs-created-hourly","text":"This metric shows the number of binlog files created hourly during the last 24 hours.","title":"Binlogs Created Hourly"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#relay-log-space","text":"This metric shows the overall size of the relay log files. It only applies to a secondary host. The relay log consists of a set of numbered files containing the events to be executed on the secondary host in order to replicate database changes. The relay log has the same format as the binlog. There can be multiple relay log files depending on the rotation policy adopted (using the configuration variable max_relay_log_size ). As soon as the SQL thread completes to execute all events in the relay log file, the file is deleted. If this metric contains a high value, the variable max_relay_log_file is high too. Generally, this not a serious issue. If the value of this metric is constantly increased, the secondary is delaying too much in applying the events. Treat this metric in the same way as the MySQL Replication Delay metric.","title":"Relay Log Space"},{"location":"details/dashboards/dashboard-mysql-replication-summary.html#relay-log-written-hourly","text":"This metric shows the amount of data written hourly into relay log files during the last 24 hours.","title":"Relay Log Written Hourly"},{"location":"details/dashboards/dashboard-mysql-table-details.html","text":"MySQL Table Details Largest Tables Largest Tables by Row Count The estimated number of rows in the table from information_schema.tables . Largest Tables by Size The size of the table components from information_schema.tables . Pie Total Database Size The total size of the database: as data + index size, so freeble one. Most Fragmented Tables by Freeable Size The list of 5 most fragmented tables ordered by their freeable size Table Activity The next two graphs are available only for Percona Server and MariaDB and require userstat variable turned on. Rows read The number of rows read from the table, shown for the top 5 tables. Rows Changed The number of rows changed in the table, shown for the top 5 tables. Auto Increment Usage The current value of an auto_increment column from information_schema , shown for the top 10 tables.","title":"MySQL Table Details"},{"location":"details/dashboards/dashboard-mysql-table-details.html#largest-tables","text":"Largest Tables by Row Count The estimated number of rows in the table from information_schema.tables . Largest Tables by Size The size of the table components from information_schema.tables .","title":"Largest Tables"},{"location":"details/dashboards/dashboard-mysql-table-details.html#pie","text":"Total Database Size The total size of the database: as data + index size, so freeble one. Most Fragmented Tables by Freeable Size The list of 5 most fragmented tables ordered by their freeable size","title":"Pie"},{"location":"details/dashboards/dashboard-mysql-table-details.html#table-activity","text":"The next two graphs are available only for Percona Server and MariaDB and require userstat variable turned on.","title":"Table Activity"},{"location":"details/dashboards/dashboard-mysql-table-details.html#rows-read","text":"The number of rows read from the table, shown for the top 5 tables.","title":"Rows read"},{"location":"details/dashboards/dashboard-mysql-table-details.html#rows-changed","text":"The number of rows changed in the table, shown for the top 5 tables.","title":"Rows Changed"},{"location":"details/dashboards/dashboard-mysql-table-details.html#auto-increment-usage","text":"The current value of an auto_increment column from information_schema , shown for the top 10 tables.","title":"Auto Increment Usage"},{"location":"details/dashboards/dashboard-mysql-tokudb-details.html","text":"MySQL TokuDB Details No description","title":"MySQL TokuDB Details"},{"location":"details/dashboards/dashboard-mysql-user-details.html","text":"MySQL User Details Note This dashboard requires Percona Server for MySQL 5.1+ or MariaDB 10.1/10.2 with XtraDB. Also userstat should be enabled, for example with the SET GLOBAL userstat=1 statement. See Setting up MySQL . Data is displayed for the 5 top users. Top Users by Connections Created The number of times user\u2019s connections connected using SSL to the server. Top Users by Traffic The number of bytes sent to the user\u2019s connections. Top Users by Rows Fetched/Read The number of rows fetched by the user\u2019s connections. Top Users by Rows Updated The number of rows updated by the user\u2019s connections. Top Users by Busy Time The cumulative number of seconds there was activity on connections from the user. Top Users by CPU Time The cumulative CPU time elapsed, in seconds, while servicing connections of the user.","title":"MySQL User Details"},{"location":"details/dashboards/dashboard-mysql-wait-event-analyses-details.html","text":"MySQL Wait Event Analyses Details This dashboard helps to analyze Performance Schema wait events. It plots the following metrics for the chosen (one or more) wait events: Count - Performance Schema Waits Load - Performance Schema Waits Avg Wait Time - Performance Schema Waits","title":"MySQL Wait Event Analyses Details"},{"location":"details/dashboards/dashboard-network-details.html","text":"Network Details Last Hour Statistic This section reports the inbound speed , outbound speed , traffic errors and drops , and retransmit rate . Network Traffic This section contains the Network traffic and network utilization hourly metrics. Network Traffic Details This section offers the following metrics: Network traffic by packets Network traffic errors Network traffic drop Network traffic multicust Network Netstat TCP This section offers the following metrics: Timeout value used for retransmitting Min TCP retransmission timeout Max TCP retransmission timeout Netstat: TCP TCP segments Network Netstat UDP In this section, you can find the following metrics: Netstat: UDP UDP Lite The graphs in the UDP Lite metric give statistics about: InDatagrams Packets received OutDatagrams Packets sent InCsumErrors Datagrams with checksum errors InErrors Datagrams that could not be delivered to an application RcvbufErrors Datagrams for which not enough socket buffer memory to receive SndbufErrors Datagrams for which not enough socket buffer memory to transmit NoPorts Datagrams received on a port with no listener ICMP This section has the following metrics: ICMP Errors Messages/Redirects Echos Timestamps/Mask Requests ICMP Errors InErrors Messages which the entity received but determined as having ICMP-specific errors (bad ICMP checksums, bad length, etc.) OutErrors Messages which this entity did not send due to problems discovered within ICMP, such as a lack of buffers InDestUnreachs Destination Unreachable messages received OutDestUnreachs Destination Unreachable messages sent InType3 Destination unreachable OutType3 Destination unreachable InCsumErrors Messages with ICMP checksum errors InTimeExcds Time Exceeded messages received Messages/Redirects InMsgs Messages which the entity received. Note that this counter includes all those counted by icmpInErrors InRedirects Redirect messages received OutMsgs Messages which this entity attempted to send. Note that this counter includes all those counted by icmpOutErrors OutRedirects Redirect messages sent. For a host, this object will always be zero, since hosts do not send redirects Echos InEchoReps Echo Reply messages received InEchos Echo (request) messages received OutEchoReps Echo Reply messages sent OutEchos Echo (request) messages sent Timestamps/Mask Requests InAddrMaskReps Address Mask Reply messages received InAddrMasks Address Mask Request messages received OutAddrMaskReps Address Mask Reply messages sent OutAddrMasks Address Mask Request messages sent InTimestampReps Timestamp Reply messages received InTimestamps Timestamp Request messages received OutTimestampReps Timestamp Reply messages sent OutTimestamps Timestamp Request messages sent","title":"Network Details"},{"location":"details/dashboards/dashboard-network-details.html#last-hour-statistic","text":"This section reports the inbound speed , outbound speed , traffic errors and drops , and retransmit rate .","title":"Last Hour Statistic"},{"location":"details/dashboards/dashboard-network-details.html#network-traffic","text":"This section contains the Network traffic and network utilization hourly metrics.","title":"Network Traffic"},{"location":"details/dashboards/dashboard-network-details.html#network-traffic-details","text":"This section offers the following metrics: Network traffic by packets Network traffic errors Network traffic drop Network traffic multicust","title":"Network Traffic Details"},{"location":"details/dashboards/dashboard-network-details.html#network-netstat-tcp","text":"This section offers the following metrics: Timeout value used for retransmitting Min TCP retransmission timeout Max TCP retransmission timeout Netstat: TCP TCP segments","title":"Network Netstat TCP"},{"location":"details/dashboards/dashboard-network-details.html#network-netstat-udp","text":"In this section, you can find the following metrics: Netstat: UDP UDP Lite The graphs in the UDP Lite metric give statistics about: InDatagrams Packets received OutDatagrams Packets sent InCsumErrors Datagrams with checksum errors InErrors Datagrams that could not be delivered to an application RcvbufErrors Datagrams for which not enough socket buffer memory to receive SndbufErrors Datagrams for which not enough socket buffer memory to transmit NoPorts Datagrams received on a port with no listener","title":"Network Netstat UDP"},{"location":"details/dashboards/dashboard-network-details.html#icmp","text":"This section has the following metrics: ICMP Errors Messages/Redirects Echos Timestamps/Mask Requests ICMP Errors InErrors Messages which the entity received but determined as having ICMP-specific errors (bad ICMP checksums, bad length, etc.) OutErrors Messages which this entity did not send due to problems discovered within ICMP, such as a lack of buffers InDestUnreachs Destination Unreachable messages received OutDestUnreachs Destination Unreachable messages sent InType3 Destination unreachable OutType3 Destination unreachable InCsumErrors Messages with ICMP checksum errors InTimeExcds Time Exceeded messages received Messages/Redirects InMsgs Messages which the entity received. Note that this counter includes all those counted by icmpInErrors InRedirects Redirect messages received OutMsgs Messages which this entity attempted to send. Note that this counter includes all those counted by icmpOutErrors OutRedirects Redirect messages sent. For a host, this object will always be zero, since hosts do not send redirects Echos InEchoReps Echo Reply messages received InEchos Echo (request) messages received OutEchoReps Echo Reply messages sent OutEchos Echo (request) messages sent Timestamps/Mask Requests InAddrMaskReps Address Mask Reply messages received InAddrMasks Address Mask Request messages received OutAddrMaskReps Address Mask Reply messages sent OutAddrMasks Address Mask Request messages sent InTimestampReps Timestamp Reply messages received InTimestamps Timestamp Request messages received OutTimestampReps Timestamp Reply messages sent OutTimestamps Timestamp Request messages sent","title":"ICMP"},{"location":"details/dashboards/dashboard-node-summary.html","text":"Node Summary System Summary The output from pt-summary , one of the Percona Toolkit utilities . CPU Usage The CPU time is measured in clock ticks or seconds. It is useful to measure CPU time as a percentage of the CPU\u2019s capacity, which is called the CPU usage. CPU Saturation and Max Core Usage When a system is running with maximum CPU utilization, the transmitting and receiving threads must all share the available CPU. This will cause data to be queued more frequently to cope with the lack of CPU. CPU Saturation may be measured as the length of a wait queue, or the time spent waiting on the queue. Interrupts and Context Switches Interrupt is an input signal to the processor indicating an event that needs immediate attention. An interrupt signal alerts the processor and serves as a request for the processor to interrupt the currently executing code, so that the event can be processed in a timely manner. Context switch is the process of storing the state of a process or thread, so that it can be restored and resume execution at a later point. This allows multiple processes to share a single CPU, and is an essential feature of a multitasking operating system. Processes No description Memory Utilization No description Virtual Memory Utilization No description Swap Space No description Swap Activity Swap Activity is memory management that involves swapping sections of memory to and from physical storage. I/O Activity Disk I/O includes read or write or input/output operations involving a physical disk. It is the speed with which the data transfer takes place between the hard disk drive and RAM. Global File Descriptors Usage No description Disk IO Latency Shows average latency for Reads and Writes IO Devices. Higher than typical latency for highly loaded storage indicates saturation (overload) and is frequent cause of performance problems. Higher than normal latency also can indicate internal storage problems. Disk IO Load Shows how much disk was loaded for reads or writes as average number of outstanding requests at different period of time. High disk load is a good measure of actual storage utilization. Different storage types handle load differently - some will show latency increases on low loads others can handle higher load with no problems. Network Traffic Network traffic refers to the amount of data moving across a network at a given point in time. Network Utilization Hourly No description Local Network Errors Total Number of Local Network Interface Transmit Errors, Receive Errors and Drops. Should be Zero TCP Retransmission Retransmission, essentially identical with Automatic repeat request (ARQ), is the resending of packets which have been either damaged or lost. Retransmission is one of the basic mechanisms used by protocols operating over a packet switched computer network to provide reliable communication (such as that provided by a reliable byte stream, for example TCP).","title":"Node Summary"},{"location":"details/dashboards/dashboard-node-summary.html#system-summary","text":"The output from pt-summary , one of the Percona Toolkit utilities .","title":"System Summary"},{"location":"details/dashboards/dashboard-node-summary.html#cpu-usage","text":"The CPU time is measured in clock ticks or seconds. It is useful to measure CPU time as a percentage of the CPU\u2019s capacity, which is called the CPU usage.","title":"CPU Usage"},{"location":"details/dashboards/dashboard-node-summary.html#cpu-saturation-and-max-core-usage","text":"When a system is running with maximum CPU utilization, the transmitting and receiving threads must all share the available CPU. This will cause data to be queued more frequently to cope with the lack of CPU. CPU Saturation may be measured as the length of a wait queue, or the time spent waiting on the queue.","title":"CPU Saturation and Max Core Usage"},{"location":"details/dashboards/dashboard-node-summary.html#interrupts-and-context-switches","text":"Interrupt is an input signal to the processor indicating an event that needs immediate attention. An interrupt signal alerts the processor and serves as a request for the processor to interrupt the currently executing code, so that the event can be processed in a timely manner. Context switch is the process of storing the state of a process or thread, so that it can be restored and resume execution at a later point. This allows multiple processes to share a single CPU, and is an essential feature of a multitasking operating system.","title":"Interrupts and Context Switches"},{"location":"details/dashboards/dashboard-node-summary.html#processes","text":"No description","title":"Processes"},{"location":"details/dashboards/dashboard-node-summary.html#memory-utilization","text":"No description","title":"Memory Utilization"},{"location":"details/dashboards/dashboard-node-summary.html#virtual-memory-utilization","text":"No description","title":"Virtual Memory Utilization"},{"location":"details/dashboards/dashboard-node-summary.html#swap-space","text":"No description","title":"Swap Space"},{"location":"details/dashboards/dashboard-node-summary.html#swap-activity","text":"Swap Activity is memory management that involves swapping sections of memory to and from physical storage.","title":"Swap Activity"},{"location":"details/dashboards/dashboard-node-summary.html#io-activity","text":"Disk I/O includes read or write or input/output operations involving a physical disk. It is the speed with which the data transfer takes place between the hard disk drive and RAM.","title":"I/O Activity"},{"location":"details/dashboards/dashboard-node-summary.html#global-file-descriptors-usage","text":"No description","title":"Global File Descriptors Usage"},{"location":"details/dashboards/dashboard-node-summary.html#disk-io-latency","text":"Shows average latency for Reads and Writes IO Devices. Higher than typical latency for highly loaded storage indicates saturation (overload) and is frequent cause of performance problems. Higher than normal latency also can indicate internal storage problems.","title":"Disk IO Latency"},{"location":"details/dashboards/dashboard-node-summary.html#disk-io-load","text":"Shows how much disk was loaded for reads or writes as average number of outstanding requests at different period of time. High disk load is a good measure of actual storage utilization. Different storage types handle load differently - some will show latency increases on low loads others can handle higher load with no problems.","title":"Disk IO Load"},{"location":"details/dashboards/dashboard-node-summary.html#network-traffic","text":"Network traffic refers to the amount of data moving across a network at a given point in time.","title":"Network Traffic"},{"location":"details/dashboards/dashboard-node-summary.html#network-utilization-hourly","text":"No description","title":"Network Utilization Hourly"},{"location":"details/dashboards/dashboard-node-summary.html#local-network-errors","text":"Total Number of Local Network Interface Transmit Errors, Receive Errors and Drops. Should be Zero","title":"Local Network Errors"},{"location":"details/dashboards/dashboard-node-summary.html#tcp-retransmission","text":"Retransmission, essentially identical with Automatic repeat request (ARQ), is the resending of packets which have been either damaged or lost. Retransmission is one of the basic mechanisms used by protocols operating over a packet switched computer network to provide reliable communication (such as that provided by a reliable byte stream, for example TCP).","title":"TCP Retransmission"},{"location":"details/dashboards/dashboard-node-temperature-details.html","text":"Node Temperature Details The Node Temperature Details dashboard exposes hardware monitoring and sensor data obtained through the sysfs virtual filesystem of the node. Hardware monitoring devices attached to the CPU and/or other chips on the motherboard let you monitor the hardware health of a system. Most modern systems include several of such devices. The actual list can include temperature sensors, voltage sensors, fan speed sensors, and various additional features, such as the ability to control the rotation speed of the fans. CPU Cores Temperatures Presents data taken from the temperature sensors of the CPU Chips Temperatures Presents data taken from the temperature sensors connected to other system controllers Fan Rotation Speeds Fan rotation speeds reported in RPM (rotations per minute). Fan Power Usage Describes the pulse width modulation of the PWN-equipped fans. PWM operates like a switch that constantly cycles on and off, thereby regulating the amount of power the fan gains: 100% makes it rotate at full speed, while lower percentage slows rotation down proportionally.","title":"Node Temperature Details"},{"location":"details/dashboards/dashboard-node-temperature-details.html#cpu-cores-temperatures","text":"Presents data taken from the temperature sensors of the CPU","title":"CPU Cores Temperatures"},{"location":"details/dashboards/dashboard-node-temperature-details.html#chips-temperatures","text":"Presents data taken from the temperature sensors connected to other system controllers","title":"Chips Temperatures"},{"location":"details/dashboards/dashboard-node-temperature-details.html#fan-rotation-speeds","text":"Fan rotation speeds reported in RPM (rotations per minute).","title":"Fan Rotation Speeds"},{"location":"details/dashboards/dashboard-node-temperature-details.html#fan-power-usage","text":"Describes the pulse width modulation of the PWN-equipped fans. PWM operates like a switch that constantly cycles on and off, thereby regulating the amount of power the fan gains: 100% makes it rotate at full speed, while lower percentage slows rotation down proportionally.","title":"Fan Power Usage"},{"location":"details/dashboards/dashboard-nodes-compare.html","text":"Nodes Compare This dashboard lets you compare a wide range of parameters. Parameters of the same type are shown side by side for all servers, grouped into the following sections: System Information CPU Memory Disk Partitions Disk Performance Network The System Information section shows the System Info summary of each server, as well as System Uptime , CPU Cores , RAM , Saturation Metrics , and Load Average gauges. The CPU section offers the CPU Usage , Interrupts , and Context Switches metrics. In the Memory section, you can find the Memory Usage , Swap Usage , and Swap Activity metrics. The Disk Partitions section encapsulates two metrics, Mountpoint Usage and Free Space . The Disk Performance section contains the I/O Activity , Disk Operations , Disk Bandwidth , Disk IO Utilization , Disk Latency , and Disk Load metrics. Finally, Network section shows Network Traffic , and Network Utilization Hourly metrics.","title":"Nodes Compare"},{"location":"details/dashboards/dashboard-nodes-overview.html","text":"Nodes Overview The Nodes Overview dashboard provides details about the efficiency of work of the following components. Each component is represented as a section in the dashboard. CPU Memory & Swap Disk Network The CPU section offers the CPU Usage , CPU Saturation and Max Core Usage , Interrupts and Context Switches , and Processes metrics. In the Memory section, you can find the Memory Utilization , Virtual Memory Utilization , Swap Space , and Swap Activity metrics. The Disk section contains the I/O Activity , Global File Descriptors Usage , Disk IO Latency , and Disk IO Load metrics. In the Network section, you can find the Network Traffic , Network Utilization Hourly , Local Network Errors , and TCP Retransmission metrics.","title":"Nodes Overview"},{"location":"details/dashboards/dashboard-numa-details.html","text":"NUMA Details For each node, this dashboard shows metrics related to Non-uniform memory access (NUMA). Memory Usage Remotes over time the total, used, and free memory. Free Memory Percent Shows the free memory as the ratio to the total available memory. NUMA Memory Usage Types Dirty Memory waiting to be written back to disk Bounce Memory used for block device bounce buffers Mapped Files which have been mmaped, such as libraries KernelStack The memory the kernel stack uses. This is not reclaimable. NUMA Allocation Hits Memory successfully allocated on this node as intended. NUMA Allocation Missed Memory missed is allocated on a node despite the process preferring some different node. Memory foreign is intended for a node, but actually allocated on some different node. Anonymous Memory Active Anonymous memory that has been used more recently and usually not swapped out. Inactive Anonymous memory that has not been used recently and can be swapped out. NUMA File (PageCache) Active(file) Pagecache memory that has been used more recently and usually not reclaimed until needed. Inactive(file) Pagecache memory that can be reclaimed without huge performance impact. Shared Memory Shmem Total used shared memory (shared between several processes, thus including RAM disks, SYS-V-IPC and BSD like SHMEM). HugePages Statistics Total Number of hugepages being allocated by the kernel (Defined with vm.nr_hugepages ). Free The number of hugepages not being allocated by a process Surp The number of hugepages in the pool above the value in vm.nr_hugepages . The maximum number of surplus hugepages is controlled by vm.nr_overcommit_hugepages . Local Processes Memory allocated on a node while a process was running on it. Remote Processes Memory allocated on a node while a process was running on some other node. Slab Memory Slab Allocation is a memory management mechanism intended for the efficient memory allocation of kernel objects. SReclaimable The part of the Slab that might be reclaimed (such as caches). SUnreclaim The part of the Slab that can\u2019t be reclaimed under memory pressure","title":"NUMA Details"},{"location":"details/dashboards/dashboard-numa-details.html#memory-usage","text":"Remotes over time the total, used, and free memory.","title":"Memory Usage"},{"location":"details/dashboards/dashboard-numa-details.html#free-memory-percent","text":"Shows the free memory as the ratio to the total available memory.","title":"Free Memory Percent"},{"location":"details/dashboards/dashboard-numa-details.html#numa-memory-usage-types","text":"Dirty Memory waiting to be written back to disk Bounce Memory used for block device bounce buffers Mapped Files which have been mmaped, such as libraries KernelStack The memory the kernel stack uses. This is not reclaimable.","title":"NUMA Memory Usage Types"},{"location":"details/dashboards/dashboard-numa-details.html#numa-allocation-hits","text":"Memory successfully allocated on this node as intended.","title":"NUMA Allocation Hits"},{"location":"details/dashboards/dashboard-numa-details.html#numa-allocation-missed","text":"Memory missed is allocated on a node despite the process preferring some different node. Memory foreign is intended for a node, but actually allocated on some different node.","title":"NUMA Allocation Missed"},{"location":"details/dashboards/dashboard-numa-details.html#anonymous-memory","text":"Active Anonymous memory that has been used more recently and usually not swapped out. Inactive Anonymous memory that has not been used recently and can be swapped out.","title":"Anonymous Memory"},{"location":"details/dashboards/dashboard-numa-details.html#numa-file-pagecache","text":"Active(file) Pagecache memory that has been used more recently and usually not reclaimed until needed. Inactive(file) Pagecache memory that can be reclaimed without huge performance impact.","title":"NUMA File (PageCache)"},{"location":"details/dashboards/dashboard-numa-details.html#shared-memory","text":"Shmem Total used shared memory (shared between several processes, thus including RAM disks, SYS-V-IPC and BSD like SHMEM).","title":"Shared Memory"},{"location":"details/dashboards/dashboard-numa-details.html#hugepages-statistics","text":"Total Number of hugepages being allocated by the kernel (Defined with vm.nr_hugepages ). Free The number of hugepages not being allocated by a process Surp The number of hugepages in the pool above the value in vm.nr_hugepages . The maximum number of surplus hugepages is controlled by vm.nr_overcommit_hugepages .","title":"HugePages Statistics"},{"location":"details/dashboards/dashboard-numa-details.html#local-processes","text":"Memory allocated on a node while a process was running on it.","title":"Local Processes"},{"location":"details/dashboards/dashboard-numa-details.html#remote-processes","text":"Memory allocated on a node while a process was running on some other node.","title":"Remote Processes"},{"location":"details/dashboards/dashboard-numa-details.html#slab-memory","text":"Slab Allocation is a memory management mechanism intended for the efficient memory allocation of kernel objects. SReclaimable The part of the Slab that might be reclaimed (such as caches). SUnreclaim The part of the Slab that can\u2019t be reclaimed under memory pressure","title":"Slab Memory"},{"location":"details/dashboards/dashboard-postgresql-instance-summary.html","text":"PostgreSQL Instance Summary Number of Temp Files Cumulative number of temporary files created by queries in this database since service start. All temporary files are counted, regardless of why the temporary file was created (e.g., sorting or hashing), and regardless of the log_temp_files setting. Size of Temp Files Cumulative amount of data written to temporary files by queries in this database since service start. All temporary files are counted, regardless of why the temporary file was created, and regardless of the log_temp_files setting. Temp Files Activity Number of temporary files created by queries in this database. All temporary files are counted, regardless of why the temporary file was created (e.g., sorting or hashing), and regardless of the log_temp_files setting. Temp Files Utilization Total amount of data written to temporary files by queries in this database. All temporary files are counted, regardless of why the temporary file was created, and regardless of the log_temp_files setting. Canceled Queries Based on pg_stat_database_conflicts view","title":"PostgreSQL Instance Summary"},{"location":"details/dashboards/dashboard-postgresql-instance-summary.html#number-of-temp-files","text":"Cumulative number of temporary files created by queries in this database since service start. All temporary files are counted, regardless of why the temporary file was created (e.g., sorting or hashing), and regardless of the log_temp_files setting.","title":"Number of Temp Files"},{"location":"details/dashboards/dashboard-postgresql-instance-summary.html#size-of-temp-files","text":"Cumulative amount of data written to temporary files by queries in this database since service start. All temporary files are counted, regardless of why the temporary file was created, and regardless of the log_temp_files setting.","title":"Size of Temp Files"},{"location":"details/dashboards/dashboard-postgresql-instance-summary.html#temp-files-activity","text":"Number of temporary files created by queries in this database. All temporary files are counted, regardless of why the temporary file was created (e.g., sorting or hashing), and regardless of the log_temp_files setting.","title":"Temp Files Activity"},{"location":"details/dashboards/dashboard-postgresql-instance-summary.html#temp-files-utilization","text":"Total amount of data written to temporary files by queries in this database. All temporary files are counted, regardless of why the temporary file was created, and regardless of the log_temp_files setting.","title":"Temp Files Utilization"},{"location":"details/dashboards/dashboard-postgresql-instance-summary.html#canceled-queries","text":"Based on pg_stat_database_conflicts view","title":"Canceled Queries"},{"location":"details/dashboards/dashboard-postgresql-instances-compare.html","text":"PostgreSQL Instances Compare No description","title":"PostgreSQL Instances Compare"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html","text":"PostgreSQL Instances Overview Connected Reports whether PMM Server can connect to the PostgreSQL instance. Version The version of the PostgreSQL instance. Shared Buffers Defines the amount of memory the database server uses for shared memory buffers. Default is 128MB . Guidance on tuning is 25% of RAM, but generally doesn\u2019t exceed 40% . Disk-Page Buffers The setting wal_buffers defines how much memory is used for caching the write-ahead log entries. Generally this value is small ( 3% of shared_buffers value), but it may need to be modified for heavily loaded servers. Memory Size for each Sort The parameter work_mem defines the amount of memory assigned for internal sort operations and hash tables before writing to temporary disk files. The default is 4MB . Disk Cache Size PostgreSQL\u2019s effective_cache_size variable tunes how much RAM you expect to be available for disk caching. Generally adding Linux free+cached will give you a good idea. This value is used by the query planner whether plans will fit in memory, and when defined too low, can lead to some plans rejecting certain indexes. Autovacuum Whether autovacuum process is enabled or not. Generally the solution is to vacuum more often, not less. PostgreSQL Connections Max Connections The maximum number of client connections allowed. Change this value with care as there are some memory resources that are allocated on a per-client basis, so setting max_connections higher will generally increase overall PostgreSQL memory usage. Connections The number of connection attempts (successful or not) to the PostgreSQL server. Active Connections The number of open connections to the PostgreSQL server. PostgreSQL Tuples Tuples The total number of rows processed by PostgreSQL server: fetched, returned, inserted, updated, and deleted. Read Tuple Activity The number of rows read from the database: as returned so fetched ones. Tuples Changed per 5min The number of rows changed in the last 5 minutes: inserted, updated, and deleted ones. PostgreSQL Transactions Transactions The total number of transactions that have been either been committed or rolled back. Duration of Transactions Maximum duration in seconds any active transaction has been running. Temp Files Number of Temp Files The number of temporary files created by queries. Size of Temp files The total amount of data written to temporary files by queries in bytes. Note All temporary files are taken into account by these two gauges, regardless of why the temporary file was created (e.g., sorting or hashing), and regardless of the log_temp_files setting. Conflicts and Locks Conflicts/Deadlocks The number of queries canceled due to conflicts with recovery in the database (due to dropped tablespaces, lock timeouts, old snapshots, pinned buffers, or deadlocks). Number of Locks The number of deadlocks detected by PostgreSQL. Buffers and Blocks Operations Operations with Blocks The time spent reading and writing data file blocks by backends, in milliseconds. Note Capturing read and write time statistics is possible only if track_io_timing setting is enabled. This can be done either in configuration file or with the following query executed on the running system: ALTER SYSTEM SET track_io_timing = ON ; SELECT pg_reload_conf (); Buffers The number of buffers allocated by PostgreSQL. Canceled Queries The number of queries that have been canceled due to dropped tablespaces, lock timeouts, old snapshots, pinned buffers, and deadlocks. Note Data shown by this gauge are based on the pg_stat_database_conflicts view. Cache Hit Ratio The number of times disk blocks were found already in the buffer cache, so that a read was not necessary. Note This only includes hits in the PostgreSQL buffer cache, not the operating system\u2019s file system cache. Checkpoint Stats The total amount of time that has been spent in the portion of checkpoint processing where files are either written or synchronized to disk, in milliseconds. PostgreSQL Settings The list of all settings of the PostgreSQL server. System Summary This section contains the following system parameters of the PostgreSQL server: CPU Usage, CPU Saturation and Max Core Usage, Disk I/O Activity, and Network Traffic.","title":"PostgreSQL Instances Overview"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#connected","text":"Reports whether PMM Server can connect to the PostgreSQL instance.","title":"Connected"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#version","text":"The version of the PostgreSQL instance.","title":"Version"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#shared-buffers","text":"Defines the amount of memory the database server uses for shared memory buffers. Default is 128MB . Guidance on tuning is 25% of RAM, but generally doesn\u2019t exceed 40% .","title":"Shared Buffers"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#disk-page-buffers","text":"The setting wal_buffers defines how much memory is used for caching the write-ahead log entries. Generally this value is small ( 3% of shared_buffers value), but it may need to be modified for heavily loaded servers.","title":"Disk-Page Buffers"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#memory-size-for-each-sort","text":"The parameter work_mem defines the amount of memory assigned for internal sort operations and hash tables before writing to temporary disk files. The default is 4MB .","title":"Memory Size for each Sort"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#disk-cache-size","text":"PostgreSQL\u2019s effective_cache_size variable tunes how much RAM you expect to be available for disk caching. Generally adding Linux free+cached will give you a good idea. This value is used by the query planner whether plans will fit in memory, and when defined too low, can lead to some plans rejecting certain indexes.","title":"Disk Cache Size"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#autovacuum","text":"Whether autovacuum process is enabled or not. Generally the solution is to vacuum more often, not less.","title":"Autovacuum"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#postgresql-connections","text":"Max Connections The maximum number of client connections allowed. Change this value with care as there are some memory resources that are allocated on a per-client basis, so setting max_connections higher will generally increase overall PostgreSQL memory usage. Connections The number of connection attempts (successful or not) to the PostgreSQL server. Active Connections The number of open connections to the PostgreSQL server.","title":"PostgreSQL Connections"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#postgresql-tuples","text":"Tuples The total number of rows processed by PostgreSQL server: fetched, returned, inserted, updated, and deleted. Read Tuple Activity The number of rows read from the database: as returned so fetched ones. Tuples Changed per 5min The number of rows changed in the last 5 minutes: inserted, updated, and deleted ones.","title":"PostgreSQL Tuples"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#postgresql-transactions","text":"Transactions The total number of transactions that have been either been committed or rolled back. Duration of Transactions Maximum duration in seconds any active transaction has been running.","title":"PostgreSQL Transactions"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#temp-files","text":"Number of Temp Files The number of temporary files created by queries. Size of Temp files The total amount of data written to temporary files by queries in bytes. Note All temporary files are taken into account by these two gauges, regardless of why the temporary file was created (e.g., sorting or hashing), and regardless of the log_temp_files setting.","title":"Temp Files"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#conflicts-and-locks","text":"Conflicts/Deadlocks The number of queries canceled due to conflicts with recovery in the database (due to dropped tablespaces, lock timeouts, old snapshots, pinned buffers, or deadlocks). Number of Locks The number of deadlocks detected by PostgreSQL.","title":"Conflicts and Locks"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#buffers-and-blocks-operations","text":"Operations with Blocks The time spent reading and writing data file blocks by backends, in milliseconds. Note Capturing read and write time statistics is possible only if track_io_timing setting is enabled. This can be done either in configuration file or with the following query executed on the running system: ALTER SYSTEM SET track_io_timing = ON ; SELECT pg_reload_conf (); Buffers The number of buffers allocated by PostgreSQL.","title":"Buffers and Blocks Operations"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#canceled-queries","text":"The number of queries that have been canceled due to dropped tablespaces, lock timeouts, old snapshots, pinned buffers, and deadlocks. Note Data shown by this gauge are based on the pg_stat_database_conflicts view.","title":"Canceled Queries"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#cache-hit-ratio","text":"The number of times disk blocks were found already in the buffer cache, so that a read was not necessary. Note This only includes hits in the PostgreSQL buffer cache, not the operating system\u2019s file system cache.","title":"Cache Hit Ratio"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#checkpoint-stats","text":"The total amount of time that has been spent in the portion of checkpoint processing where files are either written or synchronized to disk, in milliseconds.","title":"Checkpoint Stats"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#postgresql-settings","text":"The list of all settings of the PostgreSQL server.","title":"PostgreSQL Settings"},{"location":"details/dashboards/dashboard-postgresql-instances-overview.html#system-summary","text":"This section contains the following system parameters of the PostgreSQL server: CPU Usage, CPU Saturation and Max Core Usage, Disk I/O Activity, and Network Traffic.","title":"System Summary"},{"location":"details/dashboards/dashboard-processes-details.html","text":"Processes Details The Processes Details dashboard displays Linux process information - PIDs, Threads, and Processes. The dashboard shows how many processes/threads are either in the kernel run queue (runnable state) or in the blocked queue (waiting for I/O). When the number of process in the runnable state is constantly higher than the number of CPU cores available, the load is CPU bound. When the number of process blocked waiting for I/O is large, the load is disk bound. The running average of the sum of these two quantities is the basis of the loadavg metric. The dashboard consists of two parts: the first section describes metrics for all hosts, and the second part provides charts for each host. Charts for all hosts, available in the first section, are the following ones: States of Processes Number of PIDs Percentage of Max PIDs Limit Number of Threads Percentage of Max Threads Limit Runnable Processes Blocked Processes Waiting for I/O Sleeping Processes Running Processes Disk Sleep Processes Stopped Processes Zombie Processes Dead Processes The following charts are present in the second part, available for each host: Processes States of Processes Number of PIDs Percentage of Max PIDs Limit Number of Threads Percentage of Max Threads Limit Number of PIDs No description Percentage of Max PIDs Limit No description Number of Threads No description Percentage of Max Threads Limit No description Runnable Processes Processes The Processes graph shows how many processes/threads are either in the kernel run queue (runnable state) or in the blocked queue (waiting for I/O). When the number of process in the runnable state is constantly higher than the number of CPU cores available, the load is CPU bound. When the number of process blocked waiting for I/O is large, the load is disk bound. The running average of the sum of these two quantities is the basis of the loadavg metric. Blocked Processes Waiting for I/O Processes The Processes graph shows how many processes/threads are either in the kernel run queue (runnable state) or in the blocked queue (waiting for I/O). When the number of process in the runnable state is constantly higher than the number of CPU cores available, the load is CPU bound. When the number of process blocked waiting for I/O is large, the load is disk bound. The running average of the sum of these two quantities is the basis of the loadavg metric. Sleeping Processes No description Running Processes No description Disk Sleep Processes No description Stopped Processes No description Zombie Processes No description Dead Processes No description","title":"Processes Details"},{"location":"details/dashboards/dashboard-processes-details.html#number-of-pids","text":"No description","title":"Number of PIDs"},{"location":"details/dashboards/dashboard-processes-details.html#percentage-of-max-pids-limit","text":"No description","title":"Percentage of Max PIDs Limit"},{"location":"details/dashboards/dashboard-processes-details.html#number-of-threads","text":"No description","title":"Number of Threads"},{"location":"details/dashboards/dashboard-processes-details.html#percentage-of-max-threads-limit","text":"No description","title":"Percentage of Max Threads Limit"},{"location":"details/dashboards/dashboard-processes-details.html#runnable-processes","text":"Processes The Processes graph shows how many processes/threads are either in the kernel run queue (runnable state) or in the blocked queue (waiting for I/O). When the number of process in the runnable state is constantly higher than the number of CPU cores available, the load is CPU bound. When the number of process blocked waiting for I/O is large, the load is disk bound. The running average of the sum of these two quantities is the basis of the loadavg metric.","title":"Runnable Processes"},{"location":"details/dashboards/dashboard-processes-details.html#blocked-processes-waiting-for-io","text":"Processes The Processes graph shows how many processes/threads are either in the kernel run queue (runnable state) or in the blocked queue (waiting for I/O). When the number of process in the runnable state is constantly higher than the number of CPU cores available, the load is CPU bound. When the number of process blocked waiting for I/O is large, the load is disk bound. The running average of the sum of these two quantities is the basis of the loadavg metric.","title":"Blocked Processes Waiting for I/O"},{"location":"details/dashboards/dashboard-processes-details.html#sleeping-processes","text":"No description","title":"Sleeping Processes"},{"location":"details/dashboards/dashboard-processes-details.html#running-processes","text":"No description","title":"Running Processes"},{"location":"details/dashboards/dashboard-processes-details.html#disk-sleep-processes","text":"No description","title":"Disk Sleep Processes"},{"location":"details/dashboards/dashboard-processes-details.html#stopped-processes","text":"No description","title":"Stopped Processes"},{"location":"details/dashboards/dashboard-processes-details.html#zombie-processes","text":"No description","title":"Zombie Processes"},{"location":"details/dashboards/dashboard-processes-details.html#dead-processes","text":"No description","title":"Dead Processes"},{"location":"details/dashboards/dashboard-prometheus-exporter-status.html","text":"Prometheus Exporter Status The Prometheus Exporter Status dashboard reports the consumption of resources by the Prometheus exporters used by PMM. For each exporter, this dashboard reveals the following information: CPU usage Memory usage File descriptors used Exporter uptime","title":"Prometheus Exporter Status"},{"location":"details/dashboards/dashboard-prometheus-exporters-overview.html","text":"Prometheus Exporters Overview Prometheus Exporters Summary This section provides a summary of how exporters are used across the selected hosts. It includes the average usage of CPU and memory as well as the number of hosts being monitored and the total number of running exporters. Avg CPU Usage per Host Shows the average CPU usage in percent per host for all exporters. Avg Memory Usage per Host Shows the Exporters average Memory usage per host. Monitored Hosts Shows the number of monitored hosts that are running Exporters. Exporters Running Shows the total number of Exporters running with this PMM Server instance. Note The CPU usage and memory usage do not include the additional CPU and memory usage required to produce metrics by the application or operating system. Prometheus Exporters Resource Usage by Node This section shows how resources, such as CPU and memory, are being used by the exporters for the selected hosts. CPU Usage Plots the Exporters\u2019 CPU usage across each monitored host (by default, All hosts). Memory Usage Plots the Exporters\u2019 Memory usage across each monitored host (by default, All hosts). Prometheus Exporters Resource Usage by Type This section shows how resources, such as CPU and memory, are being used by the exporters for host types: MySQL, MongoDB, ProxySQL, and the system. CPU Cores Used Shows the Exporters\u2019 CPU Cores used for each type of Exporter. Memory Usage Shows the Exporters\u2019 memory used for each type of Exporter. List of Hosts At the bottom, this dashboard shows details for each running host. CPU Used Show the CPU usage as a percentage for all Exporters. Mem Used Shows total Memory Used by Exporters. Exporters Running Shows the number of Exporters running. RAM Shows the total amount of RAM of the host. Virtual CPUs Shows the total number of virtual CPUs on the host. You can click the value of the CPU Used , Memory Used , or Exporters Running columns to open the Prometheus Exporter Status dashboard for further analysis. See also Percona blog: Understand Your Prometheus Exporters with Percona Monitoring and Management (PMM)","title":"Prometheus Exporters Overview"},{"location":"details/dashboards/dashboard-prometheus-exporters-overview.html#prometheus-exporters-summary","text":"This section provides a summary of how exporters are used across the selected hosts. It includes the average usage of CPU and memory as well as the number of hosts being monitored and the total number of running exporters. Avg CPU Usage per Host Shows the average CPU usage in percent per host for all exporters. Avg Memory Usage per Host Shows the Exporters average Memory usage per host. Monitored Hosts Shows the number of monitored hosts that are running Exporters. Exporters Running Shows the total number of Exporters running with this PMM Server instance. Note The CPU usage and memory usage do not include the additional CPU and memory usage required to produce metrics by the application or operating system.","title":"Prometheus Exporters Summary"},{"location":"details/dashboards/dashboard-prometheus-exporters-overview.html#prometheus-exporters-resource-usage-by-node","text":"This section shows how resources, such as CPU and memory, are being used by the exporters for the selected hosts. CPU Usage Plots the Exporters\u2019 CPU usage across each monitored host (by default, All hosts). Memory Usage Plots the Exporters\u2019 Memory usage across each monitored host (by default, All hosts).","title":"Prometheus Exporters Resource Usage by Node"},{"location":"details/dashboards/dashboard-prometheus-exporters-overview.html#prometheus-exporters-resource-usage-by-type","text":"This section shows how resources, such as CPU and memory, are being used by the exporters for host types: MySQL, MongoDB, ProxySQL, and the system. CPU Cores Used Shows the Exporters\u2019 CPU Cores used for each type of Exporter. Memory Usage Shows the Exporters\u2019 memory used for each type of Exporter.","title":"Prometheus Exporters Resource Usage by Type"},{"location":"details/dashboards/dashboard-prometheus-exporters-overview.html#list-of-hosts","text":"At the bottom, this dashboard shows details for each running host. CPU Used Show the CPU usage as a percentage for all Exporters. Mem Used Shows total Memory Used by Exporters. Exporters Running Shows the number of Exporters running. RAM Shows the total amount of RAM of the host. Virtual CPUs Shows the total number of virtual CPUs on the host. You can click the value of the CPU Used , Memory Used , or Exporters Running columns to open the Prometheus Exporter Status dashboard for further analysis. See also Percona blog: Understand Your Prometheus Exporters with Percona Monitoring and Management (PMM)","title":"List of Hosts"},{"location":"details/dashboards/dashboard-proxysql-instance-summary.html","text":"ProxySQL Instance Summary Network Traffic Network traffic refers to the amount of data moving across a network at a given point in time.","title":"ProxySQL Instance Summary"},{"location":"details/dashboards/dashboard-proxysql-instance-summary.html#network-traffic","text":"Network traffic refers to the amount of data moving across a network at a given point in time.","title":"Network Traffic"},{"location":"details/dashboards/dashboard-pxc-galera-cluster-summary.html","text":"PXC/Galera Cluster Summary No description","title":"PXC/Galera Cluster Summary"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html","text":"PXC/Galera Node Summary Galera Replication Latency Shows figures for the replication latency on group communication. It measures latency from the time point when a message is sent out to the time point when a message is received. As replication is a group operation, this essentially gives you the slowest ACK and longest RTT in the cluster. Galera Replication Queues Shows the length of receive and send queues. Galera Cluster Size Shows the number of members currently connected to the cluster. Galera Flow Control Shows the number of FC_PAUSE events sent/received. They are sent by a node when its replication queue gets too full. If a node is sending out FC messages it indicates a problem. Galera Parallelization Efficiency Shows the average distances between highest and lowest seqno that are concurrently applied, committed and can be possibly applied in parallel (potential degree of parallelization). Galera Writing Conflicts Shows the number of local transactions being committed on this node that failed certification (some other node had a commit that conflicted with ours) \u2013 client received deadlock error on commit and also the number of local transactions in flight on this node that were aborted because they locked something an applier thread needed \u2013 deadlock error anywhere in an open transaction. Spikes in the graph may indicate writing to the same table potentially the same rows from 2 nodes. Available Downtime before SST Required Shows for how long the node can be taken out of the cluster before SST is required. SST is a full state transfer method. Galera Writeset Count Shows the count of transactions received from the cluster (any other node) and replicated to the cluster (from this node). Galera Writeset Size Shows the average transaction size received/replicated. Galera Writeset Traffic Shows the bytes of data received from the cluster (any other node) and replicated to the cluster (from this node). Galera Network Usage Hourly Shows the bytes of data received from the cluster (any other node) and replicated to the cluster (from this node).","title":"PXC/Galera Node Summary"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-replication-latency","text":"Shows figures for the replication latency on group communication. It measures latency from the time point when a message is sent out to the time point when a message is received. As replication is a group operation, this essentially gives you the slowest ACK and longest RTT in the cluster.","title":"Galera Replication Latency"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-replication-queues","text":"Shows the length of receive and send queues.","title":"Galera Replication Queues"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-cluster-size","text":"Shows the number of members currently connected to the cluster.","title":"Galera Cluster Size"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-flow-control","text":"Shows the number of FC_PAUSE events sent/received. They are sent by a node when its replication queue gets too full. If a node is sending out FC messages it indicates a problem.","title":"Galera Flow Control"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-parallelization-efficiency","text":"Shows the average distances between highest and lowest seqno that are concurrently applied, committed and can be possibly applied in parallel (potential degree of parallelization).","title":"Galera Parallelization Efficiency"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-writing-conflicts","text":"Shows the number of local transactions being committed on this node that failed certification (some other node had a commit that conflicted with ours) \u2013 client received deadlock error on commit and also the number of local transactions in flight on this node that were aborted because they locked something an applier thread needed \u2013 deadlock error anywhere in an open transaction. Spikes in the graph may indicate writing to the same table potentially the same rows from 2 nodes.","title":"Galera Writing Conflicts"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#available-downtime-before-sst-required","text":"Shows for how long the node can be taken out of the cluster before SST is required. SST is a full state transfer method.","title":"Available Downtime before SST Required"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-writeset-count","text":"Shows the count of transactions received from the cluster (any other node) and replicated to the cluster (from this node).","title":"Galera Writeset Count"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-writeset-size","text":"Shows the average transaction size received/replicated.","title":"Galera Writeset Size"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-writeset-traffic","text":"Shows the bytes of data received from the cluster (any other node) and replicated to the cluster (from this node).","title":"Galera Writeset Traffic"},{"location":"details/dashboards/dashboard-pxc-galera-node-summary.html#galera-network-usage-hourly","text":"Shows the bytes of data received from the cluster (any other node) and replicated to the cluster (from this node).","title":"Galera Network Usage Hourly"},{"location":"details/dashboards/dashboard-pxc-galera-nodes-compare.html","text":"PXC/Galera Nodes Compare $cluster - Galera Cluster Size Shows the number of members currently connected to the cluster.","title":"PXC/Galera Nodes Compare"},{"location":"details/dashboards/dashboard-pxc-galera-nodes-compare.html#cluster-galera-cluster-size","text":"Shows the number of members currently connected to the cluster.","title":"$cluster - Galera Cluster Size"},{"location":"details/dashboards/dashboard-victoriametrics-agents-overview.html","text":"VictoriaMetrics Agents Overview No description","title":"VictoriaMetrics Agents Overview"},{"location":"details/dashboards/dashboard-victoriametrics.html","text":"VictoriaMetrics No description","title":"VictoriaMetrics"},{"location":"how-to/index.html","text":"How to: Overview Configure The PMM Settings page Secure Securing your PMM installation Upgrade Upgrading PMM Server via the user interface Optimize Improving PMM performance","title":"How to: Overview"},{"location":"how-to/configure.html","text":"Configure The PMM Settings page lets you configure a number of PMM options. Diagnostics Metrics resolution Advanced Settings Data Retention Telemetry Check for updates Security Threat Tool DBaaS Integrated Alerting Public Address SSH Key Alertmanager integration Percona Platform Login Sign up Communication Email Slack Open the PMM Settings page with one of: the main menu: choose PMM\u2192PMM Settings search dashboards by name: type PMM Settings and click the search result On the left of the page is a set of sub-page selector tabs. (The Communication tab remains hidden until Integrated Alerting is activated.) Tip Click Apply changes after changing settings. Diagnostics Common to all sections is Diagnostics . PMM can generate a set of diagnostics data which can be examined and/or shared with Percona Support in case of some issue to solve it faster. You can get collected logs from PMM Server by clicking Download server diagnostics . Metrics resolution Metrics are collected at three intervals representing low, medium and high resolutions. Short time intervals are regarded as high resolution metrics, while those at longer time intervals are low resolution. The Metrics Resolution radio button lets you select one of four presets. Rare , Standard and Frequent are fixed presets. Custom is an editable preset. Each preset is a group of Low, Medium and High metrics resolution values. A low resolution interval increases the time between collection, resulting in low-resolution metrics and lower disk usage. A high resolution interval decreases the time between collection, resulting in high-resolution metrics and higher disk usage. The default values (in seconds) for the fixed presets and their resolution names are: Preset Low Medium High Rare 300 180 60 Standard 60 10 5 Frequent 30 5 1 Values for the Custom preset can be entered as values, or changed with the arrows. Note If there is poor network connectivity between PMM Server and PMM Client, or between PMM Client and the database server it is monitoring, scraping every second may not be possible when the network latency is greater than 1 second. Advanced Settings Data Retention Data retention specifies how long data is stored by PMM Server. Telemetry The Telemetry switch enables gathering and sending basic anonymous data to Percona, which helps us to determine where to focus the development and what is the uptake of the various versions of PMM. Specifically, gathering this information helps determine if we need to release patches to legacy versions beyond support, determining when supporting a particular version is no longer necessary, and even understanding how the frequency of release encourages or deters adoption. Currently, only the following information is gathered: PMM Version, Installation Method (Docker, AMI, OVF), the Server Uptime. We do not gather anything that would make the system identifiable, but the following two things are to be mentioned: The Country Code is evaluated from the submitting IP address before it is discarded. We do create an \u201cinstance ID\u201d - a random string generated using UUID v4. This instance ID is generated to distinguish new instances from existing ones, for figuring out instance upgrades. The first telemetry reporting of a new PMM Server instance is delayed by 24 hours to allow sufficient time to disable the service for those that do not wish to share any information. There is a landing page for this service, available at check.percona.com , which clearly explains what this service is, what it\u2019s collecting, and how you can turn it off. Grafana\u2019s anonymous usage statistics is not managed by PMM. To activate it, you must change the PMM Server container configuration after each update. As well as via the PMM Settings page, you can also disable telemetry with the -e DISABLE_TELEMETRY=1 option in your docker run statement for the PMM Server. Notes If the Security Threat Tool is enabled in PMM Settings, Telemetry is automatically enabled. Telemetry is sent immediately; the 24-hour grace period is not honored. Check for updates When active, PMM will automatically check for updates and put a notification in the Updates dashboard if any are available. Security Threat Tool The Security Threat Tool performs a range of security-related checks on a registered instance and reports the findings. It is disabled by default. It can be enabled in PMM\u2192PMM Settings\u2192Settings\u2192Advanced Settings\u2192Security Threat Tool . The checks are re-fetched and re-run every 24 hours. The results can be viewed in PMM\u2192PMM Database Checks . DBaaS A read-only setting that shows whether DBaaS features are activated on this server. Caution DBaaS functionality is a technical preview that must be turned on with a server feature flag. See Setting up a development environment for DBaaS . Integrated Alerting Enables Integrated Alerting and reveals the Communication tab. Public Address Public address for accessing DBaaS features on this server. SSH Key This section lets you upload your public SSH key to access the PMM Server via SSH (for example, when accessing PMM Server as a virtual appliance ). Enter your public key in the SSH Key field and click Apply SSH Key . Alertmanager integration Alertmanager manages alerts, deduplicating, grouping, and routing them to the appropriate receiver or display component. This section lets you configure integration of VictoriaMetrics with an external Alertmanager. The Alertmanager URL field should contain the URL of the Alertmanager which would serve your PMM alerts. The Alerting rules field is used to specify alerting rules in the YAML configuration format. Fill both fields and click the Apply Alertmanager settings button to proceed. Percona Platform This panel is where you create, and log into and out of your Percona Platform account. Login If you have a Percona Platform account, enter your credentials and click Login . Click Sign out to log out of your Percona Platform account. Sign up To create a Percona Platform account: Click Sign up Enter a valid email address in the Email field Choose and enter a strong password in the Password field Select the check box acknowledging our terms of service and privacy policy Click Sign up A brief message will confirm the creation of your new account and you may now log in with these credentials. Note Your Percona Platform account is separate from your PMM User account. Communication Note This tab appears only when Advanced Settings \u2192 Integrated Alerting is on. Global communications settings for Integrated Alerting . Integrated Alerting uses a separate instance of Alertmanager run by pmm-managed . The descriptions for the settings here are reproduced from Prometheus Alertmanager configuration . Email Settings for the SMTP email server: Server Address : The default SMTP smarthost used for sending emails, including port number. From : The default SMTP From header field. Username : SMTP Auth using CRAM-MD5, LOGIN and PLAIN. Password : SMTP Auth using LOGIN and PLAIN. Hello : The default hostname to identify to the SMTP server. Identity : SMTP Auth using PLAIN. Secret : SMTP Auth using CRAM-MD5. Slack Settings for Slack notifications: URL : The Slack webhook URL to use for Slack notifications.","title":"Configure"},{"location":"how-to/configure.html#diagnostics","text":"Common to all sections is Diagnostics . PMM can generate a set of diagnostics data which can be examined and/or shared with Percona Support in case of some issue to solve it faster. You can get collected logs from PMM Server by clicking Download server diagnostics .","title":"Diagnostics"},{"location":"how-to/configure.html#metrics-resolution","text":"Metrics are collected at three intervals representing low, medium and high resolutions. Short time intervals are regarded as high resolution metrics, while those at longer time intervals are low resolution. The Metrics Resolution radio button lets you select one of four presets. Rare , Standard and Frequent are fixed presets. Custom is an editable preset. Each preset is a group of Low, Medium and High metrics resolution values. A low resolution interval increases the time between collection, resulting in low-resolution metrics and lower disk usage. A high resolution interval decreases the time between collection, resulting in high-resolution metrics and higher disk usage. The default values (in seconds) for the fixed presets and their resolution names are: Preset Low Medium High Rare 300 180 60 Standard 60 10 5 Frequent 30 5 1 Values for the Custom preset can be entered as values, or changed with the arrows. Note If there is poor network connectivity between PMM Server and PMM Client, or between PMM Client and the database server it is monitoring, scraping every second may not be possible when the network latency is greater than 1 second.","title":"Metrics resolution"},{"location":"how-to/configure.html#advanced-settings","text":"","title":"Advanced Settings"},{"location":"how-to/configure.html#data-retention","text":"Data retention specifies how long data is stored by PMM Server.","title":"Data Retention"},{"location":"how-to/configure.html#telemetry","text":"The Telemetry switch enables gathering and sending basic anonymous data to Percona, which helps us to determine where to focus the development and what is the uptake of the various versions of PMM. Specifically, gathering this information helps determine if we need to release patches to legacy versions beyond support, determining when supporting a particular version is no longer necessary, and even understanding how the frequency of release encourages or deters adoption. Currently, only the following information is gathered: PMM Version, Installation Method (Docker, AMI, OVF), the Server Uptime. We do not gather anything that would make the system identifiable, but the following two things are to be mentioned: The Country Code is evaluated from the submitting IP address before it is discarded. We do create an \u201cinstance ID\u201d - a random string generated using UUID v4. This instance ID is generated to distinguish new instances from existing ones, for figuring out instance upgrades. The first telemetry reporting of a new PMM Server instance is delayed by 24 hours to allow sufficient time to disable the service for those that do not wish to share any information. There is a landing page for this service, available at check.percona.com , which clearly explains what this service is, what it\u2019s collecting, and how you can turn it off. Grafana\u2019s anonymous usage statistics is not managed by PMM. To activate it, you must change the PMM Server container configuration after each update. As well as via the PMM Settings page, you can also disable telemetry with the -e DISABLE_TELEMETRY=1 option in your docker run statement for the PMM Server. Notes If the Security Threat Tool is enabled in PMM Settings, Telemetry is automatically enabled. Telemetry is sent immediately; the 24-hour grace period is not honored.","title":"Telemetry"},{"location":"how-to/configure.html#check-for-updates","text":"When active, PMM will automatically check for updates and put a notification in the Updates dashboard if any are available.","title":"Check for updates"},{"location":"how-to/configure.html#security-threat-tool","text":"The Security Threat Tool performs a range of security-related checks on a registered instance and reports the findings. It is disabled by default. It can be enabled in PMM\u2192PMM Settings\u2192Settings\u2192Advanced Settings\u2192Security Threat Tool . The checks are re-fetched and re-run every 24 hours. The results can be viewed in PMM\u2192PMM Database Checks .","title":"Security Threat Tool"},{"location":"how-to/configure.html#dbaas","text":"A read-only setting that shows whether DBaaS features are activated on this server. Caution DBaaS functionality is a technical preview that must be turned on with a server feature flag. See Setting up a development environment for DBaaS .","title":"DBaaS"},{"location":"how-to/configure.html#integrated-alerting","text":"Enables Integrated Alerting and reveals the Communication tab.","title":"Integrated Alerting"},{"location":"how-to/configure.html#public-address","text":"Public address for accessing DBaaS features on this server.","title":"Public Address"},{"location":"how-to/configure.html#ssh-key","text":"This section lets you upload your public SSH key to access the PMM Server via SSH (for example, when accessing PMM Server as a virtual appliance ). Enter your public key in the SSH Key field and click Apply SSH Key .","title":"SSH Key"},{"location":"how-to/configure.html#alertmanager-integration","text":"Alertmanager manages alerts, deduplicating, grouping, and routing them to the appropriate receiver or display component. This section lets you configure integration of VictoriaMetrics with an external Alertmanager. The Alertmanager URL field should contain the URL of the Alertmanager which would serve your PMM alerts. The Alerting rules field is used to specify alerting rules in the YAML configuration format. Fill both fields and click the Apply Alertmanager settings button to proceed.","title":"Alertmanager integration"},{"location":"how-to/configure.html#percona-platform","text":"This panel is where you create, and log into and out of your Percona Platform account.","title":"Percona Platform"},{"location":"how-to/configure.html#login","text":"If you have a Percona Platform account, enter your credentials and click Login . Click Sign out to log out of your Percona Platform account.","title":"Login"},{"location":"how-to/configure.html#sign-up","text":"To create a Percona Platform account: Click Sign up Enter a valid email address in the Email field Choose and enter a strong password in the Password field Select the check box acknowledging our terms of service and privacy policy Click Sign up A brief message will confirm the creation of your new account and you may now log in with these credentials. Note Your Percona Platform account is separate from your PMM User account.","title":"Sign up"},{"location":"how-to/configure.html#communication","text":"Note This tab appears only when Advanced Settings \u2192 Integrated Alerting is on. Global communications settings for Integrated Alerting . Integrated Alerting uses a separate instance of Alertmanager run by pmm-managed . The descriptions for the settings here are reproduced from Prometheus Alertmanager configuration .","title":"Communication"},{"location":"how-to/configure.html#email","text":"Settings for the SMTP email server: Server Address : The default SMTP smarthost used for sending emails, including port number. From : The default SMTP From header field. Username : SMTP Auth using CRAM-MD5, LOGIN and PLAIN. Password : SMTP Auth using LOGIN and PLAIN. Hello : The default hostname to identify to the SMTP server. Identity : SMTP Auth using PLAIN. Secret : SMTP Auth using CRAM-MD5.","title":"Email"},{"location":"how-to/configure.html#slack","text":"Settings for Slack notifications: URL : The Slack webhook URL to use for Slack notifications.","title":"Slack"},{"location":"how-to/optimize.html","text":"Optimize Improving PMM Performance with Table Statistics Options If a MySQL instance has a lot of schemas or tables, there are two options to help improve the performance of PMM when adding instances with pmm-admin add : --disable-tablestats and --disable-tablestats-limit . Note These settings can only be used when adding an instance. To change them, you must remove and re-add the instances. You can only use one of these options when adding an instance. Disable per-table statistics for an instance When adding an instance with pmm-admin add , the --disable-tablestats option disables table statistics collection when there are more than the default number (1000) of tables in the instance. USAGE sudo pmm-admin add mysql --disable-tablestats Change the number of tables beyond which per-table statistics is disabled When adding an instance with pmm-admin add , the --disable-tablestats-limit option changes the number of tables (from the default of 1000) beyond which per-table statistics collection is disabled. USAGE sudo pmm-admin add mysql --disable-tablestats-limit = <LIMIT> EXAMPLE Add a MySQL instance, disabling per-table statistics collection when the number of tables in the instance reaches 2000. sudo pmm-admin add mysql --disable-tablestats-limit = 2000","title":"Optimize"},{"location":"how-to/optimize.html#improving-pmm-performance-with-table-statistics-options","text":"If a MySQL instance has a lot of schemas or tables, there are two options to help improve the performance of PMM when adding instances with pmm-admin add : --disable-tablestats and --disable-tablestats-limit . Note These settings can only be used when adding an instance. To change them, you must remove and re-add the instances. You can only use one of these options when adding an instance.","title":"Improving PMM Performance with Table Statistics Options"},{"location":"how-to/optimize.html#disable-per-table-statistics-for-an-instance","text":"When adding an instance with pmm-admin add , the --disable-tablestats option disables table statistics collection when there are more than the default number (1000) of tables in the instance.","title":"Disable per-table statistics for an instance"},{"location":"how-to/optimize.html#usage","text":"sudo pmm-admin add mysql --disable-tablestats","title":"USAGE"},{"location":"how-to/optimize.html#change-the-number-of-tables-beyond-which-per-table-statistics-is-disabled","text":"When adding an instance with pmm-admin add , the --disable-tablestats-limit option changes the number of tables (from the default of 1000) beyond which per-table statistics collection is disabled.","title":"Change the number of tables beyond which per-table statistics is disabled"},{"location":"how-to/optimize.html#usage_1","text":"sudo pmm-admin add mysql --disable-tablestats-limit = <LIMIT>","title":"USAGE"},{"location":"how-to/optimize.html#example","text":"Add a MySQL instance, disabling per-table statistics collection when the number of tables in the instance reaches 2000. sudo pmm-admin add mysql --disable-tablestats-limit = 2000","title":"EXAMPLE"},{"location":"how-to/secure.html","text":"Secure You can improve the security of your PMM installation with: SSL encryption to secure traffic between client and server; Grafana HTTPS secure cookies To see which security features are enabled: pmm-admin status Tip You can gain an extra level of security by keeping PMM Server isolated from the internet, if possible. SSL encryption You need valid SSL certificates to encrypt traffic between client and server. With our Docker, OVF and AMI images, self-signed certificates are in /srv/nginx . To use your own, you can either: mount the local certificate directory to the same location, or, copy your certificates to a running PMM Server container. Mounting certificates For example, if your own certificates are in /etc/pmm-certs : docker run -d -p 443 :443 --volumes-from pmm-data \\ --name pmm-server -v /etc/pmm-certs:/srv/nginx \\ --restart always percona/pmm-server:2 Note The certificates must be owned by root. You can do this with: sudo chown 0:0 /etc/pmm-certs/* The mounted certificate directory ( /etc/pmm-certs in this example) must contain the files certificate.crt , certificate.key , ca-certs.pem and dhparam.pem . For SSL encryption, the container must publish on port 443 instead of 80. Copying certificates If PMM Server is running as a Docker image, use docker cp to copy certificates. This example copies certificate files from the current working directory to a running PMM Server docker container. docker cp certificate.crt pmm-server:/srv/nginx/certificate.crt docker cp certificate.key pmm-server:/srv/nginx/certificate.key docker cp ca-certs.pem pmm-server:/srv/nginx/ca-certs.pem docker cp dhparam.pem pmm-server:/srv/nginx/dhparam.pem Enabling SSL when connecting PMM Client to PMM Server pmm-admin config --server-url=https://<user>:<password>@<server IP> --server-insecure-tls Grafana HTTPS secure cookies To enable: Start a shell within the Docker container: docker exec -it pmm-server bash Edit /etc/grafana/grafana.ini Enable cookie_secure and set the value to true Restart Grafana: supervisorctl restart grafana","title":"Secure"},{"location":"how-to/secure.html#ssl-encryption","text":"You need valid SSL certificates to encrypt traffic between client and server. With our Docker, OVF and AMI images, self-signed certificates are in /srv/nginx . To use your own, you can either: mount the local certificate directory to the same location, or, copy your certificates to a running PMM Server container.","title":"SSL encryption"},{"location":"how-to/secure.html#mounting-certificates","text":"For example, if your own certificates are in /etc/pmm-certs : docker run -d -p 443 :443 --volumes-from pmm-data \\ --name pmm-server -v /etc/pmm-certs:/srv/nginx \\ --restart always percona/pmm-server:2 Note The certificates must be owned by root. You can do this with: sudo chown 0:0 /etc/pmm-certs/* The mounted certificate directory ( /etc/pmm-certs in this example) must contain the files certificate.crt , certificate.key , ca-certs.pem and dhparam.pem . For SSL encryption, the container must publish on port 443 instead of 80.","title":"Mounting certificates"},{"location":"how-to/secure.html#copying-certificates","text":"If PMM Server is running as a Docker image, use docker cp to copy certificates. This example copies certificate files from the current working directory to a running PMM Server docker container. docker cp certificate.crt pmm-server:/srv/nginx/certificate.crt docker cp certificate.key pmm-server:/srv/nginx/certificate.key docker cp ca-certs.pem pmm-server:/srv/nginx/ca-certs.pem docker cp dhparam.pem pmm-server:/srv/nginx/dhparam.pem","title":"Copying certificates"},{"location":"how-to/secure.html#enabling-ssl-when-connecting-pmm-client-to-pmm-server","text":"pmm-admin config --server-url=https://<user>:<password>@<server IP> --server-insecure-tls","title":"Enabling SSL when connecting PMM Client to PMM Server"},{"location":"how-to/secure.html#grafana-https-secure-cookies","text":"To enable: Start a shell within the Docker container: docker exec -it pmm-server bash Edit /etc/grafana/grafana.ini Enable cookie_secure and set the value to true Restart Grafana: supervisorctl restart grafana","title":"Grafana HTTPS secure cookies"},{"location":"how-to/upgrade.html","text":"Upgrade Updating a Server Client and server components are installed and updated separately. PMM Server can run natively, as a Docker image, a virtual appliance, or an AWS cloud instance. Each has its own installation and update steps. The preferred and simplest way to update PMM Server is with the PMM Upgrade panel on the Home page. The panel shows: the current server version and release date; whether the server is up to date; the last time a check was made for updates. Click the refresh button to manually check for updates. If one is available, click the update button to update to the version indicated.","title":"Upgrade"},{"location":"how-to/upgrade.html#updating-a-server","text":"Client and server components are installed and updated separately. PMM Server can run natively, as a Docker image, a virtual appliance, or an AWS cloud instance. Each has its own installation and update steps. The preferred and simplest way to update PMM Server is with the PMM Upgrade panel on the Home page. The panel shows: the current server version and release date; whether the server is up to date; the last time a check was made for updates. Click the refresh button to manually check for updates. If one is available, click the update button to update to the version indicated.","title":"Updating a Server"},{"location":"release-notes/index.html","text":"Release Notes Percona Monitoring and Management 2.13.0 Percona Monitoring and Management 2.12.0 Percona Monitoring and Management 2.11.1 Percona Monitoring and Management 2.11.0 Percona Monitoring and Management 2.10.1 Percona Monitoring and Management 2.10.0 Percona Monitoring and Management 2.9.1 Percona Monitoring and Management 2.9.0 Percona Monitoring and Management 2.8.0 Percona Monitoring and Management 2.7.0 Percona Monitoring and Management 2.6.1 Percona Monitoring and Management 2.6.0 Percona Monitoring and Management 2.5.0 Percona Monitoring and Management 2.4.0 Percona Monitoring and Management 2.3.0 Percona Monitoring and Management 2.2.2 Percona Monitoring and Management 2.2.1 Percona Monitoring and Management 2.2.0 Percona Monitoring and Management 2.1.0 Percona Monitoring and Management 2.0.1 Percona Monitoring and Management 2.0.0","title":"Release Notes"},{"location":"release-notes/2.0.0.html","text":"Percona Monitoring and Management 2.0.0 Date: September 19, 2019 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. You can run PMM in your own environment for maximum security and reliability. It provides thorough time-based analysis for MySQL, MongoDB, and PostgreSQL servers to ensure that your data works as efficiently as possible. For install instructions, see Installing Percona Monitoring and Management . Note PMM 2 is designed to be used as a new installation \u2014 please don\u2019t try to upgrade your existing PMM 1 environment. The new PMM2 introduces a number of enhancements and additional feature improvements, including: Detailed query analytics and filtering technologies which enable you to identify issues faster than ever before. A better user experience: Service-level dashboards give you immediate access to the data you need. The new addition of PostgreSQL query tuning. Enhanced security protocols to ensure your data is safe. Our new API allows you to extend and interact with third-party tools. More details about new and improved features available within the release can be found in the corresponding blog post . Help us improve our software quality by reporting any Percona Monitoring and Management bugs you encounter using our bug tracking system .","title":"Percona Monitoring and Management 2.0.0"},{"location":"release-notes/2.0.1.html","text":"Percona Monitoring and Management 2.0.1 Date: October 9, 2019 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. You can run PMM in your own environment for maximum security and reliability. It provides thorough time-based analysis for MySQL, MongoDB, and PostgreSQL servers to ensure that your data works as efficiently as possible. For install instructions, see Installing Percona Monitoring and Management . Note PMM 2 is designed to be used as a new installation \u2014 please don\u2019t try to upgrade your existing PMM 1 environment. Improvements PMM-4779 : Securely share dashboards with Percona PMM-4735 : Keep one old slowlog file after rotation PMM-4724 : Alt+click on check updates button enables force-update PMM-4444 : Return \u201cwhat\u2019s new\u201d URL with the information extracted from the pmm-update package changelog Fixed bugs PMM-4758 : Remove Inventory rows from dashboards PMM-4757 : qan_mysql_perfschema_agent failed querying events_statements_summary_by_digest due to data types conversion PMM-4755 : Fixed a typo in the InnoDB AHI Miss Ratio formula PMM-4749 : Navigation from Dashboards to QAN when some Node or Service was selected now applies filtering by them in QAN PMM-4742 : General information links were updated to go to PMM 2 related pages PMM-4739 : Remove request instances list PMM-4734 : A fix was made for the collecting node_name formula at MySQL Replication Summary dashboard PMM-4729 : Fixes were made for formulas on MySQL Instances Overview PMM-4726 : Links to services in MongoDB singlestats didn\u2019t show Node name PMM-4720 : machine_id could contain trailing \\\\n PMM-4640 : It was not possible to add MongoDB remotely if password contained a # symbol Help us improve our software quality by reporting any Percona Monitoring and Management bugs you encounter using our bug tracking system .","title":"Percona Monitoring and Management 2.0.1"},{"location":"release-notes/2.0.1.html#improvements","text":"PMM-4779 : Securely share dashboards with Percona PMM-4735 : Keep one old slowlog file after rotation PMM-4724 : Alt+click on check updates button enables force-update PMM-4444 : Return \u201cwhat\u2019s new\u201d URL with the information extracted from the pmm-update package changelog","title":"Improvements"},{"location":"release-notes/2.0.1.html#fixed-bugs","text":"PMM-4758 : Remove Inventory rows from dashboards PMM-4757 : qan_mysql_perfschema_agent failed querying events_statements_summary_by_digest due to data types conversion PMM-4755 : Fixed a typo in the InnoDB AHI Miss Ratio formula PMM-4749 : Navigation from Dashboards to QAN when some Node or Service was selected now applies filtering by them in QAN PMM-4742 : General information links were updated to go to PMM 2 related pages PMM-4739 : Remove request instances list PMM-4734 : A fix was made for the collecting node_name formula at MySQL Replication Summary dashboard PMM-4729 : Fixes were made for formulas on MySQL Instances Overview PMM-4726 : Links to services in MongoDB singlestats didn\u2019t show Node name PMM-4720 : machine_id could contain trailing \\\\n PMM-4640 : It was not possible to add MongoDB remotely if password contained a # symbol Help us improve our software quality by reporting any Percona Monitoring and Management bugs you encounter using our bug tracking system .","title":"Fixed bugs"},{"location":"release-notes/2.1.0.html","text":"Percona Monitoring and Management 2.1.0 Date: November 11, 2019 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. You can run PMM in your own environment for maximum security and reliability. It provides thorough time-based analysis for MySQL, MongoDB, and PostgreSQL servers to ensure that your data works as efficiently as possible. For install instructions, see Installing Percona Monitoring and Management . Note PMM 2 is designed to be used as a new installation \u2014 please don\u2019t try to upgrade your existing PMM 1 environment. Improvements and new features PMM-4063 : Update QAN filter panel to show only labels available for selection under currently applied filters PMM-815 : Latency Detail graph added to the MongoDB Instance Summary dashboard PMM-4768 : Disable heavy-load collectors automatically when there are too many tables PMM-4821 : Use color gradient in filled graphs on all dashboards PMM-4733 : Add more log and config files to the downloadable logs.zip archive PMM-4672 : Use integer percentage values in QAN filter panel PMM-4857 : Update tooltips for all MongoDB dashboards PMM-4616 : Rename column in the Query Details section in QAN from Total to Sum PMM-4770 : Use Go 1.12.10 PMM-4780 : Update Grafana to version 6.4.1 PMM-4918 : Update Grafana plugins to newer versions, including the clickhouse-datasource plugin Fixed bugs PMM-4935 : Wrong instance name displayed on the MySQL Instance Summary dashboard due to the incorrect string crop PMM-4916 : Wrong values are shown when changing the time range for the Node Summary Dashboard in case of remote instances PMM-4895 and PMM-4814 : The update process reports completion before it is actually done and therefore some dashboards, etc. may not be updated PMM-4876 : PMM Server access credentials are shown by the pmm-admin status command instead of hiding them for security reasons PMM-4875 : PostgreSQL error log gets flooded with warnings when pg_stat_statements extension is not installed in the database used by PMM Server or when PostgreSQL user is unable to connect to it PMM-4852 : Node name has an incorrect value if the Home dashboard opened after QAN PMM-4847 : Drilldowns from the Environment Overview dashboard doesn\u2019t show data for the pre-selected host PMM-4841 and PMM-4845 : pg_stat_statement QAN Agent leaks database connections PMM-4831 : Clean-up representation of selectors names on MySQL-related dashboards for a better consistency PMM-4824 : Incorrectly calculated singlestat values on MySQL Instances Overview dashboard PMM-4819 : In case of the only one monitored host, its uptime is shown as a smaller value than the all hosts uptime due to the inaccurate rounding PMM-4816 : Set equal thresholds to avoid confusing singlestat color differences on a Home dashboard PMM-4718 : Labels are not fully displayed in the filter panel of the Query Details section in QAN PMM-4545 : Long queries are not fully visible in the Query Examples section in QAN Help us improve our software quality by reporting any Percona Monitoring and Management bugs you encounter using our bug tracking system .","title":"Percona Monitoring and Management 2.1.0"},{"location":"release-notes/2.1.0.html#improvements-and-new-features","text":"PMM-4063 : Update QAN filter panel to show only labels available for selection under currently applied filters PMM-815 : Latency Detail graph added to the MongoDB Instance Summary dashboard PMM-4768 : Disable heavy-load collectors automatically when there are too many tables PMM-4821 : Use color gradient in filled graphs on all dashboards PMM-4733 : Add more log and config files to the downloadable logs.zip archive PMM-4672 : Use integer percentage values in QAN filter panel PMM-4857 : Update tooltips for all MongoDB dashboards PMM-4616 : Rename column in the Query Details section in QAN from Total to Sum PMM-4770 : Use Go 1.12.10 PMM-4780 : Update Grafana to version 6.4.1 PMM-4918 : Update Grafana plugins to newer versions, including the clickhouse-datasource plugin","title":"Improvements and new features"},{"location":"release-notes/2.1.0.html#fixed-bugs","text":"PMM-4935 : Wrong instance name displayed on the MySQL Instance Summary dashboard due to the incorrect string crop PMM-4916 : Wrong values are shown when changing the time range for the Node Summary Dashboard in case of remote instances PMM-4895 and PMM-4814 : The update process reports completion before it is actually done and therefore some dashboards, etc. may not be updated PMM-4876 : PMM Server access credentials are shown by the pmm-admin status command instead of hiding them for security reasons PMM-4875 : PostgreSQL error log gets flooded with warnings when pg_stat_statements extension is not installed in the database used by PMM Server or when PostgreSQL user is unable to connect to it PMM-4852 : Node name has an incorrect value if the Home dashboard opened after QAN PMM-4847 : Drilldowns from the Environment Overview dashboard doesn\u2019t show data for the pre-selected host PMM-4841 and PMM-4845 : pg_stat_statement QAN Agent leaks database connections PMM-4831 : Clean-up representation of selectors names on MySQL-related dashboards for a better consistency PMM-4824 : Incorrectly calculated singlestat values on MySQL Instances Overview dashboard PMM-4819 : In case of the only one monitored host, its uptime is shown as a smaller value than the all hosts uptime due to the inaccurate rounding PMM-4816 : Set equal thresholds to avoid confusing singlestat color differences on a Home dashboard PMM-4718 : Labels are not fully displayed in the filter panel of the Query Details section in QAN PMM-4545 : Long queries are not fully visible in the Query Examples section in QAN Help us improve our software quality by reporting any Percona Monitoring and Management bugs you encounter using our bug tracking system .","title":"Fixed bugs"},{"location":"release-notes/2.10.0.html","text":"Percona Monitoring and Management 2.10.0 Date: September 15, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. New Features PMM-2045 : New dashboard: MySQL Group Replication Summary PMM-5738 : Enhanced exporter: replaced original mongodb-exporter with a completely rewritten one with improved functionality PMM-5126 : Query Analytics Dashboard: Search by query substring or dimension (Thanks to user debug for reporting this issue) PMM-6360 : Grafana Upgrade to 7.1.3 PMM-6355 : Upgrade Prometheus to v2.19.3 PMM-6597 : Documentation: Updated Image rendering instructions for PMM PMM-6568 : Reusable user interface component: Pop-up dialog. Allows for more consistent intefaces across PMM PMM-6375 , PMM-6373 , PMM-6372 : Sign in, Sign up and Sign out UI for Percona Account inside PMM Server PMM-6328 : Query Analytics Dashboard: Mouse-over crosshair shows value on sparklines PMM-3831 : Node Summary Dashboard: Add pt-summary output to dashboard to provide details on system status and configuration Improvements PMM-6647 : MongoDB dashboards: RockDB Details removed, MMAPv1 & Cluster Summary changed PMM-6536 : Query Analytics Dashboard: Improved filter/time search message when no results PMM-6467 : PMM Settings: User-friendly error message PMM-5947 : Bind services to internal address for containers Bugs Fixed PMM-6336 : Suppress sensitive data: honor pmm-admin flag \u2018\u2013disable-queryexamples\u2019 when used in conjunction with \u2018\u2013query-source=perfschema\u2019 PMM-6244 : MySQL InnoDB Details Dashboard: Inverted color scheme on \u201cBP Write Buffering\u201d panel PMM-6294 : Query Analytics Dashboard doesn\u2019t resize well for some screen resolutions (Thanks to user debug for reporting this issue) PMM-5701 : Home Dashboard: Incorrect metric for \u2018DB uptime\u2019 (Thanks to user hubi_oediv for reporting this issue) PMM-6427 : Query Analytics dashboard: Examples broken when switching from MongoDB to MySQL query PMM-5684 : Use actual data from INFORMATION_SCHEMA vs relying on cached data (which can be 24 hrs old by default) PMM-6500 : PMM Database Checks: Unwanted high-contrast styling PMM-6440 : MongoDB ReplSet Summary Dashboard: Primary shows more lag than replicas PMM-6436 : Query Analytics Dashboard: Styles updated to conform with upgrade to Grafana 7.x PMM-6415 : Node Summary Dashboard: Redirection to database\u2019s Instance Summary dashboard omits Service Name PMM-6324 : Query Analytics Dashboard: Showing stale data while fetching updated data for query details section PMM-6316 : Query Analytics Dashboard: Inconsistent scrollbar styles PMM-6276 : PMM Inventory: Long lists unclear; poor contrast & column headings scroll out of view PMM-6529 : Query Analytics filter input margin disappears after scrolling Known Issues PMM-6643 : High CPU usage for new MongoDB exporter (fixed in Percona Monitoring and Management 2.10.1 )","title":"Percona Monitoring and Management 2.10.0"},{"location":"release-notes/2.10.0.html#new-features","text":"PMM-2045 : New dashboard: MySQL Group Replication Summary PMM-5738 : Enhanced exporter: replaced original mongodb-exporter with a completely rewritten one with improved functionality PMM-5126 : Query Analytics Dashboard: Search by query substring or dimension (Thanks to user debug for reporting this issue) PMM-6360 : Grafana Upgrade to 7.1.3 PMM-6355 : Upgrade Prometheus to v2.19.3 PMM-6597 : Documentation: Updated Image rendering instructions for PMM PMM-6568 : Reusable user interface component: Pop-up dialog. Allows for more consistent intefaces across PMM PMM-6375 , PMM-6373 , PMM-6372 : Sign in, Sign up and Sign out UI for Percona Account inside PMM Server PMM-6328 : Query Analytics Dashboard: Mouse-over crosshair shows value on sparklines PMM-3831 : Node Summary Dashboard: Add pt-summary output to dashboard to provide details on system status and configuration","title":"New Features"},{"location":"release-notes/2.10.0.html#improvements","text":"PMM-6647 : MongoDB dashboards: RockDB Details removed, MMAPv1 & Cluster Summary changed PMM-6536 : Query Analytics Dashboard: Improved filter/time search message when no results PMM-6467 : PMM Settings: User-friendly error message PMM-5947 : Bind services to internal address for containers","title":"Improvements"},{"location":"release-notes/2.10.0.html#bugs-fixed","text":"PMM-6336 : Suppress sensitive data: honor pmm-admin flag \u2018\u2013disable-queryexamples\u2019 when used in conjunction with \u2018\u2013query-source=perfschema\u2019 PMM-6244 : MySQL InnoDB Details Dashboard: Inverted color scheme on \u201cBP Write Buffering\u201d panel PMM-6294 : Query Analytics Dashboard doesn\u2019t resize well for some screen resolutions (Thanks to user debug for reporting this issue) PMM-5701 : Home Dashboard: Incorrect metric for \u2018DB uptime\u2019 (Thanks to user hubi_oediv for reporting this issue) PMM-6427 : Query Analytics dashboard: Examples broken when switching from MongoDB to MySQL query PMM-5684 : Use actual data from INFORMATION_SCHEMA vs relying on cached data (which can be 24 hrs old by default) PMM-6500 : PMM Database Checks: Unwanted high-contrast styling PMM-6440 : MongoDB ReplSet Summary Dashboard: Primary shows more lag than replicas PMM-6436 : Query Analytics Dashboard: Styles updated to conform with upgrade to Grafana 7.x PMM-6415 : Node Summary Dashboard: Redirection to database\u2019s Instance Summary dashboard omits Service Name PMM-6324 : Query Analytics Dashboard: Showing stale data while fetching updated data for query details section PMM-6316 : Query Analytics Dashboard: Inconsistent scrollbar styles PMM-6276 : PMM Inventory: Long lists unclear; poor contrast & column headings scroll out of view PMM-6529 : Query Analytics filter input margin disappears after scrolling","title":"Bugs Fixed"},{"location":"release-notes/2.10.0.html#known-issues","text":"PMM-6643 : High CPU usage for new MongoDB exporter (fixed in Percona Monitoring and Management 2.10.1 )","title":"Known Issues"},{"location":"release-notes/2.10.1.html","text":"Percona Monitoring and Management 2.10.1 Date: September 22, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Bugs Fixed PMM-6643 : New MongoDB exporter has higher CPU usage compared with old","title":"Percona Monitoring and Management 2.10.1"},{"location":"release-notes/2.10.1.html#bugs-fixed","text":"PMM-6643 : New MongoDB exporter has higher CPU usage compared with old","title":"Bugs Fixed"},{"location":"release-notes/2.11.0.html","text":"Percona Monitoring and Management 2.11.0 Date: October 14, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. New Features PMM-6567 : Technical preview of new PostgreSQL extension \u2018pg_stat_monitor\u2019 PMM-6515 : Link added directly to Node/Service page from Query Analytics filters, opens in new window Improvements PMM-6727 : Grafana plugin updates: grafana-polystat-panel=1.2.2, grafana-piechart-panel=1.6.1 PMM-6625 : Default sort to \u201cAverage - descending\u201d on all dashboards PMM-6609 : MySQL Instances Compare & Summary dashboards: Changed metric in \u2018MySQL Internal Memory Overview\u2019 PMM-6598 : Dashboard image sharing (Share Panel): Improved wording with link to configuration instructions PMM-6557 : Update Prometheus to v2.21.0 PMM-6554 : MySQL InnoDB Details dashboard: Add \u201csync flushing\u201d to \u201cInnodb Flushing by Type\u201d Bugs Fixed PMM-4547 : MongoDB dashboard replication lag count incorrect (Thanks to user vvol for reporting this issue) PMM-6639 : Integrated update does not detect all container types PMM-6765 : Tables information tab reports \u2018table not found\u2019 with new PostgreSQL extension \u2018pg_stat_monitor\u2019 PMM-6764 : Query Analytics: cannot filter items that are hidden - must use \u201cShow all\u201d PMM-6742 : Upgrade via PMM UI stalls (on yum update pmm-update) PMM-6689 : No PostgreSQL queries or metrics in Query Analytics with PostgreSQL 13 (postgresql_pgstatements_agent in Waiting status) PMM-6738 : PostgreSQL examples shown despite \u2018\u2013disable-queryexamples\u2019 option PMM-6535 : Unable to open \u2018Explore\u2019 in new window from Grafana menu PMM-6532 : Click-through URLs lose time ranges when redirecting to other dashboards PMM-6531 : Counter-intuitive coloring of element \u201cUpdate Stats when Metadata Queried\u201d PMM-6645 : Clean up unnecessary errors in logs (vertamedia-clickhouse-datasource plugin) PMM-6547 : Hexagonal graph tooltip text overflows bounding box","title":"Percona Monitoring and Management 2.11.0"},{"location":"release-notes/2.11.0.html#new-features","text":"PMM-6567 : Technical preview of new PostgreSQL extension \u2018pg_stat_monitor\u2019 PMM-6515 : Link added directly to Node/Service page from Query Analytics filters, opens in new window","title":"New Features"},{"location":"release-notes/2.11.0.html#improvements","text":"PMM-6727 : Grafana plugin updates: grafana-polystat-panel=1.2.2, grafana-piechart-panel=1.6.1 PMM-6625 : Default sort to \u201cAverage - descending\u201d on all dashboards PMM-6609 : MySQL Instances Compare & Summary dashboards: Changed metric in \u2018MySQL Internal Memory Overview\u2019 PMM-6598 : Dashboard image sharing (Share Panel): Improved wording with link to configuration instructions PMM-6557 : Update Prometheus to v2.21.0 PMM-6554 : MySQL InnoDB Details dashboard: Add \u201csync flushing\u201d to \u201cInnodb Flushing by Type\u201d","title":"Improvements"},{"location":"release-notes/2.11.0.html#bugs-fixed","text":"PMM-4547 : MongoDB dashboard replication lag count incorrect (Thanks to user vvol for reporting this issue) PMM-6639 : Integrated update does not detect all container types PMM-6765 : Tables information tab reports \u2018table not found\u2019 with new PostgreSQL extension \u2018pg_stat_monitor\u2019 PMM-6764 : Query Analytics: cannot filter items that are hidden - must use \u201cShow all\u201d PMM-6742 : Upgrade via PMM UI stalls (on yum update pmm-update) PMM-6689 : No PostgreSQL queries or metrics in Query Analytics with PostgreSQL 13 (postgresql_pgstatements_agent in Waiting status) PMM-6738 : PostgreSQL examples shown despite \u2018\u2013disable-queryexamples\u2019 option PMM-6535 : Unable to open \u2018Explore\u2019 in new window from Grafana menu PMM-6532 : Click-through URLs lose time ranges when redirecting to other dashboards PMM-6531 : Counter-intuitive coloring of element \u201cUpdate Stats when Metadata Queried\u201d PMM-6645 : Clean up unnecessary errors in logs (vertamedia-clickhouse-datasource plugin) PMM-6547 : Hexagonal graph tooltip text overflows bounding box","title":"Bugs Fixed"},{"location":"release-notes/2.11.1.html","text":"Percona Monitoring and Management 2.11.1 Date: October 19, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Bugs Fixed PMM-6782 : High CPU usage after update to 2.11.0","title":"Percona Monitoring and Management 2.11.1"},{"location":"release-notes/2.11.1.html#bugs-fixed","text":"PMM-6782 : High CPU usage after update to 2.11.0","title":"Bugs Fixed"},{"location":"release-notes/2.12.0.html","text":"Percona Monitoring and Management 2.12.0 Date: December 1, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Release Highlights VictoriaMetrics replaces Prometheus and is now the default datasource. VictoriaMetrics supports both PUSH (client to server) and PULL metrics collection modes. ( Read more. ) PMM Client can be run as a Docker image. The \u2018Add Instance\u2019 page and forms have been redesigned and look much better. New Features PMM-5799 : PMM Client now available as docker image in addition to RPM, DEB and .tgz PMM-6968 : Integrated Alerting: Basic notification channels actions API Create, Read, Update, Delete PMM-6842 : VictoriaMetrics: Grafana dashboards to monitor VictoriaMetricsDB as replacement for dashboards that used to monitor Prometheus DB PMM-6395 : Replace Prometheus with VictoriaMetrics in PMM for better performance and additional functionality Improvements PMM-6744 : Prevent timeout of low resolution metrics in MySQL instances with many tables (~1000\u2019s) PMM-6504 : MySQL Replication Summary: MySQL Replication Delay graph not factoring in value of intentionally set SQL_Delay thus inflating time displayed PMM-6820 : \u2018pmm-admin status \u2013wait\u2019 option added to allow for configurable delay in checking status of pmm-agent PMM-6710 : pmm-admin: Allow user-specified custom \u2018group\u2019 name when adding external services PMM-6825 : Allow user to specify \u2018listen address\u2019 to pmm-agent otherwise default to 127.0.0.1 PMM-6793 : Improve user experience of \u2018add remote instance\u2019 workflow PMM-6759 : Enable kubernetes startup probes to get status of pmm-agent using \u2018GET HTTP\u2019 verb PMM-6736 : MongoDB Instance Summary dashboard: Ensure colors for ReplSet status matches those in MongoDB ReplSet Summary dashboard for better consistency PMM-6730 : Node Overview/Summary Cleanup: Remove duplicate service type \u2018DB Service Connections\u2019 PMM-6542 : PMM Add Instance: Redesign page for more intuitive experience when adding various instance types to monitoring PMM-6518 : Update default datasource name from \u2018Prometheus\u2019 to \u2018Metrics\u2019 to ensure graphs are populated correctly after upgrade to VictoriaMetrics PMM-6428 : Query Analytics dashboard - Ensure user-selected filter selections are always visible even if they don\u2019t appear in top 5 results PMM-5020 : PMM Add Remote Instance: User can specify \u2018Table Statistics Limit\u2019 for MySQL and AWS RDS MySQL to disable table stat metrics which can have an adverse impact on performance with too many tables Bugs Fixed PMM-6811 : MongoDB Cluster Summary: when secondary optime is newer than primary optime, lag incorrectly shows 136 years PMM-6650 : Custom queries for MySQL 8 fail on 5.x (on update to pmm-agent 2.10) (Thanks to user debug for reporting this issue) PMM-6751 : PXC/Galera dashboards: Empty service name with MySQL version < 5.6.40 PMM-5823 : PMM Server: Timeout when simultaneously generating and accessing logs via download or API PMM-4547 : MongoDB dashboard replication lag count incorrect (Thanks to user vvol for reporting this issue) PMM-7057 : MySQL Instances Overview: Many monitored instances (~250+) gives \u2018too long query\u2019 error PMM-6883 : Query Analytics: \u2018Reset All\u2019 and \u2018Show Selected\u2019 filters behaving incorrectly PMM-6686 : Query Analytics: Filters panel blank on Microsoft Edge 44.18362.449.0 PMM-6007 : PMM Server virtual appliance\u2019s IP address not shown in OVF console PMM-6754 : Query Analytics: Bad alignment of percentage values in Filters panel PMM-6752 : Query Analytics: Time interval not preserved when using filter panel dashboard shortcuts PMM-6664 : Query Analytics: No horizontal scroll bar on Explain tab PMM-6632 : Node Summary - Virtual Memory Utilization chart: incorrect formulas PMM-6537 : MySQL InnoDB Details - Logging - Group Commit Batch Size: giving incorrect description PMM-6055 : PMM Inventory - Services: \u2018Service Type\u2019 column empty when it should be \u2018External\u2019 for external services Known Issues PMM-7092 : Update docker pmm-server 2.11.1 to 2.12.0 results in an unhealthy container. Workaround: A folder is not created on container upgrade and will need to be created manually for one of the components. Before starting the new pmm-server 2.12.0, execute: docker exec -ti pmm-server mkdir -p /srv/victoriametrics/data docker exec -ti pmm-server chown -R pmm:pmm /srv/victoriametrics/ docker restart pmm-server","title":"Percona Monitoring and Management 2.12.0"},{"location":"release-notes/2.12.0.html#release-highlights","text":"VictoriaMetrics replaces Prometheus and is now the default datasource. VictoriaMetrics supports both PUSH (client to server) and PULL metrics collection modes. ( Read more. ) PMM Client can be run as a Docker image. The \u2018Add Instance\u2019 page and forms have been redesigned and look much better.","title":"Release Highlights"},{"location":"release-notes/2.12.0.html#new-features","text":"PMM-5799 : PMM Client now available as docker image in addition to RPM, DEB and .tgz PMM-6968 : Integrated Alerting: Basic notification channels actions API Create, Read, Update, Delete PMM-6842 : VictoriaMetrics: Grafana dashboards to monitor VictoriaMetricsDB as replacement for dashboards that used to monitor Prometheus DB PMM-6395 : Replace Prometheus with VictoriaMetrics in PMM for better performance and additional functionality","title":"New Features"},{"location":"release-notes/2.12.0.html#improvements","text":"PMM-6744 : Prevent timeout of low resolution metrics in MySQL instances with many tables (~1000\u2019s) PMM-6504 : MySQL Replication Summary: MySQL Replication Delay graph not factoring in value of intentionally set SQL_Delay thus inflating time displayed PMM-6820 : \u2018pmm-admin status \u2013wait\u2019 option added to allow for configurable delay in checking status of pmm-agent PMM-6710 : pmm-admin: Allow user-specified custom \u2018group\u2019 name when adding external services PMM-6825 : Allow user to specify \u2018listen address\u2019 to pmm-agent otherwise default to 127.0.0.1 PMM-6793 : Improve user experience of \u2018add remote instance\u2019 workflow PMM-6759 : Enable kubernetes startup probes to get status of pmm-agent using \u2018GET HTTP\u2019 verb PMM-6736 : MongoDB Instance Summary dashboard: Ensure colors for ReplSet status matches those in MongoDB ReplSet Summary dashboard for better consistency PMM-6730 : Node Overview/Summary Cleanup: Remove duplicate service type \u2018DB Service Connections\u2019 PMM-6542 : PMM Add Instance: Redesign page for more intuitive experience when adding various instance types to monitoring PMM-6518 : Update default datasource name from \u2018Prometheus\u2019 to \u2018Metrics\u2019 to ensure graphs are populated correctly after upgrade to VictoriaMetrics PMM-6428 : Query Analytics dashboard - Ensure user-selected filter selections are always visible even if they don\u2019t appear in top 5 results PMM-5020 : PMM Add Remote Instance: User can specify \u2018Table Statistics Limit\u2019 for MySQL and AWS RDS MySQL to disable table stat metrics which can have an adverse impact on performance with too many tables","title":"Improvements"},{"location":"release-notes/2.12.0.html#bugs-fixed","text":"PMM-6811 : MongoDB Cluster Summary: when secondary optime is newer than primary optime, lag incorrectly shows 136 years PMM-6650 : Custom queries for MySQL 8 fail on 5.x (on update to pmm-agent 2.10) (Thanks to user debug for reporting this issue) PMM-6751 : PXC/Galera dashboards: Empty service name with MySQL version < 5.6.40 PMM-5823 : PMM Server: Timeout when simultaneously generating and accessing logs via download or API PMM-4547 : MongoDB dashboard replication lag count incorrect (Thanks to user vvol for reporting this issue) PMM-7057 : MySQL Instances Overview: Many monitored instances (~250+) gives \u2018too long query\u2019 error PMM-6883 : Query Analytics: \u2018Reset All\u2019 and \u2018Show Selected\u2019 filters behaving incorrectly PMM-6686 : Query Analytics: Filters panel blank on Microsoft Edge 44.18362.449.0 PMM-6007 : PMM Server virtual appliance\u2019s IP address not shown in OVF console PMM-6754 : Query Analytics: Bad alignment of percentage values in Filters panel PMM-6752 : Query Analytics: Time interval not preserved when using filter panel dashboard shortcuts PMM-6664 : Query Analytics: No horizontal scroll bar on Explain tab PMM-6632 : Node Summary - Virtual Memory Utilization chart: incorrect formulas PMM-6537 : MySQL InnoDB Details - Logging - Group Commit Batch Size: giving incorrect description PMM-6055 : PMM Inventory - Services: \u2018Service Type\u2019 column empty when it should be \u2018External\u2019 for external services","title":"Bugs Fixed"},{"location":"release-notes/2.12.0.html#known-issues","text":"PMM-7092 : Update docker pmm-server 2.11.1 to 2.12.0 results in an unhealthy container. Workaround: A folder is not created on container upgrade and will need to be created manually for one of the components. Before starting the new pmm-server 2.12.0, execute: docker exec -ti pmm-server mkdir -p /srv/victoriametrics/data docker exec -ti pmm-server chown -R pmm:pmm /srv/victoriametrics/ docker restart pmm-server","title":"Known Issues"},{"location":"release-notes/2.13.0.html","text":"Percona Monitoring and Management 2.13.0 Date: December 29, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Release Highlights Ability to monitor SSL-enabled MongoDB Allows PMM administrators to set up configured SSL certificate \u201ckeys\u201d to authenticate the connection to PMM, specifically for setting up MongoDB. This is a critical security requirement especially in large enterprise infrastructure environments. Technical Previews Note: We do not recommend the use of technical preview features in enterprise or production environments until the functionality is released as general availability (GA). While in Technical Preview status, these features are not supported by Percona Support SLA, except by Product/Engineering on a best-efforts basis. Integrated Alerting MVP A new feature in PMM to set up parameters and revive alerts about the Services and Nodes monitored by PMM. Read more on our blog and in our documentation . Node Summary/Nodes Overview dashboards: Show External services on dashboards Improves the user experience for adding and viewing external services on the Node Summary dashboard of PMM. External services means any data that can be monitored by a Prometheus exporter, for example, non-Percona supported databases like Redis, ElasticSearch, Cassandra, etc. or an organization\u2019s external application. DBaaS Preview phase 1.0 We are also releasing the first preview of DBaaS functionality; when combined with a compatible Kubernetes environment and Percona Operators, you can create Percona XtraDB or MongoDB clusters with just a few clicks. (Read more about configuration and usage .) Improvements PMM-5364 : Ability to monitor SSL-enabled MongoDB by passing certificate parameters in \u2018pmm-admin add\u2019 command (Thanks to Hubertus Krogmann for reporting this issue) PMM-7086 : Re-mapped /prometheus/ to /victoriametrics/ but created aliases for users that still rely on the /prometheus/ in bookmarks and scripts (Thanks to Daniel Guzman Burgos for reporting this issue) PMM-6713 : Node Summary/Nodes Overview dashboards: External exporters can now be added to dashboard and shown as part of grouping of a broader service PMM-7173 : VictoriaMetrics updated to v1.50.2: Includes HTML pages vs JSON output and new functions available for alerting rules ( see all tags ) Bugs Fixed PMM-7054 : ProxySQL Instance Summary dashboard: no Node Metrics PMM-7092 : PMM Server Docker update from 2.11.1 to 2.12.0 leaves container in unhealthy state (Thanks to Hubertus Krogmann for reporting this issue) PMM-7208 : Confusing \u201cAccess denied\u201d message for \u2018Viewer\u2019 users on many dashboards PMM-6987 : No IP address shown in log file of OVF appliance running in headless mode PMM-7146 : MongoDB Instance Summary dashboard: ReplSet element showing metric name instead of replication set PMM-6992 : Administrators can\u2019t see user\u2019s actual IP address in Grafana profile-Preferences-Sessions PMM-6865 : Rendered dashboard images partly obscured by error message","title":"Percona Monitoring and Management 2.13.0"},{"location":"release-notes/2.13.0.html#release-highlights","text":"Ability to monitor SSL-enabled MongoDB Allows PMM administrators to set up configured SSL certificate \u201ckeys\u201d to authenticate the connection to PMM, specifically for setting up MongoDB. This is a critical security requirement especially in large enterprise infrastructure environments. Technical Previews Note: We do not recommend the use of technical preview features in enterprise or production environments until the functionality is released as general availability (GA). While in Technical Preview status, these features are not supported by Percona Support SLA, except by Product/Engineering on a best-efforts basis. Integrated Alerting MVP A new feature in PMM to set up parameters and revive alerts about the Services and Nodes monitored by PMM. Read more on our blog and in our documentation . Node Summary/Nodes Overview dashboards: Show External services on dashboards Improves the user experience for adding and viewing external services on the Node Summary dashboard of PMM. External services means any data that can be monitored by a Prometheus exporter, for example, non-Percona supported databases like Redis, ElasticSearch, Cassandra, etc. or an organization\u2019s external application. DBaaS Preview phase 1.0 We are also releasing the first preview of DBaaS functionality; when combined with a compatible Kubernetes environment and Percona Operators, you can create Percona XtraDB or MongoDB clusters with just a few clicks. (Read more about configuration and usage .)","title":"Release Highlights"},{"location":"release-notes/2.13.0.html#improvements","text":"PMM-5364 : Ability to monitor SSL-enabled MongoDB by passing certificate parameters in \u2018pmm-admin add\u2019 command (Thanks to Hubertus Krogmann for reporting this issue) PMM-7086 : Re-mapped /prometheus/ to /victoriametrics/ but created aliases for users that still rely on the /prometheus/ in bookmarks and scripts (Thanks to Daniel Guzman Burgos for reporting this issue) PMM-6713 : Node Summary/Nodes Overview dashboards: External exporters can now be added to dashboard and shown as part of grouping of a broader service PMM-7173 : VictoriaMetrics updated to v1.50.2: Includes HTML pages vs JSON output and new functions available for alerting rules ( see all tags )","title":"Improvements"},{"location":"release-notes/2.13.0.html#bugs-fixed","text":"PMM-7054 : ProxySQL Instance Summary dashboard: no Node Metrics PMM-7092 : PMM Server Docker update from 2.11.1 to 2.12.0 leaves container in unhealthy state (Thanks to Hubertus Krogmann for reporting this issue) PMM-7208 : Confusing \u201cAccess denied\u201d message for \u2018Viewer\u2019 users on many dashboards PMM-6987 : No IP address shown in log file of OVF appliance running in headless mode PMM-7146 : MongoDB Instance Summary dashboard: ReplSet element showing metric name instead of replication set PMM-6992 : Administrators can\u2019t see user\u2019s actual IP address in Grafana profile-Preferences-Sessions PMM-6865 : Rendered dashboard images partly obscured by error message","title":"Bugs Fixed"},{"location":"release-notes/2.2.0.html","text":"Percona Monitoring and Management 2.2.0 Date: December 24, 2019 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. You can run PMM in your own environment for maximum security and reliability. It provides thorough time-based analysis for MySQL, MongoDB, and PostgreSQL servers to ensure that your data works as efficiently as possible. Main improvements in this release are: Alternative installation methods available for PMM 1.x are re-implemented for PMM 2: now PMM Server can be installed as a virtual appliance, or run using AWS Marketplace AWS RDS and remote instances monitoring re-added in this release include AWS RDS MySQL / Aurora MySQL instances, and remote PostgreSQL, MySQL, MongoDB, and ProxySQL ones The new Settings dashboard allows configuring PMM Server via the graphical interface For PMM install instructions, see Installing PMM Server and Installing PMM Client . Note PMM 2 is designed to be used as a new installation \u2014 your existing PMM 1 environment can\u2019t be upgraded to this version. Improvements and new features PMM-4575 : The new PMM Settings dashboard allows users to configure various PMM Server options: setting metrics resolution and data retention, enabling or disabling send usage data statistics back to Percona and checking for updates; this dashboard is now the proper place to upload your public key for the SSH login and to download PMM Server logs for diagnostics PMM-4907 and PMM-4767 : The user\u2019s AMI Instance ID is now used to setup running PMM Server using AWS Marketplace as an additional verification on the user, based on the Amazon Marketplace rules PMM-4950 and PMM-3094 : Alternative AWS partitions are now supported when adding an AWS RDS MySQL or Aurora MySQL Instance to PMM PMM-4976 : Home dashboard clean-up: \u201cSystems under monitoring\u201d and \u201cNetwork IO\u201d singlestats were refined to be based on the host variable; also avoiding using color as an indicator of state; \u201cAll\u201d row elements were relinked to the \u201cNodes Overview\u201d dashboard with regards to the selected host. PMM-4800 : The pmm-admin add mysql command has been modified to make help text more descriptive: now when you enable tablestats you will get more detail on if they\u2019re enabled for your environment and where you stand with respect to the auto-disable limit PMM-4969 : Update Grafana to version 6.5.1 PMM-5053 : A tooltip was added to the Head Block graph on the Prometheus dashboard PMM-5068 : Drill-down links were added to the Node Summary dashboard graphs PMM-5050 : Drill-down links were added to the graphs on all Services Compare dashboards PMM-5037 : Drill-down links were added to all graphs on the Services Overview dashboards PMM-4988 : Filtering in Query Analytics have undergone improvements to make group selection more intuitive: Labels unavailable under the current selection are shown as gray/disabled, and the percentage values are dynamically recalculated to reflect Labels available within the currently applied filters PMM-4966 : All passwords are now substituted with asterisk signs in the exporter logs for security reasons when not in debug mode PMM-527 : node_exporter is now providing hardware monitoring information such as CPU temperatures and fan statuses; while this information is being collected by PMM Server, it will not be shown until a dedicated dashboard is added in a future release PMM-3198 : Instead of showing All graphs for all services by default, MySQL Command/Handler Counters Compare dashboard now shows the pre-defined set of ten most informative ones, to reduce load on PMM Server at its first open Fixed bugs PMM-4978 : The \u201cTop MySQL Questions\u201d singlestat on the MySQL Instances Overview dashboard was changed to show ops instead of percentage PMM-4917 : The \u201cSystems under monitoring\u201d and \u201cMonitored DB Instances\u201d singlestats on the Home dashboard now have a sparkline to make situation more clear with recently shut down nodes/instances PMM-4979 : Set decimal precision 2 for all the elements, including charts and singlestats, on all dashboards PMM-4980 : Fix \u201cLoad Average\u201d singlestat on the Node Summary dashboard to show decimal value instead of percent PMM-4981 : Disable automatic color gradient in filled graphs on all dashboards PMM-4941 : Some charts were incorrectly showing empty fragments with high time resolution turned on PMM-5022 : Fix outdated drill-down links on the Prometheus Exporters Overview and Nodes Overview dashboards PMM-5023 : Make the All instances uptime singlestat on the Home dashboard to show Min values instead of Avg PMM-5029 : Option to upload dashboard snapshot to Percona was disappearing after upgrade to 2.1.x PMM-4946 : Rename singlestats on the Home dashboard for better clarity: \u201cSystems under monitoring\u201d to \u201cNodes under monitoring\u201d and \u201cMonitored DB Instances\u201d to \u201cMonitored DB Services\u201d, and make the last one to count remote DB instances also PMM-5015 : Fix format of Disk Page Buffers singlestat on the Compare dashboard for PostgreSQL to have two digits precision for the consistency with other singlestats PMM-5014 : LVM logical volumes were wrongly sized on a new AWS deployment, resulting in \u201cno space left on device\u201d errors. PMM-4804 : Incorrect parameters validation required both service-name and service-id parameters of the pmm-admin remove command to be presented, while the command itself demanded only one of them to identify the service. PMM-3298 : Panic errors were present in the rds_exporter log after adding an RDS instance from the second AWS account PMM-5089 : The serialize-javascript package was updated to version 2.1.1 because of the possibility of regular expressions cross-site scripting vulnerability in it (CVE-2019-16769). Please note PMM versions were not affected by this vulnerability, as serialize-javascript package is used as a build dependency only. PMM-5149 : Disk Space singlestat was unable to show data for RDS instances because of not taking into account sources with unknown filesystem type","title":"Percona Monitoring and Management 2.2.0"},{"location":"release-notes/2.2.0.html#improvements-and-new-features","text":"PMM-4575 : The new PMM Settings dashboard allows users to configure various PMM Server options: setting metrics resolution and data retention, enabling or disabling send usage data statistics back to Percona and checking for updates; this dashboard is now the proper place to upload your public key for the SSH login and to download PMM Server logs for diagnostics PMM-4907 and PMM-4767 : The user\u2019s AMI Instance ID is now used to setup running PMM Server using AWS Marketplace as an additional verification on the user, based on the Amazon Marketplace rules PMM-4950 and PMM-3094 : Alternative AWS partitions are now supported when adding an AWS RDS MySQL or Aurora MySQL Instance to PMM PMM-4976 : Home dashboard clean-up: \u201cSystems under monitoring\u201d and \u201cNetwork IO\u201d singlestats were refined to be based on the host variable; also avoiding using color as an indicator of state; \u201cAll\u201d row elements were relinked to the \u201cNodes Overview\u201d dashboard with regards to the selected host. PMM-4800 : The pmm-admin add mysql command has been modified to make help text more descriptive: now when you enable tablestats you will get more detail on if they\u2019re enabled for your environment and where you stand with respect to the auto-disable limit PMM-4969 : Update Grafana to version 6.5.1 PMM-5053 : A tooltip was added to the Head Block graph on the Prometheus dashboard PMM-5068 : Drill-down links were added to the Node Summary dashboard graphs PMM-5050 : Drill-down links were added to the graphs on all Services Compare dashboards PMM-5037 : Drill-down links were added to all graphs on the Services Overview dashboards PMM-4988 : Filtering in Query Analytics have undergone improvements to make group selection more intuitive: Labels unavailable under the current selection are shown as gray/disabled, and the percentage values are dynamically recalculated to reflect Labels available within the currently applied filters PMM-4966 : All passwords are now substituted with asterisk signs in the exporter logs for security reasons when not in debug mode PMM-527 : node_exporter is now providing hardware monitoring information such as CPU temperatures and fan statuses; while this information is being collected by PMM Server, it will not be shown until a dedicated dashboard is added in a future release PMM-3198 : Instead of showing All graphs for all services by default, MySQL Command/Handler Counters Compare dashboard now shows the pre-defined set of ten most informative ones, to reduce load on PMM Server at its first open","title":"Improvements and new features"},{"location":"release-notes/2.2.0.html#fixed-bugs","text":"PMM-4978 : The \u201cTop MySQL Questions\u201d singlestat on the MySQL Instances Overview dashboard was changed to show ops instead of percentage PMM-4917 : The \u201cSystems under monitoring\u201d and \u201cMonitored DB Instances\u201d singlestats on the Home dashboard now have a sparkline to make situation more clear with recently shut down nodes/instances PMM-4979 : Set decimal precision 2 for all the elements, including charts and singlestats, on all dashboards PMM-4980 : Fix \u201cLoad Average\u201d singlestat on the Node Summary dashboard to show decimal value instead of percent PMM-4981 : Disable automatic color gradient in filled graphs on all dashboards PMM-4941 : Some charts were incorrectly showing empty fragments with high time resolution turned on PMM-5022 : Fix outdated drill-down links on the Prometheus Exporters Overview and Nodes Overview dashboards PMM-5023 : Make the All instances uptime singlestat on the Home dashboard to show Min values instead of Avg PMM-5029 : Option to upload dashboard snapshot to Percona was disappearing after upgrade to 2.1.x PMM-4946 : Rename singlestats on the Home dashboard for better clarity: \u201cSystems under monitoring\u201d to \u201cNodes under monitoring\u201d and \u201cMonitored DB Instances\u201d to \u201cMonitored DB Services\u201d, and make the last one to count remote DB instances also PMM-5015 : Fix format of Disk Page Buffers singlestat on the Compare dashboard for PostgreSQL to have two digits precision for the consistency with other singlestats PMM-5014 : LVM logical volumes were wrongly sized on a new AWS deployment, resulting in \u201cno space left on device\u201d errors. PMM-4804 : Incorrect parameters validation required both service-name and service-id parameters of the pmm-admin remove command to be presented, while the command itself demanded only one of them to identify the service. PMM-3298 : Panic errors were present in the rds_exporter log after adding an RDS instance from the second AWS account PMM-5089 : The serialize-javascript package was updated to version 2.1.1 because of the possibility of regular expressions cross-site scripting vulnerability in it (CVE-2019-16769). Please note PMM versions were not affected by this vulnerability, as serialize-javascript package is used as a build dependency only. PMM-5149 : Disk Space singlestat was unable to show data for RDS instances because of not taking into account sources with unknown filesystem type","title":"Fixed bugs"},{"location":"release-notes/2.2.1.html","text":"Percona Monitoring and Management 2.2.1 Date: January 23, 2020 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. For PMM install instructions, see Installing PMM Server and Installing PMM Client . Note PMM 2 is designed to be used as a new installation \u2014 your existing PMM 1 environment can\u2019t be upgraded to this version. PMM Server version 2.2.0 suffered an unauthenticated denial of service vulnerability (CVE-2020-7920). Any other PMM versions do not carry the same code logic, and are thus unaffected by this issue. Users who have already deployed PMM Server 2.2.0 are advised to upgrade to version 2.2.1 which resolves this issue. Improvements and new features PMM-5229 : The new RDS Exporter section added to the Prometheus Exporter Status dashboard shows singlestats and charts related to the rds_exporter PMM-5228 and PMM-5238 : The Prometheus dashboard and the Exporters Overview dashboard were updated to include the rds_exporter metrics in their charts, allowing better understanding of the impacts of monitoring RDS instances PMM-4830 : The consistency of the applied filters between the Query Analytics and the Overview dashboards was implemented, and now filters selected in QAN will continue to be active after the switch to any of the Overview dashboards available in the Services menu PMM-5235 : The DB uptime singlestats in node rows on the Home dashboard were changed to show minimal values instead of average ones to be consistent with the top row PMM-5127 : The \u201cSearch by\u201d bar on the Query Analytics dashboard was renamed to \u201cFilter by\u201d to make its purpose more clear PMM-5131 : The Filter panel on the Query Analytics dashboard now shows the total number of available Labels within the \u201cSee all\u201d link, which appears if the Filter panel section shows only top 5 of its Labels Fixed bugs PMM-5232 : The pmm-managed component of the PMM Server 2.2.0 is vulnerable to DoS attacks, that could be carried out by anyone who knows the PMM Server IP address (CVE-2020-7920). Versions other than 2.2.0 are not affected. PMM-5226 : The handlebars package was updated to version 4.5.3 because of the Prototype Pollution vulnerability in it (CVE-2019-19919). Please note PMM versions were not affected by this vulnerability, as handlebars package is used as a build dependency only. PMM-5206 : Switching to the Settings dashboard was breaking the visual style of some elements on the Home dashboard PMM-5139 : The breadcrumb panel, which shows all dashboards visited within one session starting from the root, was unable to fully show breadcrumb longer than one line PMM-5212 : The explanatory text was added to the Download PMM Server Logs button in the Diagnostic section of the PMM Settings dashboard, and a link to it was added to the Prometheus dashboard which was the previous place to download logs PMM-5215 : The unneeded mariadb-libs package was removed from the PMM Server 2.2.0 OVF image, resulting in both faster updating with the yum update command and avoiding dependency conflict messages in the update logs PMM-5216 : PMM Server Upgrade to 2.2.0 was showing Grafana Update Error page with the Refresh button which had to be clicked to start using the updated version PMM-5211 : The \u201cWhere do I get the security credentials for my Amazon RDS DB instance\u201d link in the Add AWS RDS MySQL or Aurora MySQL instance dialog was not targeted at the appropriate instruction PMM-5217 : PMM2.x OVF Image memory size was increased from 1Gb to 4Gb with the additional 1Gb swap space because the previous amount was hardly housing the PMM Server, and it wasn\u2019t enough in some cases like performing an upgrade PMM-5271 : LVM logical volumes were wrongly resized on AWS deployment, resulting in \u201cno space left on device\u201d errors PMM-5295 : Innodb Transaction Rollback Rate values on the MySQL InnoDB Details dashboard were calculated incorrectly PMM-5270 : PXC/Galera Cluster Summary dashboard was showing empty Cluster drop-down list, making it impossible to choose the cluster name PMM-4769 : The wrongly named \u201cTimeout value used for retransmitting\u201d singlestat on the Network Details dashboard was renamed to \u201cThe algorithm used to determine the timeout value\u201d and updated to show the algorithm name instead of a digital code PMM-5260 : Extensive resource consumption by pmm-agent took place in case of Query Analytics for PostgreSQL; it was fixed by a number of optimizations in the code, resulting in about 4 times smaller memory usage PMM-5261 : CPU usage charts on all dashboards which contain them have undergone colors update to make softIRQ and Steal curves better differentiated PMM-5244 : High memory consumption in the PMM Server with a large number of agents sending data simultaneously was fixed by improving bulk data insertion to the ClickHouse database","title":"Percona Monitoring and Management 2.2.1"},{"location":"release-notes/2.2.1.html#improvements-and-new-features","text":"PMM-5229 : The new RDS Exporter section added to the Prometheus Exporter Status dashboard shows singlestats and charts related to the rds_exporter PMM-5228 and PMM-5238 : The Prometheus dashboard and the Exporters Overview dashboard were updated to include the rds_exporter metrics in their charts, allowing better understanding of the impacts of monitoring RDS instances PMM-4830 : The consistency of the applied filters between the Query Analytics and the Overview dashboards was implemented, and now filters selected in QAN will continue to be active after the switch to any of the Overview dashboards available in the Services menu PMM-5235 : The DB uptime singlestats in node rows on the Home dashboard were changed to show minimal values instead of average ones to be consistent with the top row PMM-5127 : The \u201cSearch by\u201d bar on the Query Analytics dashboard was renamed to \u201cFilter by\u201d to make its purpose more clear PMM-5131 : The Filter panel on the Query Analytics dashboard now shows the total number of available Labels within the \u201cSee all\u201d link, which appears if the Filter panel section shows only top 5 of its Labels","title":"Improvements and new features"},{"location":"release-notes/2.2.1.html#fixed-bugs","text":"PMM-5232 : The pmm-managed component of the PMM Server 2.2.0 is vulnerable to DoS attacks, that could be carried out by anyone who knows the PMM Server IP address (CVE-2020-7920). Versions other than 2.2.0 are not affected. PMM-5226 : The handlebars package was updated to version 4.5.3 because of the Prototype Pollution vulnerability in it (CVE-2019-19919). Please note PMM versions were not affected by this vulnerability, as handlebars package is used as a build dependency only. PMM-5206 : Switching to the Settings dashboard was breaking the visual style of some elements on the Home dashboard PMM-5139 : The breadcrumb panel, which shows all dashboards visited within one session starting from the root, was unable to fully show breadcrumb longer than one line PMM-5212 : The explanatory text was added to the Download PMM Server Logs button in the Diagnostic section of the PMM Settings dashboard, and a link to it was added to the Prometheus dashboard which was the previous place to download logs PMM-5215 : The unneeded mariadb-libs package was removed from the PMM Server 2.2.0 OVF image, resulting in both faster updating with the yum update command and avoiding dependency conflict messages in the update logs PMM-5216 : PMM Server Upgrade to 2.2.0 was showing Grafana Update Error page with the Refresh button which had to be clicked to start using the updated version PMM-5211 : The \u201cWhere do I get the security credentials for my Amazon RDS DB instance\u201d link in the Add AWS RDS MySQL or Aurora MySQL instance dialog was not targeted at the appropriate instruction PMM-5217 : PMM2.x OVF Image memory size was increased from 1Gb to 4Gb with the additional 1Gb swap space because the previous amount was hardly housing the PMM Server, and it wasn\u2019t enough in some cases like performing an upgrade PMM-5271 : LVM logical volumes were wrongly resized on AWS deployment, resulting in \u201cno space left on device\u201d errors PMM-5295 : Innodb Transaction Rollback Rate values on the MySQL InnoDB Details dashboard were calculated incorrectly PMM-5270 : PXC/Galera Cluster Summary dashboard was showing empty Cluster drop-down list, making it impossible to choose the cluster name PMM-4769 : The wrongly named \u201cTimeout value used for retransmitting\u201d singlestat on the Network Details dashboard was renamed to \u201cThe algorithm used to determine the timeout value\u201d and updated to show the algorithm name instead of a digital code PMM-5260 : Extensive resource consumption by pmm-agent took place in case of Query Analytics for PostgreSQL; it was fixed by a number of optimizations in the code, resulting in about 4 times smaller memory usage PMM-5261 : CPU usage charts on all dashboards which contain them have undergone colors update to make softIRQ and Steal curves better differentiated PMM-5244 : High memory consumption in the PMM Server with a large number of agents sending data simultaneously was fixed by improving bulk data insertion to the ClickHouse database","title":"Fixed bugs"},{"location":"release-notes/2.2.2.html","text":"Percona Monitoring and Management 2.2.2 Date: February 4, 2020 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. For PMM install instructions, see Installing PMM Server and Installing PMM Client . Note PMM 2 is designed to be used as a new installation \u2014 your existing PMM 1 environment can\u2019t be upgraded to this version. Improvements and new features PMM-5321 : The optimization of the Query Analytics parser code for PostgreSQL queries allowed us to reduce the memory resources consumption by 1-5%, and the parsing time of an individual query by 30-40% PMM-5184 : The pmm-admin summary command have gained a new --skip-server flag which makes it operating in a local-only mode, creating summary file without contacting the PMM Server Fixed bugs PMM-5340 : The Scraping Time Drift graph on the Prometheus dashboard was showing wrong values because the actual metrics resolution wasn\u2019t taken into account PMM-5060 : Qery Analytics Dashboard did not show the row with the last query of the first page, if the number of queries to display was 11","title":"Percona Monitoring and Management 2.2.2"},{"location":"release-notes/2.2.2.html#improvements-and-new-features","text":"PMM-5321 : The optimization of the Query Analytics parser code for PostgreSQL queries allowed us to reduce the memory resources consumption by 1-5%, and the parsing time of an individual query by 30-40% PMM-5184 : The pmm-admin summary command have gained a new --skip-server flag which makes it operating in a local-only mode, creating summary file without contacting the PMM Server","title":"Improvements and new features"},{"location":"release-notes/2.2.2.html#fixed-bugs","text":"PMM-5340 : The Scraping Time Drift graph on the Prometheus dashboard was showing wrong values because the actual metrics resolution wasn\u2019t taken into account PMM-5060 : Qery Analytics Dashboard did not show the row with the last query of the first page, if the number of queries to display was 11","title":"Fixed bugs"},{"location":"release-notes/2.3.0.html","text":"Percona Monitoring and Management 2.3.0 Date: February 19, 2020 Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. For PMM install instructions, see Installing PMM Server and Installing PMM Client . Note PMM 2 is designed to be used as a new installation \u2014 your existing PMM 1 environment can\u2019t be upgraded to this version. Improvements and new features PMM-5064 and PMM-5065 : Starting from this release, users will be able to integrate PMM with an external Alertmanager by specifying the Alertmanager URL and the Alert Rules to be executed inside the PMM server Note This feature is for advanced users only at this point PMM-4954 : Query Analytics dashboard now shows units both in the list of queries in a summary table and in the Details section to ease understanding of the presented data PMM-5179 : Relations between metrics are now specified in the Query Analytics Details section PMM-5115 : The CPU frequency and temperature graphs were added to the CPU Utilization dashboard PMM-5394 : A special treatment for the node-related dashboards was implemented for the situations when the data resolution change causes new metrics to be generated for existing nodes and services, to make graphs show continuous lines of the same colors Fixed bugs PMM-4620 : The high CPU usage by the pmm-agent process related to MongoDB Query Analytics was fixed PMM-5377 : Singlestats showing percentage had sparklines scaled vertically along with the graph swing, which made it difficult to visually notice the difference between neighboring singlestats. PMM-5204 : Changing resolution on the PMM settings page was breaking some singlestats on the Home and MySQL Overview dashboards PMM-5251 : Vertical scrollbars on the graph elements were not allowed to do a full scroll, making last rows of the legend unavailable for some graphs PMM-5410 : The \u201cAvailable Downtime before SST Required\u201d chart on the PXC/Galera Node Summary dashboard was not showing data because it was unable to use metrics available with different scraping intervals","title":"Percona Monitoring and Management 2.3.0"},{"location":"release-notes/2.3.0.html#improvements-and-new-features","text":"PMM-5064 and PMM-5065 : Starting from this release, users will be able to integrate PMM with an external Alertmanager by specifying the Alertmanager URL and the Alert Rules to be executed inside the PMM server Note This feature is for advanced users only at this point PMM-4954 : Query Analytics dashboard now shows units both in the list of queries in a summary table and in the Details section to ease understanding of the presented data PMM-5179 : Relations between metrics are now specified in the Query Analytics Details section PMM-5115 : The CPU frequency and temperature graphs were added to the CPU Utilization dashboard PMM-5394 : A special treatment for the node-related dashboards was implemented for the situations when the data resolution change causes new metrics to be generated for existing nodes and services, to make graphs show continuous lines of the same colors","title":"Improvements and new features"},{"location":"release-notes/2.3.0.html#fixed-bugs","text":"PMM-4620 : The high CPU usage by the pmm-agent process related to MongoDB Query Analytics was fixed PMM-5377 : Singlestats showing percentage had sparklines scaled vertically along with the graph swing, which made it difficult to visually notice the difference between neighboring singlestats. PMM-5204 : Changing resolution on the PMM settings page was breaking some singlestats on the Home and MySQL Overview dashboards PMM-5251 : Vertical scrollbars on the graph elements were not allowed to do a full scroll, making last rows of the legend unavailable for some graphs PMM-5410 : The \u201cAvailable Downtime before SST Required\u201d chart on the PXC/Galera Node Summary dashboard was not showing data because it was unable to use metrics available with different scraping intervals","title":"Fixed bugs"},{"location":"release-notes/2.4.0.html","text":"Percona Monitoring and Management 2.4.0 Date: March 18, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. New Features PMM-3387 : Prometheus custom configuration is now supported by PMM Server. The feature is targeted at experienced users and is done by adding the base configuration file into the PMM Server container to be parsed and included into the managed Prometheus configuration. PMM-5186 : Including \u2013pprof option in the pmm-admin summary command adds pprof debug profiles to the diagnostics data archive PMM-5102 : The new \u201cNode Details\u201d dashboard now displays data from the hardware monitoring sensors in hwmon The new dashboard is based on the hwmon collector data from the node_exporter. Please note that data may be unavailable for some nodes because of the configuration or virtualization parameters Improvements PMM-4915 : The Query Analytics dashboard now shows Time Metrics in the Profile Section as \u201cAVG per query\u201d instead of \u201cAVG per second\u201d PMM-5470 : Clickhouse query optimized for Query Analytics to improve its speed and reduce the load on the backend PMM-5448 : The default high and medium metrics resolutions were changed to 1-5-30 and 5-10-60 sec. To reduce the effect of this change on existing installations, systems having the \u201cold\u201d high resolution chosen on the PMM Settings page (5-5-60 sec.) will be automatically re-configured to the medium one during an upgrade. If the resolution was changed to some custom values via API, it will not be affected PMM-5531 : A healthcheck indicator was implemented for the PMM Server Docker image. It is based on the Docker HEALTHCHECK . This feature can be leveraged as follows: docker inspect -f { { .State.Health.Status }} until [ \"`docker inspect -f { {.State.Health.Status}} pmm-server`\" == \"healthy\" ] ; do sleep 1 ; done PMM-5489 : The \u201cTotal\u201d line in all charts is now drawn with the same red color for better consistency PMM-5461 : Memory graphs on the node-related dashboards were adjusted to have fixed colors that are more distinguishable from each other PMM-5329 : Prometheus in PMM Server was updated to version 2.16.0. This update has brought several improvements. Among them are significantly reduced memory footprint of the loaded TSDB blocks, lower memory footprint for the compaction process (caused by the more balanced choice of what to buffer during compaction), and improved query performance for the queries that only touch the most recent 2h of data. PMM-5210 : Data Retention is now specified in days instead of seconds on the PMM Settings page. Please note this is the UI-only change, so the actual data retention precision is not changed PMM-5182 : The logs.zip archive available on the PMM Settings page now includes additional self-monitoring information in a separate \u201cclient\u201d subfolder. This subfolder contains information collected on the PMM Server and is equivalent to the one collected on a node by the pmm-admin summary command. PMM-5112 : The Inventory API List requests now can be filtered by the Node/Service/Agent type Bugs Fixed PMM-5178 : Query Detail Section of the Query Analytics dashboard didn\u2019t show tables definitions and indexes for the internal PostgreSQL database PMM-5465 : MySQL Instance related dashboards had row names not always matching the actual contents. To fix this, elements were re-ordered and additional rows were added for better matching of the row name and the corresponding elements PMM-5455 : Dashboards from the Insight menu were fixed to work correctly when the low resolution is set on the PMM Settings page PMM-5446 : A number of the Compare Dashboards were fixed to work correctly when the low resolution is set on the PMM Settings page PMM-5430 : MySQL Exporter section on the Prometheus Exporter Status dashboard now collapsed by default to be consistent with other database-related sections PMM-5445 , PMM-5439 , PMM-5427 , PMM-5426 , PMM-5419 : Labels change (which occurs e.g. when the metrics resolution is changed on the PMM Settings page) was breaking dashboards PMM-5347 : Selecting queries on the Query Analytics dashboard was generating errors in the browser console PMM-5305 : Some applied filters on the Query Analytics dashboard were not preserved after changing the time range PMM-5267 : The Refresh button was not working on the Query Analytics dashboard PMM-5003 : pmm-admin list and status use different JSON naming for the same data PMM-5526 : A typo was fixed in the Replication Dashboard description tooltip Help us improve our software quality by reporting any bugs you encounter using our bug tracking system .","title":"Percona Monitoring and Management 2.4.0"},{"location":"release-notes/2.4.0.html#new-features","text":"PMM-3387 : Prometheus custom configuration is now supported by PMM Server. The feature is targeted at experienced users and is done by adding the base configuration file into the PMM Server container to be parsed and included into the managed Prometheus configuration. PMM-5186 : Including \u2013pprof option in the pmm-admin summary command adds pprof debug profiles to the diagnostics data archive PMM-5102 : The new \u201cNode Details\u201d dashboard now displays data from the hardware monitoring sensors in hwmon The new dashboard is based on the hwmon collector data from the node_exporter. Please note that data may be unavailable for some nodes because of the configuration or virtualization parameters","title":"New Features"},{"location":"release-notes/2.4.0.html#improvements","text":"PMM-4915 : The Query Analytics dashboard now shows Time Metrics in the Profile Section as \u201cAVG per query\u201d instead of \u201cAVG per second\u201d PMM-5470 : Clickhouse query optimized for Query Analytics to improve its speed and reduce the load on the backend PMM-5448 : The default high and medium metrics resolutions were changed to 1-5-30 and 5-10-60 sec. To reduce the effect of this change on existing installations, systems having the \u201cold\u201d high resolution chosen on the PMM Settings page (5-5-60 sec.) will be automatically re-configured to the medium one during an upgrade. If the resolution was changed to some custom values via API, it will not be affected PMM-5531 : A healthcheck indicator was implemented for the PMM Server Docker image. It is based on the Docker HEALTHCHECK . This feature can be leveraged as follows: docker inspect -f { { .State.Health.Status }} until [ \"`docker inspect -f { {.State.Health.Status}} pmm-server`\" == \"healthy\" ] ; do sleep 1 ; done PMM-5489 : The \u201cTotal\u201d line in all charts is now drawn with the same red color for better consistency PMM-5461 : Memory graphs on the node-related dashboards were adjusted to have fixed colors that are more distinguishable from each other PMM-5329 : Prometheus in PMM Server was updated to version 2.16.0. This update has brought several improvements. Among them are significantly reduced memory footprint of the loaded TSDB blocks, lower memory footprint for the compaction process (caused by the more balanced choice of what to buffer during compaction), and improved query performance for the queries that only touch the most recent 2h of data. PMM-5210 : Data Retention is now specified in days instead of seconds on the PMM Settings page. Please note this is the UI-only change, so the actual data retention precision is not changed PMM-5182 : The logs.zip archive available on the PMM Settings page now includes additional self-monitoring information in a separate \u201cclient\u201d subfolder. This subfolder contains information collected on the PMM Server and is equivalent to the one collected on a node by the pmm-admin summary command. PMM-5112 : The Inventory API List requests now can be filtered by the Node/Service/Agent type","title":"Improvements"},{"location":"release-notes/2.4.0.html#bugs-fixed","text":"PMM-5178 : Query Detail Section of the Query Analytics dashboard didn\u2019t show tables definitions and indexes for the internal PostgreSQL database PMM-5465 : MySQL Instance related dashboards had row names not always matching the actual contents. To fix this, elements were re-ordered and additional rows were added for better matching of the row name and the corresponding elements PMM-5455 : Dashboards from the Insight menu were fixed to work correctly when the low resolution is set on the PMM Settings page PMM-5446 : A number of the Compare Dashboards were fixed to work correctly when the low resolution is set on the PMM Settings page PMM-5430 : MySQL Exporter section on the Prometheus Exporter Status dashboard now collapsed by default to be consistent with other database-related sections PMM-5445 , PMM-5439 , PMM-5427 , PMM-5426 , PMM-5419 : Labels change (which occurs e.g. when the metrics resolution is changed on the PMM Settings page) was breaking dashboards PMM-5347 : Selecting queries on the Query Analytics dashboard was generating errors in the browser console PMM-5305 : Some applied filters on the Query Analytics dashboard were not preserved after changing the time range PMM-5267 : The Refresh button was not working on the Query Analytics dashboard PMM-5003 : pmm-admin list and status use different JSON naming for the same data PMM-5526 : A typo was fixed in the Replication Dashboard description tooltip Help us improve our software quality by reporting any bugs you encounter using our bug tracking system .","title":"Bugs Fixed"},{"location":"release-notes/2.5.0.html","text":"Percona Monitoring and Management 2.5.0 Date: April 14, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. New Features PMM-5042 and PMM-5272 : PMM can now connect to MySQL instances by specifying a UNIX socket. This can be done with a new --socket option of the pmm-admin add mysql command. (Note: Updates to both PMM Client and PMM Server were done to allow UNIX socket connections.) PMM-4145 : Amazon RDS instance metrics can now be independently enabled/disabled for Basic and/or Enhanced metrics. Improvements PMM-5581 : PMM Server Grafana plugins can now be updated on the command line with the grafana-cli command-line utility. PMM-5536 : Three Grafana plugins were updated to the latest versions: vertamedia-clickhouse-datasource to 1.9.5, grafana-polystat-panel to 1.1.0, and grafana-piechart-panel to 1.4.0. PMM-4252 : The resolution of the PMM Server favicon image has been improved. Bugs Fixed PMM-5547 : PMM dashboards were failing when presenting data from more than 100 monitored instances (error message proxy error: context canceled ). PMM-5624 : Empty charts were being shown in some Node Temperature dashboards. PMM-5637 : The Data retention value in Settings was incorrectly showing the value as minutes instead of days. PMM-5613 : Sorting data by Query Time was not working properly in Query Analytics. PMM-5554 : Totals in charts were inconsistently plotted with different colors across charts. PMM-4919 : The force option ( --force ) in pmm-admin config was not always working. PMM-5351 : The documentation on MongoDB user privileges has been corrected. Help us improve our software quality by reporting any bugs you encounter using our bug tracking system .","title":"Percona Monitoring and Management 2.5.0"},{"location":"release-notes/2.5.0.html#new-features","text":"PMM-5042 and PMM-5272 : PMM can now connect to MySQL instances by specifying a UNIX socket. This can be done with a new --socket option of the pmm-admin add mysql command. (Note: Updates to both PMM Client and PMM Server were done to allow UNIX socket connections.) PMM-4145 : Amazon RDS instance metrics can now be independently enabled/disabled for Basic and/or Enhanced metrics.","title":"New Features"},{"location":"release-notes/2.5.0.html#improvements","text":"PMM-5581 : PMM Server Grafana plugins can now be updated on the command line with the grafana-cli command-line utility. PMM-5536 : Three Grafana plugins were updated to the latest versions: vertamedia-clickhouse-datasource to 1.9.5, grafana-polystat-panel to 1.1.0, and grafana-piechart-panel to 1.4.0. PMM-4252 : The resolution of the PMM Server favicon image has been improved.","title":"Improvements"},{"location":"release-notes/2.5.0.html#bugs-fixed","text":"PMM-5547 : PMM dashboards were failing when presenting data from more than 100 monitored instances (error message proxy error: context canceled ). PMM-5624 : Empty charts were being shown in some Node Temperature dashboards. PMM-5637 : The Data retention value in Settings was incorrectly showing the value as minutes instead of days. PMM-5613 : Sorting data by Query Time was not working properly in Query Analytics. PMM-5554 : Totals in charts were inconsistently plotted with different colors across charts. PMM-4919 : The force option ( --force ) in pmm-admin config was not always working. PMM-5351 : The documentation on MongoDB user privileges has been corrected. Help us improve our software quality by reporting any bugs you encounter using our bug tracking system .","title":"Bugs Fixed"},{"location":"release-notes/2.6.0.html","text":"Percona Monitoring and Management 2.6.0 Date: May 11, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. New Features PMM-5728 : Technical preview of External Services monitoring feature. A new command provides integration with hundreds of third-party systems ( https://prometheus.io/docs/instrumenting/exporters/ ) via the Prometheus protocol so that you can monitor external services on a node where pmm-agent is installed. PMM-5822 : PMM now includes a Security Threat Tool to help users avoid the most common database security issues. Read more here . PMM-5559 : Global annotations can now be set with the pmm-admin annotate command. PMM-4931 : PMM now checks Docker environment variables and warns about invalid ones. Improvements PMM-1962 : The PMM Server API (via /v1/readyz) now also returns Grafana status information in addition to that for Prometheus. PMM-5854 : The Service Details dashboards were cleaned up and some unused selectors were removed. PMM-5775 : It is now clearer which nodes are Primary and which are Secondary on MongoDB Instance dashboards. PMM-5549 : PMM\u2019s Grafana component is now the latest, v6.7.3. PMM-5393 : There\u2019s a new \u2018Node Summary\u2019 row in the services Summary and Details dashboards summarizing the system update, load average, RAM and memory. PMM-4778 : The mongodb_exporter is now the latest version, v0.11.0. PMM-5734 : Temporary files activity and utilization charts (rate & irate) were added to the PostgreSQL Instance overview. PMM-5695 : The error message explains better when using the \u2013socket option incorrectly. Bugs Fixed PMM-4829 : The MongoDB Exporter wasn\u2019t able to collect metrics from hidden nodes without either the latest driver or using the \u2018connect-direct\u2019 parameter. PMM-5056 : The average values for Query time in the Details and Profile sections were different. PMM-2717 : Updating MongoDB Exporter resolves an error ( Failed to execute find query on 'config.locks': not found. ) when used with shardedCluster 3.6.4. PMM-4541 : MongoDB exporter metrics collection was including system collections from collStats and indexStats, causing \u201clog bloat\u201d. PMM-5913 : Only totals were shown in QAN when filtering on Cluster=MongoDB. PMM-5903 : When applying a filter the QAN Overview was being refreshed twice. PMM-5821 : The Compare button was missing from HA Dashboard main menus. PMM-5687 : Cumulative charts for Disk Details were not showing any data if metrics were returning \u2018NaN\u2019 results. PMM-5663 : The \u2018version\u2019 value was not being refreshed in various MySQL dashboards. PMM-5643 : Advanced Data Exploration charts were showing \u2018N/A\u2019 for Metric Resolution and \u2018No data to show\u2019 in the Metric Data Table. PMM-4756 : Dashboards were not showing services with empty environments. PMM-4562 : MongoDB and MySQL registered instances with empty cluster labels ( \u2013environment=<label> ) were not visible in the dashboard despite being added instances. PMM-4906 : The MongoDB exporter for MongoDB 4.0 and above was causing a \u201clog bloat\u201d condition. Help us improve our software quality by reporting any bugs you encounter using our bug tracking system .","title":"Percona Monitoring and Management 2.6.0"},{"location":"release-notes/2.6.0.html#new-features","text":"PMM-5728 : Technical preview of External Services monitoring feature. A new command provides integration with hundreds of third-party systems ( https://prometheus.io/docs/instrumenting/exporters/ ) via the Prometheus protocol so that you can monitor external services on a node where pmm-agent is installed. PMM-5822 : PMM now includes a Security Threat Tool to help users avoid the most common database security issues. Read more here . PMM-5559 : Global annotations can now be set with the pmm-admin annotate command. PMM-4931 : PMM now checks Docker environment variables and warns about invalid ones.","title":"New Features"},{"location":"release-notes/2.6.0.html#improvements","text":"PMM-1962 : The PMM Server API (via /v1/readyz) now also returns Grafana status information in addition to that for Prometheus. PMM-5854 : The Service Details dashboards were cleaned up and some unused selectors were removed. PMM-5775 : It is now clearer which nodes are Primary and which are Secondary on MongoDB Instance dashboards. PMM-5549 : PMM\u2019s Grafana component is now the latest, v6.7.3. PMM-5393 : There\u2019s a new \u2018Node Summary\u2019 row in the services Summary and Details dashboards summarizing the system update, load average, RAM and memory. PMM-4778 : The mongodb_exporter is now the latest version, v0.11.0. PMM-5734 : Temporary files activity and utilization charts (rate & irate) were added to the PostgreSQL Instance overview. PMM-5695 : The error message explains better when using the \u2013socket option incorrectly.","title":"Improvements"},{"location":"release-notes/2.6.0.html#bugs-fixed","text":"PMM-4829 : The MongoDB Exporter wasn\u2019t able to collect metrics from hidden nodes without either the latest driver or using the \u2018connect-direct\u2019 parameter. PMM-5056 : The average values for Query time in the Details and Profile sections were different. PMM-2717 : Updating MongoDB Exporter resolves an error ( Failed to execute find query on 'config.locks': not found. ) when used with shardedCluster 3.6.4. PMM-4541 : MongoDB exporter metrics collection was including system collections from collStats and indexStats, causing \u201clog bloat\u201d. PMM-5913 : Only totals were shown in QAN when filtering on Cluster=MongoDB. PMM-5903 : When applying a filter the QAN Overview was being refreshed twice. PMM-5821 : The Compare button was missing from HA Dashboard main menus. PMM-5687 : Cumulative charts for Disk Details were not showing any data if metrics were returning \u2018NaN\u2019 results. PMM-5663 : The \u2018version\u2019 value was not being refreshed in various MySQL dashboards. PMM-5643 : Advanced Data Exploration charts were showing \u2018N/A\u2019 for Metric Resolution and \u2018No data to show\u2019 in the Metric Data Table. PMM-4756 : Dashboards were not showing services with empty environments. PMM-4562 : MongoDB and MySQL registered instances with empty cluster labels ( \u2013environment=<label> ) were not visible in the dashboard despite being added instances. PMM-4906 : The MongoDB exporter for MongoDB 4.0 and above was causing a \u201clog bloat\u201d condition. Help us improve our software quality by reporting any bugs you encounter using our bug tracking system .","title":"Bugs Fixed"},{"location":"release-notes/2.6.1.html","text":"Percona Monitoring and Management 2.6.1 Date: May 18, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Improvements PMM-5936 : Improved Summary dashboard for Security Threat Tool \u2018Failed Checks\u2019 PMM-5937 : Improved Details dashboard for Security Threat Tool \u2018Failed Database Checks\u2019 Bugs Fixed PMM-5924 : Alertmanager not running after PMM Server upgrade via Docker PMM-5915 : Supervisord not restarting after restart of PMM Server virtual appliances (OVF/AMI) PMM-5945 : \u2018Updates\u2019 dashboard not showing available updates PMM-5870 : MySQL Table Details dashboard not showing separate service names for tables","title":"Percona Monitoring and Management 2.6.1"},{"location":"release-notes/2.6.1.html#improvements","text":"PMM-5936 : Improved Summary dashboard for Security Threat Tool \u2018Failed Checks\u2019 PMM-5937 : Improved Details dashboard for Security Threat Tool \u2018Failed Database Checks\u2019","title":"Improvements"},{"location":"release-notes/2.6.1.html#bugs-fixed","text":"PMM-5924 : Alertmanager not running after PMM Server upgrade via Docker PMM-5915 : Supervisord not restarting after restart of PMM Server virtual appliances (OVF/AMI) PMM-5945 : \u2018Updates\u2019 dashboard not showing available updates PMM-5870 : MySQL Table Details dashboard not showing separate service names for tables","title":"Bugs Fixed"},{"location":"release-notes/2.7.0.html","text":"Percona Monitoring and Management 2.7.0 Date: June 9, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. In this release, we have updated Grafana to version 6.7.4 to fix CVE-2020-13379 . We recommend updating to the latest version of PMM as soon as possible. New Features PMM-5257 , PMM-5256 , & PMM-5243 : pmm-admin socket option (\u2013socket) to specify UNIX socket path for connecting to MongoDB, PostgreSQL, and ProxySQL instances Improvements PMM-2244 : \u2018pmm-admin status\u2019 command output shows both pmm-admin and pmm-agent versions PMM-5968 : Disallow PMM Server node or agent removal via API PMM-5946 : MySQL Table Details dashboard filter on Service Name prevents display of services without data PMM-5926 : Expose PMM-agent version in pmm-admin status command PMM-5891 : PMM Home page now includes News panel PMM-5906 : Independent update of PMM components deactivated Bugs Fixed PMM-6004 : MySQL exporter reporting wrong values for cluster status (wsrep_cluster_status) PMM-4547 : MongoDB dashboard replication lag count incorrect PMM-5524 : Prometheus alerting rule changes needs docker restart to activate PMM-5949 : Unwanted filters applied when moving from QAN to Add Instance page PMM-5870 : MySQL Table Details dashboard not showing separate service names for tables PMM-5839 : PostgreSQL metrics disparity between query time and block read/write time PMM-5348 : Inventory page has inaccessible tabs that need reload to access PMM-5348 : Incorrect access control vulnerability fix (CVE-2020-13379) by upgrading grafana to v6.7.4","title":"Percona Monitoring and Management 2.7.0"},{"location":"release-notes/2.7.0.html#new-features","text":"PMM-5257 , PMM-5256 , & PMM-5243 : pmm-admin socket option (\u2013socket) to specify UNIX socket path for connecting to MongoDB, PostgreSQL, and ProxySQL instances","title":"New Features"},{"location":"release-notes/2.7.0.html#improvements","text":"PMM-2244 : \u2018pmm-admin status\u2019 command output shows both pmm-admin and pmm-agent versions PMM-5968 : Disallow PMM Server node or agent removal via API PMM-5946 : MySQL Table Details dashboard filter on Service Name prevents display of services without data PMM-5926 : Expose PMM-agent version in pmm-admin status command PMM-5891 : PMM Home page now includes News panel PMM-5906 : Independent update of PMM components deactivated","title":"Improvements"},{"location":"release-notes/2.7.0.html#bugs-fixed","text":"PMM-6004 : MySQL exporter reporting wrong values for cluster status (wsrep_cluster_status) PMM-4547 : MongoDB dashboard replication lag count incorrect PMM-5524 : Prometheus alerting rule changes needs docker restart to activate PMM-5949 : Unwanted filters applied when moving from QAN to Add Instance page PMM-5870 : MySQL Table Details dashboard not showing separate service names for tables PMM-5839 : PostgreSQL metrics disparity between query time and block read/write time PMM-5348 : Inventory page has inaccessible tabs that need reload to access PMM-5348 : Incorrect access control vulnerability fix (CVE-2020-13379) by upgrading grafana to v6.7.4","title":"Bugs Fixed"},{"location":"release-notes/2.8.0.html","text":"Percona Monitoring and Management 2.8.0 Date: June 25, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Improvements PMM-544 : Agents, Services and Nodes can now be removed via the \u2018PMM Inventory\u2019 page PMM-5706 : User-installed Grafana plugins unaffected by PMM upgrade Bugs Fixed PMM-6153 : PMM 2.7.0 inoperable when no Internet connectivity PMM-5365 : Client fails to send non-UTF-8 query analytics content to server (Thanks to user romulus for reporting this issue) PMM-5920 : Incorrect metric used in formula for \u201cTop Users by Rows Fetched/Read\u201d graph PMM-6084 : Annotations not showing consistently on dashboards PMM-6011 : No data in MongoDB Cluster summary, RocksDB & MMAPv1 details PMM-5987 : Incorrect total value for virtual memory utilization","title":"Percona Monitoring and Management 2.8.0"},{"location":"release-notes/2.8.0.html#improvements","text":"PMM-544 : Agents, Services and Nodes can now be removed via the \u2018PMM Inventory\u2019 page PMM-5706 : User-installed Grafana plugins unaffected by PMM upgrade","title":"Improvements"},{"location":"release-notes/2.8.0.html#bugs-fixed","text":"PMM-6153 : PMM 2.7.0 inoperable when no Internet connectivity PMM-5365 : Client fails to send non-UTF-8 query analytics content to server (Thanks to user romulus for reporting this issue) PMM-5920 : Incorrect metric used in formula for \u201cTop Users by Rows Fetched/Read\u201d graph PMM-6084 : Annotations not showing consistently on dashboards PMM-6011 : No data in MongoDB Cluster summary, RocksDB & MMAPv1 details PMM-5987 : Incorrect total value for virtual memory utilization","title":"Bugs Fixed"},{"location":"release-notes/2.9.0.html","text":"Percona Monitoring and Management 2.9.0 Date: July 14, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Highlights This release brings a major rework of the Query Analytics (QAN) component, completing the migration from Angular to React, and adding new UI functionality and features. For details, see: PMM-5125 : Implement new version of QAN PMM-5516 : QAN migration to React and new UI implementation You can read more in the accompanying blog post ( here ). New Features PMM-6124 : New dashboards: MongoDB Replica Set Summary and MongoDB Cluster Summary PMM-1027 : New dashboard: MySQL User Details (INFORMATION_SCHEMA.CLIENT_STATISTICS) PMM-5604 : User interface for MongoDB EXPLAIN PMM-5563 : Per-Service and per-Node Annotations (This completes the work on improvements to the Annotation functionality.) Improvements PMM-6114 : Sort Agents, Nodes, and Services alphabetically by name in Inventory page (Thanks to user debug for reporting this issue) PMM-6147 : Update Grafana plugins to latest versions Bugs Fixed PMM-5800 : QAN explain and tables tabs not working after removing MySQL metrics agent PMM-5812 : Prometheus relabeling broken (relabel_configs unmarshal errors) (Thanks to user b4bufr1k for reporting this issue) PMM-6184 : MongoDB Instances Compare dashboard shows MySQL metric PMM-5941 : Stacked Incoming/Outgoing Network Traffic graphs in MySQL Instances Overview dashboard prevents comparison PMM-6194 : Missing UID for Advanced Data Exploration dashboard PMM-6191 : Incorrect computation for Prometheus Process CPU Usage panel values in Prometheus dashboard PMM-6175 : Node Overview dashboard shows unit for unitless value \u2018Top I/O Load\u2019","title":"Percona Monitoring and Management 2.9.0"},{"location":"release-notes/2.9.0.html#highlights","text":"This release brings a major rework of the Query Analytics (QAN) component, completing the migration from Angular to React, and adding new UI functionality and features. For details, see: PMM-5125 : Implement new version of QAN PMM-5516 : QAN migration to React and new UI implementation You can read more in the accompanying blog post ( here ).","title":"Highlights"},{"location":"release-notes/2.9.0.html#new-features","text":"PMM-6124 : New dashboards: MongoDB Replica Set Summary and MongoDB Cluster Summary PMM-1027 : New dashboard: MySQL User Details (INFORMATION_SCHEMA.CLIENT_STATISTICS) PMM-5604 : User interface for MongoDB EXPLAIN PMM-5563 : Per-Service and per-Node Annotations (This completes the work on improvements to the Annotation functionality.)","title":"New Features"},{"location":"release-notes/2.9.0.html#improvements","text":"PMM-6114 : Sort Agents, Nodes, and Services alphabetically by name in Inventory page (Thanks to user debug for reporting this issue) PMM-6147 : Update Grafana plugins to latest versions","title":"Improvements"},{"location":"release-notes/2.9.0.html#bugs-fixed","text":"PMM-5800 : QAN explain and tables tabs not working after removing MySQL metrics agent PMM-5812 : Prometheus relabeling broken (relabel_configs unmarshal errors) (Thanks to user b4bufr1k for reporting this issue) PMM-6184 : MongoDB Instances Compare dashboard shows MySQL metric PMM-5941 : Stacked Incoming/Outgoing Network Traffic graphs in MySQL Instances Overview dashboard prevents comparison PMM-6194 : Missing UID for Advanced Data Exploration dashboard PMM-6191 : Incorrect computation for Prometheus Process CPU Usage panel values in Prometheus dashboard PMM-6175 : Node Overview dashboard shows unit for unitless value \u2018Top I/O Load\u2019","title":"Bugs Fixed"},{"location":"release-notes/2.9.1.html","text":"Percona Monitoring and Management 2.9.1 Date: August 4, 2020 Installation: Installing Percona Monitoring and Management Percona Monitoring and Management (PMM) is a free and open-source platform for managing and monitoring MySQL, MongoDB, and PostgreSQL performance. Improvements PMM-6230 : Custom dashboards set as Home remain so after update PMM-6300 : Query Analytics Dashboard: Column sorting arrows made easier to use (Thanks to user debug for reporting this issue) PMM-6208 : Security Threat Tool: Temporarily silence viewed but unactioned alerts PMM-6315 : Query Analytics Dashboard: Improved metrics names and descriptions PMM-6274 : MySQL User Details Dashboard: View selected user\u2019s queries in Query Analytics Dashboard PMM-6266 : Query Analytics Dashboard: Pagination device menu lists 25, 50 or 100 items per page PMM-6262 : PostgreSQL Instance Summary Dashboard: Descriptions for all \u2018Temp Files\u2019 views PMM-6253 : Query Analytics Dashboard: Improved SQL formatting in Examples panel PMM-6211 : Query Analytics Dashboard: Loading activity spinner added to Example, Explain and Tables tabs PMM-6162 : Consistent sort order in dashboard drop-down filter lists PMM-5132 : Better message when filter search returns nothing Bugs Fixed PMM-5783 : Bulk failure of SHOW ALL SLAVES STATUS scraping on PS/MySQL distributions triggers errors PMM-6294 : Query Analytics Dashboard doesn\u2019t resize well for some screen resolutions (Thanks to user debug for reporting this issue) PMM-6420 : Wrong version in successful update pop-up window PMM-6319 : Query Analytics Dashboard: Query scrolls out of view when selected PMM-6302 : Query Analytics Dashboard: Unnecessary EXPLAIN requests PMM-6256 : Query Analytics Dashboard: \u2018InvalidNamespace\u2019 EXPLAIN error with some MongoDB queries PMM-6329 : Query Analytics Dashboard: Unclear origin of sparkline tooltip on mouse-over PMM-6259 : Query Analytics Dashboard: Slow appearance of query time distribution graph for some queries PMM-6189 : Disk Details Dashboard: Disk IO Size chart larger by factor of 512 PMM-6269 : Query Analytics Dashboard: Metrics dropdown list obscured when opened PMM-6247 : Query Analytics Dashboard: Overview table not resizing on window size change PMM-6227 : Home Dashboard redirection to Node Summary Dashboard not working","title":"Percona Monitoring and Management 2.9.1"},{"location":"release-notes/2.9.1.html#improvements","text":"PMM-6230 : Custom dashboards set as Home remain so after update PMM-6300 : Query Analytics Dashboard: Column sorting arrows made easier to use (Thanks to user debug for reporting this issue) PMM-6208 : Security Threat Tool: Temporarily silence viewed but unactioned alerts PMM-6315 : Query Analytics Dashboard: Improved metrics names and descriptions PMM-6274 : MySQL User Details Dashboard: View selected user\u2019s queries in Query Analytics Dashboard PMM-6266 : Query Analytics Dashboard: Pagination device menu lists 25, 50 or 100 items per page PMM-6262 : PostgreSQL Instance Summary Dashboard: Descriptions for all \u2018Temp Files\u2019 views PMM-6253 : Query Analytics Dashboard: Improved SQL formatting in Examples panel PMM-6211 : Query Analytics Dashboard: Loading activity spinner added to Example, Explain and Tables tabs PMM-6162 : Consistent sort order in dashboard drop-down filter lists PMM-5132 : Better message when filter search returns nothing","title":"Improvements"},{"location":"release-notes/2.9.1.html#bugs-fixed","text":"PMM-5783 : Bulk failure of SHOW ALL SLAVES STATUS scraping on PS/MySQL distributions triggers errors PMM-6294 : Query Analytics Dashboard doesn\u2019t resize well for some screen resolutions (Thanks to user debug for reporting this issue) PMM-6420 : Wrong version in successful update pop-up window PMM-6319 : Query Analytics Dashboard: Query scrolls out of view when selected PMM-6302 : Query Analytics Dashboard: Unnecessary EXPLAIN requests PMM-6256 : Query Analytics Dashboard: \u2018InvalidNamespace\u2019 EXPLAIN error with some MongoDB queries PMM-6329 : Query Analytics Dashboard: Unclear origin of sparkline tooltip on mouse-over PMM-6259 : Query Analytics Dashboard: Slow appearance of query time distribution graph for some queries PMM-6189 : Disk Details Dashboard: Disk IO Size chart larger by factor of 512 PMM-6269 : Query Analytics Dashboard: Metrics dropdown list obscured when opened PMM-6247 : Query Analytics Dashboard: Overview table not resizing on window size change PMM-6227 : Home Dashboard redirection to Node Summary Dashboard not working","title":"Bugs Fixed"},{"location":"setting-up/index.html","text":"Setting up: Overview PMM Server as a: Docker container Virtual appliance based on our OVA/OVF image Amazon AWS EC2 instance via the Amazon AWS Marketplace PMM Client as a Docker container PMM Clients on: MySQL Percona Server for MySQL MongDB PostgreSQL ProxySQL Amazon RDS Linux External services","title":"Setting up: Overview"},{"location":"setting-up/client/index.html","text":"Setting up PMM Clients Supported platforms Storage requirements Installing PMM Client with your Linux package manager Using apt-get (Debian/Ubuntu) Using yum (RedHat/Centos) Connecting PMM Clients to PMM Server Removing monitoring services with pmm-admin remove PMM Client is a package of agents and exporters installed on the host you wish to monitor. Before installing, know your PMM Server\u2019s IP address and make sure that it is accessible. You will need root access on the database host where you install PMM Client (either logged in as a user with root privileges or have sudo rights). Note Credentials used in communication between the exporters and the PMM Server are the following ones: login is pmm password is equal to Agent ID, which can be seen e.g. on the Inventory Dashboard. Supported platforms PMM Client should run on any modern RedHat or Debian-based 64-bit Linux distribution, but is only tested on: RHEL/CentOS 6, 7, 8 Debian 8, 9, 10 Ubuntu 16.04, 18.04, 20.04 We recommended installing PMM Client via your system\u2019s package management tool , using the software repository provided by Percona for popular Linux distributions. If this option does not work for you, Percona provides downloadable PMM Client packages from the Download Percona Monitoring and Management page. As well as DEB and RPM packages, you will also find: generic tarballs that you can extract and run the included install script; source code tarball to build the PMM client from source. Storage requirements A minimum of 100 MB of storage is required for installing the PMM Client package. With a good constant connection to PMM Server, additional storage is not required. However, the client needs to store any collected data that it is not able to send over immediately, so additional storage may be required if connection is unstable or throughput is too low. Installing PMM Client with your Linux package manager Using apt-get (Debian/Ubuntu) Configure Percona repositories using the percona-release tool. First you\u2019ll need to download and install the official percona-release package from Percona: wget https://repo.percona.com/apt/percona-release_latest.generic_all.deb sudo dpkg -i percona-release_latest.generic_all.deb Note If you have previously enabled the experimental or testing Percona repository, don\u2019t forget to disable them and enable the release component of the original repository as follows: sudo percona-release disable all sudo percona-release enable original release Install the PMM client package: sudo apt-get update sudo apt-get install pmm2-client Register your Node: pmm-admin config --server-insecure-tls --server-url = https://admin:admin@<IP Address>:443 You should see the following output: Checking local pmm-agent status... pmm-agent is running. Registering pmm-agent on PMM Server... Registered. Configuration file /usr/local/percona/pmm-agent.yaml updated. Reloading pmm-agent configuration... Configuration reloaded. Using yum (RedHat/Centos) Configure Percona repositories using the percona-release tool. First you\u2019ll need to download and install the official percona-release package from Percona: sudo yum install https://repo.percona.com/yum/percona-release-latest.noarch.rpm Note If you have previously enabled the experimental or testing Percona repository, don\u2019t forget to disable them and enable the release component of the original repository as follows: sudo percona-release disable all sudo percona-release enable original release See percona-release official documentation for details. Install the pmm2-client package: yum install pmm2-client Once PMM Client is installed, run the pmm-admin config command with your PMM Server IP address to register your Node within the Server: pmm-admin config --server-insecure-tls --server-url = https://admin:admin@<IP Address>:443 You should see the following: Checking local pmm-agent status... pmm-agent is running. Registering pmm-agent on PMM Server... Registered. Configuration file /usr/local/percona/pmm-agent.yaml updated. Reloading pmm-agent configuration... Configuration reloaded. Connecting PMM Clients to PMM Server With your server and clients set up, you must configure each PMM Client and specify which PMM Server it should send its data to. To connect a PMM Client, enter the IP address of the PMM Server as the value of the --server-url parameter to the pmm-admin config command, and allow using self-signed certificates with --server-insecure-tls . Note The --server-url argument should include https:// prefix and PMM Server credentials, which are admin / admin by default, if not changed at first PMM Server GUI access. Run this command as root or by using the sudo command pmm-admin config --server-insecure-tls --server-url = https://admin:admin@192.168.100.1:443 For example, if your PMM Server is running on 192.168.100.1, you have installed PMM Client on a machine with IP 192.168.200.1, and didn\u2019t change default PMM Server credentials, run the following in the terminal of your client. Run the following commands as root or by using the sudo command: pmm-admin config --server-insecure-tls --server-url = https://admin:admin@192.168.100.1:443 Checking local pmm-agent status... pmm-agent is running. Registering pmm-agent on PMM Server... Registered. Configuration file /usr/local/percona/pmm-agent.yaml updated. Reloading pmm-agent configuration... Configuration reloaded. Checking local pmm-agent status... pmm-agent is running. If you change the default port 443 when running PMM Server, specify the new port number after the IP address of PMM Server. Note By default pmm-admin config refuses to add client if it already exists in the PMM Server inventory database. If you need to re-add an already existing client (e.g. after full reinstall, hostname changes, etc.), you can run pmm-admin config with the additional --force option. This will remove an existing node with the same name, if any, and all its dependent services. By default, the node name is the hostname. If you have non-unique client hostnames, specify the node name when adding the client: pmm-admin add TYPE [ options ] NODE-NAME Removing monitoring services with pmm-admin remove Use the pmm-admin remove command to remove monitoring services. USAGE Run this command as root or by using the sudo command pmm-admin remove [ OPTIONS ] [ SERVICE-TYPE ] [ SERVICE-NAME ] When you remove a service, collected data remains in Metrics Monitor on PMM Server for the specified retention period . SERVICES Service type can be mysql, mongodb, postgresql or proxysql, and service name is a monitoring service alias. To see which services are enabled, run pmm-admin list . EXAMPLES # Removing MySQL service named mysql-sl pmm-admin remove mysql mysql-sl # remove MongoDB service named mongo pmm-admin remove mongodb mongo # remove PostgreSQL service named postgres pmm-admin remove postgresql postgres # remove ProxySQL service named ubuntu-proxysql pmm-admin remove proxysql ubuntu-proxysql For more information, run pmm-admin remove --help . Seealso pmm-admin Percona Tools Supported Platforms .","title":"Setting up PMM Clients"},{"location":"setting-up/client/index.html#supported-platforms","text":"PMM Client should run on any modern RedHat or Debian-based 64-bit Linux distribution, but is only tested on: RHEL/CentOS 6, 7, 8 Debian 8, 9, 10 Ubuntu 16.04, 18.04, 20.04 We recommended installing PMM Client via your system\u2019s package management tool , using the software repository provided by Percona for popular Linux distributions. If this option does not work for you, Percona provides downloadable PMM Client packages from the Download Percona Monitoring and Management page. As well as DEB and RPM packages, you will also find: generic tarballs that you can extract and run the included install script; source code tarball to build the PMM client from source.","title":"Supported platforms"},{"location":"setting-up/client/index.html#storage-requirements","text":"A minimum of 100 MB of storage is required for installing the PMM Client package. With a good constant connection to PMM Server, additional storage is not required. However, the client needs to store any collected data that it is not able to send over immediately, so additional storage may be required if connection is unstable or throughput is too low.","title":"Storage requirements"},{"location":"setting-up/client/index.html#installing-pmm-client-with-your-linux-package-manager","text":"","title":"Installing PMM Client with your Linux package manager"},{"location":"setting-up/client/index.html#using-apt-get-debianubuntu","text":"Configure Percona repositories using the percona-release tool. First you\u2019ll need to download and install the official percona-release package from Percona: wget https://repo.percona.com/apt/percona-release_latest.generic_all.deb sudo dpkg -i percona-release_latest.generic_all.deb Note If you have previously enabled the experimental or testing Percona repository, don\u2019t forget to disable them and enable the release component of the original repository as follows: sudo percona-release disable all sudo percona-release enable original release Install the PMM client package: sudo apt-get update sudo apt-get install pmm2-client Register your Node: pmm-admin config --server-insecure-tls --server-url = https://admin:admin@<IP Address>:443 You should see the following output: Checking local pmm-agent status... pmm-agent is running. Registering pmm-agent on PMM Server... Registered. Configuration file /usr/local/percona/pmm-agent.yaml updated. Reloading pmm-agent configuration... Configuration reloaded.","title":"Using apt-get (Debian/Ubuntu)"},{"location":"setting-up/client/index.html#using-yum-redhatcentos","text":"Configure Percona repositories using the percona-release tool. First you\u2019ll need to download and install the official percona-release package from Percona: sudo yum install https://repo.percona.com/yum/percona-release-latest.noarch.rpm Note If you have previously enabled the experimental or testing Percona repository, don\u2019t forget to disable them and enable the release component of the original repository as follows: sudo percona-release disable all sudo percona-release enable original release See percona-release official documentation for details. Install the pmm2-client package: yum install pmm2-client Once PMM Client is installed, run the pmm-admin config command with your PMM Server IP address to register your Node within the Server: pmm-admin config --server-insecure-tls --server-url = https://admin:admin@<IP Address>:443 You should see the following: Checking local pmm-agent status... pmm-agent is running. Registering pmm-agent on PMM Server... Registered. Configuration file /usr/local/percona/pmm-agent.yaml updated. Reloading pmm-agent configuration... Configuration reloaded.","title":"Using yum (RedHat/Centos)"},{"location":"setting-up/client/index.html#connecting-pmm-clients-to-pmm-server","text":"With your server and clients set up, you must configure each PMM Client and specify which PMM Server it should send its data to. To connect a PMM Client, enter the IP address of the PMM Server as the value of the --server-url parameter to the pmm-admin config command, and allow using self-signed certificates with --server-insecure-tls . Note The --server-url argument should include https:// prefix and PMM Server credentials, which are admin / admin by default, if not changed at first PMM Server GUI access. Run this command as root or by using the sudo command pmm-admin config --server-insecure-tls --server-url = https://admin:admin@192.168.100.1:443 For example, if your PMM Server is running on 192.168.100.1, you have installed PMM Client on a machine with IP 192.168.200.1, and didn\u2019t change default PMM Server credentials, run the following in the terminal of your client. Run the following commands as root or by using the sudo command: pmm-admin config --server-insecure-tls --server-url = https://admin:admin@192.168.100.1:443 Checking local pmm-agent status... pmm-agent is running. Registering pmm-agent on PMM Server... Registered. Configuration file /usr/local/percona/pmm-agent.yaml updated. Reloading pmm-agent configuration... Configuration reloaded. Checking local pmm-agent status... pmm-agent is running. If you change the default port 443 when running PMM Server, specify the new port number after the IP address of PMM Server. Note By default pmm-admin config refuses to add client if it already exists in the PMM Server inventory database. If you need to re-add an already existing client (e.g. after full reinstall, hostname changes, etc.), you can run pmm-admin config with the additional --force option. This will remove an existing node with the same name, if any, and all its dependent services. By default, the node name is the hostname. If you have non-unique client hostnames, specify the node name when adding the client: pmm-admin add TYPE [ options ] NODE-NAME","title":"Connecting PMM Clients to PMM Server"},{"location":"setting-up/client/index.html#removing-monitoring-services-with-pmm-admin-remove","text":"Use the pmm-admin remove command to remove monitoring services. USAGE Run this command as root or by using the sudo command pmm-admin remove [ OPTIONS ] [ SERVICE-TYPE ] [ SERVICE-NAME ] When you remove a service, collected data remains in Metrics Monitor on PMM Server for the specified retention period . SERVICES Service type can be mysql, mongodb, postgresql or proxysql, and service name is a monitoring service alias. To see which services are enabled, run pmm-admin list . EXAMPLES # Removing MySQL service named mysql-sl pmm-admin remove mysql mysql-sl # remove MongoDB service named mongo pmm-admin remove mongodb mongo # remove PostgreSQL service named postgres pmm-admin remove postgresql postgres # remove ProxySQL service named ubuntu-proxysql pmm-admin remove proxysql ubuntu-proxysql For more information, run pmm-admin remove --help . Seealso pmm-admin Percona Tools Supported Platforms .","title":"Removing monitoring services with pmm-admin remove"},{"location":"setting-up/client/aws.html","text":"Amazon RDS Required settings It is possible to use PMM for monitoring Amazon RDS (just like any remote MySQL instance). In this case, the PMM Client is not installed on the host where the database server is deployed. By using the PMM web interface, you connect to the Amazon RDS DB instance. You only need to provide the IAM user access key (or assign an IAM role) and PMM discovers the Amazon RDS DB instances available for monitoring. First of all, ensure that there is the minimal latency between PMM Server and the Amazon RDS instance. Network connectivity can become an issue for VictoriaMetrics to scrape metrics with 1 second resolution. We strongly suggest that you run PMM Server on AWS (Amazon Web Services) in the same availability zone as Amazon RDS instances. It is crucial that enhanced monitoring be enabled for the Amazon RDS DB instances you intend to monitor. Set the Enable Enhanced Monitoring option in the settings of your Amazon RDS DB instance. Creating an IAM user with permission to access Amazon RDS DB instances It is recommended that you use an IAM user account to access Amazon RDS DB instances instead of using your AWS account. This measure improves security as the permissions of an IAM user account can be limited so that this account only grants access to your Amazon RDS DB instances. On the other hand, you use your AWS account to access all AWS services. The procedure for creating IAM user accounts is well described in the Amazon RDS documentation. This section only goes through the essential steps and points out the steps required for using Amazon RDS with Percona Monitoring and Management. The first step is to define a policy which will hold all the necessary permissions. Then, you need to associate this policy with the IAM user or group. In this section, we will create a new user for this purpose. Creating a policy A policy defines how AWS services can be accessed. Once defined it can be associated with an existing user or group. To define a new policy use the IAM page at AWS. Select the Policies option on the navigation panel and click the Create policy button. On the Create policy page, select the JSON tab and replace the existing contents with the following JSON document. { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"Stmt1508404837000\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"rds:DescribeDBInstances\" , \"cloudwatch:GetMetricStatistics\" , \"cloudwatch:ListMetrics\" ], \"Resource\" : [ \"*\" ] }, { \"Sid\" : \"Stmt1508410723001\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"logs:DescribeLogStreams\" , \"logs:GetLogEvents\" , \"logs:FilterLogEvents\" ], \"Resource\" : [ \"arn:aws:logs:*:*:log-group:RDSOSMetrics:*\" ]} ] } Click Review policy and set a name to your policy, such as AmazonRDSforPMMPolicy . Then, click the Create policy button. Creating an IAM user Policies are attached to existing IAM users or groups. To create a new IAM user, select Users on the Identity and Access Management page at AWS. Then click Add user and complete the following steps: On the Add user page, set the user name and select the Programmatic access option under Select AWS access type . Set a custom password and then proceed to permissions by clicking the Permissions button. On the Set permissions page, add the new user to one or more groups if necessary. Then, click Review . On the Add user page, click Create user . Creating an access key for an IAM user In order to be able to discover an Amazon RDS DB instance in PMM, you either need to use the access key and secret access key of an existing IAM user or an IAM role. To create an access key for use with PMM, open the IAM console and click Users on the navigation pane. Then, select your IAM user. To create the access key, open the Security credentials tab and click the Create access key button. The system automatically generates a new access key ID and a secret access key that you can provide on the PMM Add Instance dashboard to have your Amazon RDS DB instances discovered. Note You may use an IAM role instead of IAM user provided your Amazon RDS DB instances are associated with the same AWS account as PMM. In case, the PMM Server and Amazon RDS DB instance were created by using the same AWS account, you do not need create the access key ID and secret access key manually. PMM retrieves this information automatically and attempts to discover your Amazon RDS DB instances. Attaching a policy to an IAM user The last step before you are ready to create an Amazon RDS DB instance is to attach the policy with the required permissions to the IAM user. First, make sure that the Identity and Access Management page is open and open Users . Then, locate and open the IAM user that you plan to use with Amazon RDS DB instances. Complete the following steps, to apply the policy: On the Permissions tab, click the Add permissions button. On the Add permissions page, click Attach existing policies directly . Using the Filter , locate the policy with the required permissions (such as AmazonRDSforPMMPolicy ). Select a checkbox next to the name of the policy and click Review . The selected policy appears on the Permissions summary page. Click Add permissions . The AmazonRDSforPMMPolicy is now added to your IAM user. Setting up the Amazon RDS DB Instance Query Analytics requires Configuring Performance Schema as the query source, because the slow query log is stored on the AWS (Amazon Web Services) side, and QAN agent is not able to read it. Enable the performance_schema option under Parameter Groups in Amazon RDS. Warning Enabling Performance Schema on T2 instances is not recommended because it can easily run the T2 instance out of memory. When adding a monitoring instance for Amazon RDS, specify a unique name to distinguish it from the local MySQL instance. If you do not specify a name, it will use the client\u2019s host name. Create the pmm user with the following privileges on the Amazon RDS instance that you want to monitor: GRANT SELECT , PROCESS , REPLICATION CLIENT ON * . * TO 'pmm' @ '%' IDENTIFIED BY 'pass' WITH MAX_USER_CONNECTIONS 10 ; GRANT SELECT , UPDATE , DELETE , DROP ON performance_schema . * TO 'pmm' @ '%' ; If you have Amazon RDS with a MySQL version prior to 5.5, REPLICATION CLIENT privilege is not available there and has to be excluded from the above statement. Note General system metrics are monitored by using the rds_exporter exporter which replaces node_exporter . rds_exporter gives access to Amazon Cloudwatch metrics. node_exporter , used in versions of PMM prior to 1.8.0, was not able to monitor general system metrics remotely. Adding an Amazon RDS MySQL, Aurora MySQL, or Remote Instance The PMM Add Instance is now a preferred method of adding an Amazon RDS database instance to PMM. This method supports Amazon RDS database instances that use Amazon Aurora, MySQL, or MariaDB engines, as well as any remote PostgreSQL, ProxySQL, MySQL and MongoDB instances. Following steps are needed to add an Amazon RDS database instance to PMM: In the PMM web interface, go to PMM > PMM Add Instance . Select Add an AWS RDS MySQL or Aurora MySQL Instance . Enter the access key ID and the secret access key of your IAM user. Click the Discover button for PMM to retrieve the available Amazon RDS instances. For the instance that you would like to monitor, select the Start monitoring button. You will see a new page with the number of fields. The list is divided into the following groups: Main details , RDS database , Labels , and Additional options . Some already known data, such as already entered AWS access key , are filled in automatically, and some fields are optional. The Main details section allows to specify the DNS hostname of your instance, service name to use within PMM, the port your service is listening on, the database user name and password. The Labels section allows specifying labels for the environment, the AWS region and availability zone to be used, the Replication set and Cluster names and also it allows to set the list of custom labels in a key:value format. The Additional options section contains specific flags which allow to tune the RDS monitoring. They can allow you to skip connection check, to use TLS for the database connection, not to validate the TLS certificate and the hostname, as well as to disable basic and/or enhanced metrics collection for the RDS instance to reduce costs. Also this section contains a database-specific flag, which would allow Query Analytics for the selected remote database: when adding some remote MySQL, AWS RDS MySQL or Aurora MySQL instance, you will be able to choose using performance schema for the database monitoring when adding a PostgreSQL instance, you will be able to activate using pg_stat_statements extension when adding a MongoDB instance, you will be able to choose using Query Analytics MongoDB profiler Finally press the Add service button to start monitoring your instance.","title":"Amazon RDS"},{"location":"setting-up/client/aws.html#required-settings","text":"It is possible to use PMM for monitoring Amazon RDS (just like any remote MySQL instance). In this case, the PMM Client is not installed on the host where the database server is deployed. By using the PMM web interface, you connect to the Amazon RDS DB instance. You only need to provide the IAM user access key (or assign an IAM role) and PMM discovers the Amazon RDS DB instances available for monitoring. First of all, ensure that there is the minimal latency between PMM Server and the Amazon RDS instance. Network connectivity can become an issue for VictoriaMetrics to scrape metrics with 1 second resolution. We strongly suggest that you run PMM Server on AWS (Amazon Web Services) in the same availability zone as Amazon RDS instances. It is crucial that enhanced monitoring be enabled for the Amazon RDS DB instances you intend to monitor. Set the Enable Enhanced Monitoring option in the settings of your Amazon RDS DB instance.","title":"Required settings"},{"location":"setting-up/client/aws.html#creating-an-iam-user-with-permission-to-access-amazon-rds-db-instances","text":"It is recommended that you use an IAM user account to access Amazon RDS DB instances instead of using your AWS account. This measure improves security as the permissions of an IAM user account can be limited so that this account only grants access to your Amazon RDS DB instances. On the other hand, you use your AWS account to access all AWS services. The procedure for creating IAM user accounts is well described in the Amazon RDS documentation. This section only goes through the essential steps and points out the steps required for using Amazon RDS with Percona Monitoring and Management. The first step is to define a policy which will hold all the necessary permissions. Then, you need to associate this policy with the IAM user or group. In this section, we will create a new user for this purpose.","title":"Creating an IAM user with permission to access Amazon RDS DB instances"},{"location":"setting-up/client/aws.html#creating-a-policy","text":"A policy defines how AWS services can be accessed. Once defined it can be associated with an existing user or group. To define a new policy use the IAM page at AWS. Select the Policies option on the navigation panel and click the Create policy button. On the Create policy page, select the JSON tab and replace the existing contents with the following JSON document. { \"Version\" : \"2012-10-17\" , \"Statement\" : [{ \"Sid\" : \"Stmt1508404837000\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"rds:DescribeDBInstances\" , \"cloudwatch:GetMetricStatistics\" , \"cloudwatch:ListMetrics\" ], \"Resource\" : [ \"*\" ] }, { \"Sid\" : \"Stmt1508410723001\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"logs:DescribeLogStreams\" , \"logs:GetLogEvents\" , \"logs:FilterLogEvents\" ], \"Resource\" : [ \"arn:aws:logs:*:*:log-group:RDSOSMetrics:*\" ]} ] } Click Review policy and set a name to your policy, such as AmazonRDSforPMMPolicy . Then, click the Create policy button.","title":"Creating a policy"},{"location":"setting-up/client/aws.html#creating-an-iam-user","text":"Policies are attached to existing IAM users or groups. To create a new IAM user, select Users on the Identity and Access Management page at AWS. Then click Add user and complete the following steps: On the Add user page, set the user name and select the Programmatic access option under Select AWS access type . Set a custom password and then proceed to permissions by clicking the Permissions button. On the Set permissions page, add the new user to one or more groups if necessary. Then, click Review . On the Add user page, click Create user .","title":"Creating an IAM user"},{"location":"setting-up/client/aws.html#creating-an-access-key-for-an-iam-user","text":"In order to be able to discover an Amazon RDS DB instance in PMM, you either need to use the access key and secret access key of an existing IAM user or an IAM role. To create an access key for use with PMM, open the IAM console and click Users on the navigation pane. Then, select your IAM user. To create the access key, open the Security credentials tab and click the Create access key button. The system automatically generates a new access key ID and a secret access key that you can provide on the PMM Add Instance dashboard to have your Amazon RDS DB instances discovered. Note You may use an IAM role instead of IAM user provided your Amazon RDS DB instances are associated with the same AWS account as PMM. In case, the PMM Server and Amazon RDS DB instance were created by using the same AWS account, you do not need create the access key ID and secret access key manually. PMM retrieves this information automatically and attempts to discover your Amazon RDS DB instances.","title":"Creating an access key for an IAM user"},{"location":"setting-up/client/aws.html#attaching-a-policy-to-an-iam-user","text":"The last step before you are ready to create an Amazon RDS DB instance is to attach the policy with the required permissions to the IAM user. First, make sure that the Identity and Access Management page is open and open Users . Then, locate and open the IAM user that you plan to use with Amazon RDS DB instances. Complete the following steps, to apply the policy: On the Permissions tab, click the Add permissions button. On the Add permissions page, click Attach existing policies directly . Using the Filter , locate the policy with the required permissions (such as AmazonRDSforPMMPolicy ). Select a checkbox next to the name of the policy and click Review . The selected policy appears on the Permissions summary page. Click Add permissions . The AmazonRDSforPMMPolicy is now added to your IAM user.","title":"Attaching a policy to an IAM user"},{"location":"setting-up/client/aws.html#setting-up-the-amazon-rds-db-instance","text":"Query Analytics requires Configuring Performance Schema as the query source, because the slow query log is stored on the AWS (Amazon Web Services) side, and QAN agent is not able to read it. Enable the performance_schema option under Parameter Groups in Amazon RDS. Warning Enabling Performance Schema on T2 instances is not recommended because it can easily run the T2 instance out of memory. When adding a monitoring instance for Amazon RDS, specify a unique name to distinguish it from the local MySQL instance. If you do not specify a name, it will use the client\u2019s host name. Create the pmm user with the following privileges on the Amazon RDS instance that you want to monitor: GRANT SELECT , PROCESS , REPLICATION CLIENT ON * . * TO 'pmm' @ '%' IDENTIFIED BY 'pass' WITH MAX_USER_CONNECTIONS 10 ; GRANT SELECT , UPDATE , DELETE , DROP ON performance_schema . * TO 'pmm' @ '%' ; If you have Amazon RDS with a MySQL version prior to 5.5, REPLICATION CLIENT privilege is not available there and has to be excluded from the above statement. Note General system metrics are monitored by using the rds_exporter exporter which replaces node_exporter . rds_exporter gives access to Amazon Cloudwatch metrics. node_exporter , used in versions of PMM prior to 1.8.0, was not able to monitor general system metrics remotely.","title":"Setting up the Amazon RDS DB Instance"},{"location":"setting-up/client/docker.html","text":"Docker A PMM Client docker image is available from percona/pmm-client . It runs with Docker 1.12.6 or later. Tip Make sure that the firewall and routing rules of the host do not constrain the Docker container. ( Read more in the FAQ. ) The Docker image is a collection of preinstalled software which lets you run a selected version of PMM Client. The Docker image is not run directly. You use it to create a Docker container for your PMM Client. When launched, the Docker container gives access to the whole functionality of PMM Client. Running PMM Client as a Docker container Pull the image docker pull percona/pmm-client:2 Create a persistent data store docker create -v /srv --name pmm-client-data percona/pmm-client:2 /bin/true Note This container does not run, but exists only to make sure you retain all PMM data when upgrading to a newer image. Run the container docker run --rm \\ -e PMM_AGENT_SERVER_ADDRESS=<your-pmm-server-IP-address>:443 \\ -e PMM_AGENT_SERVER_USERNAME=admin \\ -e PMM_AGENT_SERVER_PASSWORD=admin \\ -e PMM_AGENT_SERVER_INSECURE_TLS=1 \\ -e PMM_AGENT_SETUP=1 \\ -e PMM_AGENT_CONFIG_FILE=pmm-agent.yml \\ --volumes-from pmm-client-data percona/pmm-client:2 Connecting to a Docker PMM Server by container name To connect to a Dockerized PMM Server by name instead of IP: Put both containers on a non-default network: docker network create <network-name> to create a network, docker network connect <network-name> <container> to connect a container to that network. Change the value of the first option to -e PMM_AGENT_SERVER_ADDRESS=<your-pmm-server-container-name>:443 . Tip To get help: docker run --rm percona/pmm-client:2 --help See also pmm-agent options and environment","title":"Docker"},{"location":"setting-up/client/docker.html#running-pmm-client-as-a-docker-container","text":"Pull the image docker pull percona/pmm-client:2 Create a persistent data store docker create -v /srv --name pmm-client-data percona/pmm-client:2 /bin/true Note This container does not run, but exists only to make sure you retain all PMM data when upgrading to a newer image. Run the container docker run --rm \\ -e PMM_AGENT_SERVER_ADDRESS=<your-pmm-server-IP-address>:443 \\ -e PMM_AGENT_SERVER_USERNAME=admin \\ -e PMM_AGENT_SERVER_PASSWORD=admin \\ -e PMM_AGENT_SERVER_INSECURE_TLS=1 \\ -e PMM_AGENT_SETUP=1 \\ -e PMM_AGENT_CONFIG_FILE=pmm-agent.yml \\ --volumes-from pmm-client-data percona/pmm-client:2 Connecting to a Docker PMM Server by container name To connect to a Dockerized PMM Server by name instead of IP: Put both containers on a non-default network: docker network create <network-name> to create a network, docker network connect <network-name> <container> to connect a container to that network. Change the value of the first option to -e PMM_AGENT_SERVER_ADDRESS=<your-pmm-server-container-name>:443 . Tip To get help: docker run --rm percona/pmm-client:2 --help See also pmm-agent options and environment","title":"Running PMM Client as a Docker container"},{"location":"setting-up/client/external.html","text":"External Services Adding general external services You can collect metrics from an external (custom) exporter on a node when: there is already a PMM Agent instance running and, this node has been configured using the pmm-admin config command. Usage pmm-admin add external [ --service-name = <service-name> ] [ --listen-port = <listen-port> ] [ --metrics-path = <metrics-path> ] [ --scheme = <scheme> ]","title":"External Services"},{"location":"setting-up/client/external.html#adding-general-external-services","text":"You can collect metrics from an external (custom) exporter on a node when: there is already a PMM Agent instance running and, this node has been configured using the pmm-admin config command.","title":"Adding general external services"},{"location":"setting-up/client/external.html#usage","text":"pmm-admin add external [ --service-name = <service-name> ] [ --listen-port = <listen-port> ] [ --metrics-path = <metrics-path> ] [ --scheme = <scheme> ]","title":"Usage"},{"location":"setting-up/client/linux.html","text":"Linux Adding general system metrics service PMM collects Linux metrics automatically starting from the moment when you have configured your node for monitoring with pmm-admin config .","title":"Linux"},{"location":"setting-up/client/linux.html#adding-general-system-metrics-service","text":"PMM collects Linux metrics automatically starting from the moment when you have configured your node for monitoring with pmm-admin config .","title":"Adding general system metrics service"},{"location":"setting-up/client/mongodb.html","text":"MongoDB Configuring MongoDB for Monitoring in PMM Query Analytics In Query Analytics, you can monitor MongoDB metrics and queries. Run the pmm-admin add command to use these monitoring services. Supported versions of MongoDB Query Analytics supports MongoDB version 3.2 or higher. Setting Up the Required Permissions For MongoDB monitoring services to work in Query Analytics, you need to set up the mongodb_exporter user. Here is an example for the MongoDB shell that creates and assigns the appropriate roles to the user. db . createRole ({ role : \"explainRole\" , privileges : [{ resource : { db : \"\" , collection : \"\" }, actions : [ \"listIndexes\" , \"listCollections\" , \"dbStats\" , \"dbHash\" , \"collStats\" , \"find\" ] }], roles : [] }) db . getSiblingDB ( \"admin\" ). createUser ({ user : \"mongodb_exporter\" , pwd : \"s3cR#tpa$$worD\" , roles : [ { role : \"explainRole\" , db : \"admin\" }, { role : \"clusterMonitor\" , db : \"admin\" }, { role : \"read\" , db : \"local\" } ] }) Enabling Profiling For MongoDB to work correctly with Query Analytics, you need to enable profiling in your mongod configuration. When started without profiling enabled, Query Analytics displays the following warning: Note A warning message is displayed when profiling is not enabled It is required that profiling of the monitored MongoDB databases be enabled, however profiling is not enabled by default because it may reduce the performance of your MongoDB server. Enabling Profiling on Command Line You can enable profiling from command line when you start the mongod server. This command is useful if you start mongod manually. Run this command as root or by using the sudo command mongod --dbpath = DATABASEDIR --profile 2 --slowms 200 --rateLimit 100 Note that you need to specify a path to an existing directory that stores database files with the --dpbath . When the --profile option is set to 2, mongod collects the profiling data for all operations. To decrease the load, you may consider setting this option to 1 so that the profiling data are only collected for slow operations. The --slowms option sets the minimum time for a slow operation. In the given example, any operation which takes longer than 200 milliseconds is a slow operation. The --rateLimit option, which is available if you use PSMDB instead of MongoDB, refers to the number of queries that the MongoDB profiler collects. The lower the rate limit, the less impact on the performance. However, the accuracy of the collected information decreases as well. Enabling Profiling in the Configuration File If you run mongod as a service, you need to use the configuration file which by default is /etc/mongod.conf . In this file, you need to locate the operationProfiling: section and add the following settings: operationProfiling: slowOpThresholdMs: 200 mode: slowOp These settings affect mongod in the same way as the command line options. Note that the configuration file is in the YAML format. In this format the indentation of your lines is important as it defines levels of nesting. Restart the mongod service to enable the settings. Run this command as root or by using the sudo command service mongod restart Adding MongoDB Service Monitoring Add monitoring as follows: pmm-admin add mongodb --username = pmm --password = pmm where username and password are credentials for the monitored MongoDB access, which will be used locally on the database host. Additionally, two positional arguments can be appended to the command line flags: a service name to be used by PMM, and a service address. If not specified, they are substituted automatically as <node>-mongodb and 127.0.0.1:27017 . The command line and the output of this command may look as follows: pmm-admin add mongodb --username = pmm --password = pmm mongo 127 .0.0.1:27017 MongoDB Service added. Service ID : /service_id/f1af8a88-5a95-4bf1-a646-0101f8a20791 Service name: mongo Beside positional arguments shown above you can specify service name and service address with the following flags: --service-name , --host (the hostname or IP address of the service), and --port (the port number of the service). If both flag and positional argument are present, flag gains higher priority. Here is the previous example modified to use these flags: pmm-admin add mongodb --username = pmm --password = pmm --service-name = mongo --host = 127 .0.0.1 --port = 27017 Note It is also possible to add a MongoDB instance using a UNIX socket with just the --socket flag followed by the path to a socket: pmm-admin add mongodb --socket = /tmp/mongodb-27017.sock Passing SSL parameters to the mongodb monitoring service SSL/TLS related parameters are passed to an SSL enabled MongoDB server as monitoring service parameters along with the pmm-admin add command when adding the MongoDB monitoring service. Run this command as root or by using the sudo command pmm-admin add mongodb --tls Supported SSL/TLS Parameters --tls Enable a TLS connection with mongo server --tls-skip-verify Skip TLS certificates validation","title":"MongoDB"},{"location":"setting-up/client/mongodb.html#configuring-mongodb-for-monitoring-in-pmm-query-analytics","text":"In Query Analytics, you can monitor MongoDB metrics and queries. Run the pmm-admin add command to use these monitoring services. Supported versions of MongoDB Query Analytics supports MongoDB version 3.2 or higher.","title":"Configuring MongoDB for Monitoring in PMM Query Analytics"},{"location":"setting-up/client/mongodb.html#setting-up-the-required-permissions","text":"For MongoDB monitoring services to work in Query Analytics, you need to set up the mongodb_exporter user. Here is an example for the MongoDB shell that creates and assigns the appropriate roles to the user. db . createRole ({ role : \"explainRole\" , privileges : [{ resource : { db : \"\" , collection : \"\" }, actions : [ \"listIndexes\" , \"listCollections\" , \"dbStats\" , \"dbHash\" , \"collStats\" , \"find\" ] }], roles : [] }) db . getSiblingDB ( \"admin\" ). createUser ({ user : \"mongodb_exporter\" , pwd : \"s3cR#tpa$$worD\" , roles : [ { role : \"explainRole\" , db : \"admin\" }, { role : \"clusterMonitor\" , db : \"admin\" }, { role : \"read\" , db : \"local\" } ] })","title":"Setting Up the Required Permissions"},{"location":"setting-up/client/mongodb.html#enabling-profiling","text":"For MongoDB to work correctly with Query Analytics, you need to enable profiling in your mongod configuration. When started without profiling enabled, Query Analytics displays the following warning: Note A warning message is displayed when profiling is not enabled It is required that profiling of the monitored MongoDB databases be enabled, however profiling is not enabled by default because it may reduce the performance of your MongoDB server.","title":"Enabling Profiling"},{"location":"setting-up/client/mongodb.html#enabling-profiling-on-command-line","text":"You can enable profiling from command line when you start the mongod server. This command is useful if you start mongod manually. Run this command as root or by using the sudo command mongod --dbpath = DATABASEDIR --profile 2 --slowms 200 --rateLimit 100 Note that you need to specify a path to an existing directory that stores database files with the --dpbath . When the --profile option is set to 2, mongod collects the profiling data for all operations. To decrease the load, you may consider setting this option to 1 so that the profiling data are only collected for slow operations. The --slowms option sets the minimum time for a slow operation. In the given example, any operation which takes longer than 200 milliseconds is a slow operation. The --rateLimit option, which is available if you use PSMDB instead of MongoDB, refers to the number of queries that the MongoDB profiler collects. The lower the rate limit, the less impact on the performance. However, the accuracy of the collected information decreases as well.","title":"Enabling Profiling on Command Line"},{"location":"setting-up/client/mongodb.html#enabling-profiling-in-the-configuration-file","text":"If you run mongod as a service, you need to use the configuration file which by default is /etc/mongod.conf . In this file, you need to locate the operationProfiling: section and add the following settings: operationProfiling: slowOpThresholdMs: 200 mode: slowOp These settings affect mongod in the same way as the command line options. Note that the configuration file is in the YAML format. In this format the indentation of your lines is important as it defines levels of nesting. Restart the mongod service to enable the settings. Run this command as root or by using the sudo command service mongod restart","title":"Enabling Profiling in the Configuration File"},{"location":"setting-up/client/mongodb.html#adding-mongodb-service-monitoring","text":"Add monitoring as follows: pmm-admin add mongodb --username = pmm --password = pmm where username and password are credentials for the monitored MongoDB access, which will be used locally on the database host. Additionally, two positional arguments can be appended to the command line flags: a service name to be used by PMM, and a service address. If not specified, they are substituted automatically as <node>-mongodb and 127.0.0.1:27017 . The command line and the output of this command may look as follows: pmm-admin add mongodb --username = pmm --password = pmm mongo 127 .0.0.1:27017 MongoDB Service added. Service ID : /service_id/f1af8a88-5a95-4bf1-a646-0101f8a20791 Service name: mongo Beside positional arguments shown above you can specify service name and service address with the following flags: --service-name , --host (the hostname or IP address of the service), and --port (the port number of the service). If both flag and positional argument are present, flag gains higher priority. Here is the previous example modified to use these flags: pmm-admin add mongodb --username = pmm --password = pmm --service-name = mongo --host = 127 .0.0.1 --port = 27017 Note It is also possible to add a MongoDB instance using a UNIX socket with just the --socket flag followed by the path to a socket: pmm-admin add mongodb --socket = /tmp/mongodb-27017.sock","title":"Adding MongoDB Service Monitoring"},{"location":"setting-up/client/mongodb.html#passing-ssl-parameters-to-the-mongodb-monitoring-service","text":"SSL/TLS related parameters are passed to an SSL enabled MongoDB server as monitoring service parameters along with the pmm-admin add command when adding the MongoDB monitoring service. Run this command as root or by using the sudo command pmm-admin add mongodb --tls Supported SSL/TLS Parameters --tls Enable a TLS connection with mongo server --tls-skip-verify Skip TLS certificates validation","title":"Passing SSL parameters to the mongodb monitoring service"},{"location":"setting-up/client/mysql.html","text":"MySQL PMM supports all commonly used variants of MySQL, including Percona Server, MariaDB, and Amazon RDS. To prevent data loss and performance issues, PMM does not automatically change MySQL configuration. However, there are certain recommended settings that help maximize monitoring efficiency. These recommendations depend on the variant and version of MySQL you are using, and mostly apply to very high loads. PMM can collect query data either from the slow query log or from Performance Schema . The slow query log provides maximum details, but can impact performance on heavily loaded systems. On Percona Server the query sampling feature may reduce the performance impact. Performance Schema is generally better for recent versions of other MySQL variants. For older MySQL variants, which have neither sampling, nor Performance Schema , configure logging only slow queries. Note MySQL with too many tables can lead to PMM Server overload due to the streaming of too much time series data. It can also lead to too many queries from mysqld_exporter causing extra load on MySQL. Therefore PMM Server disables most consuming mysqld_exporter collectors automatically if there are more than 1000 tables. You can add configuration examples provided below to my.cnf and restart the server or change variables dynamically using the following syntax: SET GLOBAL < var_name >=< var_value > The following sample configurations can be used depending on the variant and version of MySQL: If you are running Percona Server (or XtraDB Cluster), configure the slow query log to capture all queries and enable sampling. This will provide the most amount of information with the lowest overhead. log_output = file slow_query_log = ON long_query_time = 0 log_slow_rate_limit = 100 log_slow_rate_type = query log_slow_verbosity = full log_slow_admin_statements = ON log_slow_slave_statements = ON slow_query_log_always_write_time = 1 slow_query_log_use_global_control = all innodb_monitor_enable = all userstat = 1 If you are running MySQL 5.6+ or MariaDB 10.0+, see Performance Schema . innodb_monitor_enable = all performance_schema = ON If you are running MySQL 5.5 or MariaDB 5.5, configure logging only slow queries to avoid high performance overhead. log_output = file slow_query_log = ON long_query_time = 0 log_slow_admin_statements = ON log_slow_slave_statements = ON Caution This may affect the quality of monitoring data gathered by Query Analytics. Creating a MySQL User Account for PMM When adding a MySQL instance to monitoring, you can specify the MySQL server superuser account credentials. However, monitoring with the superuser account is not advised. It\u2019s better to create a user with only the necessary privileges for collecting data. As an example, the user pmm can be created manually with the necessary privileges and pass its credentials when adding the instance. To enable complete MySQL instance monitoring, a command similar to the following is recommended: sudo pmm-admin add mysql --username pmm --password <password> Of course this user should have necessary privileges for collecting data. If the pmm user already exists, you can grant the required privileges as follows: CREATE USER 'pmm' @ 'localhost' IDENTIFIED BY 'pass' WITH MAX_USER_CONNECTIONS 10 ; GRANT SELECT , PROCESS , SUPER , REPLICATION CLIENT , RELOAD ON * . * TO 'pmm' @ 'localhost' ; Performance Schema The default source of query data for PMM is the slow query log . It is available in MySQL 5.1 and later versions. Starting from MySQL 5.6 (including Percona Server 5.6 and later), you can choose to parse query data from the Performance Schema instead of slow query log . Starting from MySQL 5.6.6, Performance Schema is enabled by default. Performance Schema is not as data-rich as the slow query log , but it has all the critical data and is generally faster to parse. If you are not running Percona Server (which supports sampling for the slow query log), then Performance Schema is a better alternative. Note Use of the performance schema is off by default in MariaDB 10.x. To use Performance Schema , set the performance_schema variable to ON : SHOW VARIABLES LIKE 'performance_schema' ; +--------------------+-------+ | Variable_name | Value | +--------------------+-------+ | performance_schema | ON | +--------------------+-------+ If this variable is not set to ON , add the the following lines to the MySQL configuration file my.cnf and restart MySQL: [mysql] performance_schema = ON If you are running a custom Performance Schema configuration, make sure that the statements_digest consumer is enabled: select * from setup_consumers ; +----------------------------------+---------+ | NAME | ENABLED | +----------------------------------+---------+ | events_stages_current | NO | | events_stages_history | NO | | events_stages_history_long | NO | | events_statements_current | YES | | events_statements_history | YES | | events_statements_history_long | NO | | events_transactions_current | NO | | events_transactions_history | NO | | events_transactions_history_long | NO | | events_waits_current | NO | | events_waits_history | NO | | events_waits_history_long | NO | | global_instrumentation | YES | | thread_instrumentation | YES | | statements_digest | YES | +----------------------------------+---------+ 15 rows in set (0.00 sec) Note Performance Schema instrumentation is enabled by default in MySQL 5.6.6 and later versions. It is not available at all in MySQL versions prior to 5.6. If certain instruments are not enabled, you will not see the corresponding graphs in the MySQL Performance Schema dashboard. To enable full instrumentation, set the option --performance_schema_instrument to '%=on' when starting the MySQL server. mysqld --performance-schema-instrument = '%=on' This option can cause additional overhead and should be used with care. If the instance is already running, configure the QAN agent to collect data from Performance Schema : Open the PMM Query Analytics dashboard. Click the Settings button. Open the Settings section. Select Performance Schema in the Collect from drop-down list. Click Apply to save changes. If you are adding a new monitoring instance with the pmm-admin tool, use the --query-source perfschema option: Run this command as root or by using the sudo command pmm-admin add mysql --username = pmm --password = pmmpassword --query-source = 'perfschema' ps-mysql 127 .0.0.1:3306 For more information, run pmm-admin add mysql --help . Adding MySQL Service Monitoring You add MySQL services (Metrics and Query Analytics) with the following command: USAGE pmm-admin add mysql --query-source = slowlog --username = pmm --password = pmm where username and password are credentials for the monitored MySQL access, which will be used locally on the database host. Additionally, two positional arguments can be appended to the command line flags: a service name to be used by PMM, and a service address. If not specified, they are substituted automatically as <node>-mysql and 127.0.0.1:3306 . The command line and the output of this command may look as follows: pmm-admin add mysql --query-source = slowlog --username = pmm --password = pmm sl-mysql 127 .0.0.1:3306 MySQL Service added. Service ID : /service_id/a89191d4-7d75-44a9-b37f-a528e2c4550f Service name: sl-mysql Note There are two possible sources for query metrics provided by MySQL to get data for the Query Analytics: the slow log and the Performance Schema . The --query-source option can be used to specify it, either as slowlog (it is also used by default if nothing specified) or as perfschema : pmm-admin add mysql --username = pmm --password = pmm --query-source = perfschema ps-mysql 127 .0.0.1:3306 Beside positional arguments shown above you can specify service name and service address with the following flags: --service-name , --host (the hostname or IP address of the service), and --port (the port number of the service). If both flag and positional argument are present, flag gains higher priority. Here is the previous example modified to use these flags: pmm-admin add mysql --username = pmm --password = pmm --service-name = ps-mysql --host = 127 .0.0.1 --port = 3306 Note It is also possible to add MySQL instance using UNIX socket with use of a special --socket flag followed with the path to a socket without username, password and network type: pmm-admin add mysql --socket = /var/path/to/mysql/socket After adding the service you can view MySQL metrics or examine the added node on the new PMM Inventory Dashboard. MySQL InnoDB Metrics Collecting metrics and statistics for graphs increases overhead. You can keep collecting and graphing low-overhead metrics all the time, and enable high-overhead metrics only when troubleshooting problems. InnoDB metrics provide detailed insight about InnoDB operation. Although you can select to capture only specific counters, their overhead is low even when they all are enabled all the time. To enable all InnoDB metrics, set the global variable innodb_monitor_enable to all : SET GLOBAL innodb_monitor_enable = all Slow Log Settings If you are running Percona Server for MySQL, a properly configured slow query log will provide the most amount of information with the lowest overhead. In other cases, use Performance Schema if it is supported. Configuring the Slow Log File The first and obvious variable to enable is slow_query_log which controls the global Slow Query on/off status. Secondly, verify that the log is sent to a FILE instead of a TABLE. This is controlled with the log_output variable. By definition, the slow query log is supposed to capture only slow queries . These are the queries the execution time of which is above a certain threshold. The threshold is defined by the long_query_time variable. In heavily-loaded applications, frequent fast queries can actually have a much bigger impact on performance than rare slow queries. To ensure comprehensive analysis of your query traffic, set the long_query_time to 0 so that all queries are captured. Fine tune Depending on the amount of traffic, logging could become aggressive and resource consuming. However, Percona Server for MySQL provides a way to throttle the level of intensity of the data capture without compromising information. The most important variable is log_slow_rate_limit , which controls the query sampling in Percona Server for MySQL. Details on that variable can be found here . A possible problem with query sampling is that rare slow queries might not get captured at all. To avoid this, use the slow_query_log_always_write_time variable to specify which queries should ignore sampling. That is, queries with longer execution time will always be captured by the slow query log. Slow log file rotation PMM will take care of rotating and removing old slow log files, only if you set the --size-slow-logs variable via pmm-admin . When the limit is reached, PMM will remove the previous old slow log file, rename the current file with the suffix .old , and execute the MySQL command FLUSH LOGS . It will only keep one old file. Older files will be deleted on the next iteration. Configuring MySQL 8.0 for PMM MySQL 8 (in version 8.0.4) changes the way clients are authenticated by default. The default_authentication_plugin parameter is set to caching_sha2_password . This change of the default value implies that MySQL drivers must support the SHA-256 authentication. Also, the communication channel with MySQL 8 must be encrypted when using caching_sha2_password . The MySQL driver used with PMM does not yet support the SHA-256 authentication. With currently supported versions of MySQL, PMM requires that a dedicated MySQL user be set up. This MySQL user should be authenticated using the mysql_native_password plugin. Although MySQL is configured to support SSL clients, connections to MySQL Server are not encrypted. There are two workarounds to be able to add MySQL Server version 8.0.4 or higher as a monitoring service to PMM: Alter the MySQL user that you plan to use with PMM Change the global MySQL configuration Altering the MySQL User Provided you have already created the MySQL user that you plan to use with PMM, alter this user as follows: ALTER USER pmm @ 'localhost' IDENTIFIED WITH mysql_native_password BY '$eCR8Tp@s$w*rD' ; Then, pass this user to pmm-admin add as the value of the --username parameter. This is a preferred approach as it only weakens the security of one user. Changing the global MySQL Configuration A less secure approach is to set default_authentication_plugin to the value mysql_native_password before adding it as a monitoring service. Then, restart your MySQL Server to apply this change. [mysqld] default_authentication_plugin = mysql_native_password","title":"MySQL"},{"location":"setting-up/client/mysql.html#creating-a-mysql-user-account-for-pmm","text":"When adding a MySQL instance to monitoring, you can specify the MySQL server superuser account credentials. However, monitoring with the superuser account is not advised. It\u2019s better to create a user with only the necessary privileges for collecting data. As an example, the user pmm can be created manually with the necessary privileges and pass its credentials when adding the instance. To enable complete MySQL instance monitoring, a command similar to the following is recommended: sudo pmm-admin add mysql --username pmm --password <password> Of course this user should have necessary privileges for collecting data. If the pmm user already exists, you can grant the required privileges as follows: CREATE USER 'pmm' @ 'localhost' IDENTIFIED BY 'pass' WITH MAX_USER_CONNECTIONS 10 ; GRANT SELECT , PROCESS , SUPER , REPLICATION CLIENT , RELOAD ON * . * TO 'pmm' @ 'localhost' ;","title":"Creating a MySQL User Account for PMM"},{"location":"setting-up/client/mysql.html#performance-schema","text":"The default source of query data for PMM is the slow query log . It is available in MySQL 5.1 and later versions. Starting from MySQL 5.6 (including Percona Server 5.6 and later), you can choose to parse query data from the Performance Schema instead of slow query log . Starting from MySQL 5.6.6, Performance Schema is enabled by default. Performance Schema is not as data-rich as the slow query log , but it has all the critical data and is generally faster to parse. If you are not running Percona Server (which supports sampling for the slow query log), then Performance Schema is a better alternative. Note Use of the performance schema is off by default in MariaDB 10.x. To use Performance Schema , set the performance_schema variable to ON : SHOW VARIABLES LIKE 'performance_schema' ; +--------------------+-------+ | Variable_name | Value | +--------------------+-------+ | performance_schema | ON | +--------------------+-------+ If this variable is not set to ON , add the the following lines to the MySQL configuration file my.cnf and restart MySQL: [mysql] performance_schema = ON If you are running a custom Performance Schema configuration, make sure that the statements_digest consumer is enabled: select * from setup_consumers ; +----------------------------------+---------+ | NAME | ENABLED | +----------------------------------+---------+ | events_stages_current | NO | | events_stages_history | NO | | events_stages_history_long | NO | | events_statements_current | YES | | events_statements_history | YES | | events_statements_history_long | NO | | events_transactions_current | NO | | events_transactions_history | NO | | events_transactions_history_long | NO | | events_waits_current | NO | | events_waits_history | NO | | events_waits_history_long | NO | | global_instrumentation | YES | | thread_instrumentation | YES | | statements_digest | YES | +----------------------------------+---------+ 15 rows in set (0.00 sec) Note Performance Schema instrumentation is enabled by default in MySQL 5.6.6 and later versions. It is not available at all in MySQL versions prior to 5.6. If certain instruments are not enabled, you will not see the corresponding graphs in the MySQL Performance Schema dashboard. To enable full instrumentation, set the option --performance_schema_instrument to '%=on' when starting the MySQL server. mysqld --performance-schema-instrument = '%=on' This option can cause additional overhead and should be used with care. If the instance is already running, configure the QAN agent to collect data from Performance Schema : Open the PMM Query Analytics dashboard. Click the Settings button. Open the Settings section. Select Performance Schema in the Collect from drop-down list. Click Apply to save changes. If you are adding a new monitoring instance with the pmm-admin tool, use the --query-source perfschema option: Run this command as root or by using the sudo command pmm-admin add mysql --username = pmm --password = pmmpassword --query-source = 'perfschema' ps-mysql 127 .0.0.1:3306 For more information, run pmm-admin add mysql --help .","title":"Performance Schema"},{"location":"setting-up/client/mysql.html#adding-mysql-service-monitoring","text":"You add MySQL services (Metrics and Query Analytics) with the following command:","title":"Adding MySQL Service Monitoring"},{"location":"setting-up/client/mysql.html#usage","text":"pmm-admin add mysql --query-source = slowlog --username = pmm --password = pmm where username and password are credentials for the monitored MySQL access, which will be used locally on the database host. Additionally, two positional arguments can be appended to the command line flags: a service name to be used by PMM, and a service address. If not specified, they are substituted automatically as <node>-mysql and 127.0.0.1:3306 . The command line and the output of this command may look as follows: pmm-admin add mysql --query-source = slowlog --username = pmm --password = pmm sl-mysql 127 .0.0.1:3306 MySQL Service added. Service ID : /service_id/a89191d4-7d75-44a9-b37f-a528e2c4550f Service name: sl-mysql Note There are two possible sources for query metrics provided by MySQL to get data for the Query Analytics: the slow log and the Performance Schema . The --query-source option can be used to specify it, either as slowlog (it is also used by default if nothing specified) or as perfschema : pmm-admin add mysql --username = pmm --password = pmm --query-source = perfschema ps-mysql 127 .0.0.1:3306 Beside positional arguments shown above you can specify service name and service address with the following flags: --service-name , --host (the hostname or IP address of the service), and --port (the port number of the service). If both flag and positional argument are present, flag gains higher priority. Here is the previous example modified to use these flags: pmm-admin add mysql --username = pmm --password = pmm --service-name = ps-mysql --host = 127 .0.0.1 --port = 3306 Note It is also possible to add MySQL instance using UNIX socket with use of a special --socket flag followed with the path to a socket without username, password and network type: pmm-admin add mysql --socket = /var/path/to/mysql/socket After adding the service you can view MySQL metrics or examine the added node on the new PMM Inventory Dashboard.","title":"USAGE"},{"location":"setting-up/client/mysql.html#mysql-innodb-metrics","text":"Collecting metrics and statistics for graphs increases overhead. You can keep collecting and graphing low-overhead metrics all the time, and enable high-overhead metrics only when troubleshooting problems. InnoDB metrics provide detailed insight about InnoDB operation. Although you can select to capture only specific counters, their overhead is low even when they all are enabled all the time. To enable all InnoDB metrics, set the global variable innodb_monitor_enable to all : SET GLOBAL innodb_monitor_enable = all","title":"MySQL InnoDB Metrics"},{"location":"setting-up/client/mysql.html#slow-log-settings","text":"If you are running Percona Server for MySQL, a properly configured slow query log will provide the most amount of information with the lowest overhead. In other cases, use Performance Schema if it is supported.","title":"Slow Log Settings"},{"location":"setting-up/client/mysql.html#configuring-the-slow-log-file","text":"The first and obvious variable to enable is slow_query_log which controls the global Slow Query on/off status. Secondly, verify that the log is sent to a FILE instead of a TABLE. This is controlled with the log_output variable. By definition, the slow query log is supposed to capture only slow queries . These are the queries the execution time of which is above a certain threshold. The threshold is defined by the long_query_time variable. In heavily-loaded applications, frequent fast queries can actually have a much bigger impact on performance than rare slow queries. To ensure comprehensive analysis of your query traffic, set the long_query_time to 0 so that all queries are captured.","title":"Configuring the Slow Log File"},{"location":"setting-up/client/mysql.html#fine-tune","text":"Depending on the amount of traffic, logging could become aggressive and resource consuming. However, Percona Server for MySQL provides a way to throttle the level of intensity of the data capture without compromising information. The most important variable is log_slow_rate_limit , which controls the query sampling in Percona Server for MySQL. Details on that variable can be found here . A possible problem with query sampling is that rare slow queries might not get captured at all. To avoid this, use the slow_query_log_always_write_time variable to specify which queries should ignore sampling. That is, queries with longer execution time will always be captured by the slow query log.","title":"Fine tune"},{"location":"setting-up/client/mysql.html#slow-log-file-rotation","text":"PMM will take care of rotating and removing old slow log files, only if you set the --size-slow-logs variable via pmm-admin . When the limit is reached, PMM will remove the previous old slow log file, rename the current file with the suffix .old , and execute the MySQL command FLUSH LOGS . It will only keep one old file. Older files will be deleted on the next iteration.","title":"Slow log file rotation"},{"location":"setting-up/client/mysql.html#configuring-mysql-80-for-pmm","text":"MySQL 8 (in version 8.0.4) changes the way clients are authenticated by default. The default_authentication_plugin parameter is set to caching_sha2_password . This change of the default value implies that MySQL drivers must support the SHA-256 authentication. Also, the communication channel with MySQL 8 must be encrypted when using caching_sha2_password . The MySQL driver used with PMM does not yet support the SHA-256 authentication. With currently supported versions of MySQL, PMM requires that a dedicated MySQL user be set up. This MySQL user should be authenticated using the mysql_native_password plugin. Although MySQL is configured to support SSL clients, connections to MySQL Server are not encrypted. There are two workarounds to be able to add MySQL Server version 8.0.4 or higher as a monitoring service to PMM: Alter the MySQL user that you plan to use with PMM Change the global MySQL configuration Altering the MySQL User Provided you have already created the MySQL user that you plan to use with PMM, alter this user as follows: ALTER USER pmm @ 'localhost' IDENTIFIED WITH mysql_native_password BY '$eCR8Tp@s$w*rD' ; Then, pass this user to pmm-admin add as the value of the --username parameter. This is a preferred approach as it only weakens the security of one user. Changing the global MySQL Configuration A less secure approach is to set default_authentication_plugin to the value mysql_native_password before adding it as a monitoring service. Then, restart your MySQL Server to apply this change. [mysqld] default_authentication_plugin = mysql_native_password","title":"Configuring MySQL 8.0 for PMM"},{"location":"setting-up/client/percona-server.html","text":"Percona Server Not all dashboards in Metrics Monitor are available by default for all MySQL variants and configurations: Oracle\u2019s MySQL, Percona Server. or MariaDB. Some graphs require Percona Server, and specialized plugins, or additional configuration. log_slow_rate_limit The log_slow_rate_limit variable defines the fraction of queries captured by the slow query log . A good rule of thumb is to have approximately 100 queries logged per second. For example, if your Percona Server instance processes 10_000 queries per second, you should set log_slow_rate_limit to 100 and capture every 100 th query for the slow query log . Note When using query sampling, set log_slow_rate_type to query so that it applies to queries, rather than sessions. It is also a good idea to set log_slow_verbosity to full so that maximum amount of information about each captured query is stored in the slow query log. log_slow_verbosity log_slow_verbosity variable specifies how much information to include in the slow query log. It is a good idea to set log_slow_verbosity to full so that maximum amount of information about each captured query is stored. slow_query_log_use_global_control By default, slow query log settings apply only to new sessions. If you want to configure the slow query log during runtime and apply these settings to existing connections, set the slow_query_log_use_global_control variable to all . Query Response Time Plugin Query response time distribution is a feature available in Percona Server. It provides information about changes in query response time for different groups of queries, often allowing to spot performance problems before they lead to serious issues. To enable collection of query response time: Install the QUERY_RESPONSE_TIME plugins: INSTALL PLUGIN QUERY_RESPONSE_TIME_AUDIT SONAME 'query_response_time.so' ; INSTALL PLUGIN QUERY_RESPONSE_TIME SONAME 'query_response_time.so' ; INSTALL PLUGIN QUERY_RESPONSE_TIME_READ SONAME 'query_response_time.so' ; INSTALL PLUGIN QUERY_RESPONSE_TIME_WRITE SONAME 'query_response_time.so' ; Set the global variable query_response_time_stats to ON : SET GLOBAL query_response_time_stats = ON ; MySQL User Statistics ( userstat ) User statistics is a feature of Percona Server and MariaDB. It provides information about user activity, individual table and index access. In some cases, collecting user statistics can lead to high overhead, so use this feature sparingly. To enable user statistics, set the userstat variable to 1 .","title":"Percona Server"},{"location":"setting-up/client/percona-server.html#log_slow_rate_limit","text":"The log_slow_rate_limit variable defines the fraction of queries captured by the slow query log . A good rule of thumb is to have approximately 100 queries logged per second. For example, if your Percona Server instance processes 10_000 queries per second, you should set log_slow_rate_limit to 100 and capture every 100 th query for the slow query log . Note When using query sampling, set log_slow_rate_type to query so that it applies to queries, rather than sessions. It is also a good idea to set log_slow_verbosity to full so that maximum amount of information about each captured query is stored in the slow query log.","title":"log_slow_rate_limit"},{"location":"setting-up/client/percona-server.html#log_slow_verbosity","text":"log_slow_verbosity variable specifies how much information to include in the slow query log. It is a good idea to set log_slow_verbosity to full so that maximum amount of information about each captured query is stored.","title":"log_slow_verbosity"},{"location":"setting-up/client/percona-server.html#slow_query_log_use_global_control","text":"By default, slow query log settings apply only to new sessions. If you want to configure the slow query log during runtime and apply these settings to existing connections, set the slow_query_log_use_global_control variable to all .","title":"slow_query_log_use_global_control"},{"location":"setting-up/client/percona-server.html#query-response-time-plugin","text":"Query response time distribution is a feature available in Percona Server. It provides information about changes in query response time for different groups of queries, often allowing to spot performance problems before they lead to serious issues. To enable collection of query response time: Install the QUERY_RESPONSE_TIME plugins: INSTALL PLUGIN QUERY_RESPONSE_TIME_AUDIT SONAME 'query_response_time.so' ; INSTALL PLUGIN QUERY_RESPONSE_TIME SONAME 'query_response_time.so' ; INSTALL PLUGIN QUERY_RESPONSE_TIME_READ SONAME 'query_response_time.so' ; INSTALL PLUGIN QUERY_RESPONSE_TIME_WRITE SONAME 'query_response_time.so' ; Set the global variable query_response_time_stats to ON : SET GLOBAL query_response_time_stats = ON ;","title":"Query Response Time Plugin"},{"location":"setting-up/client/percona-server.html#mysql-user-statistics-userstat","text":"User statistics is a feature of Percona Server and MariaDB. It provides information about user activity, individual table and index access. In some cases, collecting user statistics can lead to high overhead, so use this feature sparingly. To enable user statistics, set the userstat variable to 1 .","title":"MySQL User Statistics (userstat)"},{"location":"setting-up/client/postgresql.html","text":"PostgreSQL PMM follows the postgresql.org EOL policy . For specific details on supported platforms and versions, see Percona\u2019s Software Platform Lifecycle page . To monitor PostgreSQL queries, you must install a database extension. There are two choices: pg_stat_monitor , a new extension created by Percona, based on pg_stat_statements and compatible with it. pg_stat_statements , the original extension created by PostgreSQL, part of the postgres-contrib package available on Linux. pg_stat_monitor provides all the features of pg_stat_statements , but extends it to provide bucket-based data aggregation, a feature missing from pg_stat_statements . ( pg_stat_statements accumulates data without providing aggregated statistics or histogram information.) Note pg_stat_monitor is the recommended option. Although nothing prevents you from installing and using both, we don\u2019t recommend this as you will get duplicate metrics. Caution pg_stat_monitor is beta software and currently unsupported. Prerequisites We recommend that you create a PostgreSQL user for SUPERUSER level access. This lets you gather the most data with the least fuss. This user must be able to connect to the postgres database where the extension was installed. The PostgreSQL user should have local password authentication enabled to access PMM. To do this, set ident to md5 for the user in the pg_hba.conf configuration file. To create a superuser: CREATE USER pmm_user WITH SUPERUSER ENCRYPTED PASSWORD '******' ; Or, if your database runs on Amazon RDS: CREATE USER pmm_user WITH rds_superuser ENCRYPTED PASSWORD '******' ; pg_stat_monitor pg_stat_monitor collects statistics and aggregates data in a data collection unit called a bucket linked together to form a bucket chain . You can specify: the number of buckets (the length of the chain); how much space is available for all buckets; a time limit for each bucket\u2019s data collection (the bucket expiry ). When a bucket\u2019s expiration time is reached, accumulated statistics are reset and data is stored in the next available bucket in the chain. When all buckets in the chain have been used, the first bucket is reused and its contents are overwritten. If a bucket fills before its expiration time is reached, data is discarded. Compatibility pg_stat_monitor has been tested with: PostgreSQL versions 11, 12. Percona Distribution for PostgreSQL versions 11, 12. (It should also work with versions 13 of both, but hasn\u2019t been tested.) Install This extension can be installed in two ways: For Percona Distribution for PostgreSQL: Using standard Linux package manager tools. For PostgreSQL or Percona Distribution for PostgreSQL: download and compile the source code . Install using Linux package manager The pg-stat-monitor extension is included in Percona Distribution for PostgreSQL . This can be installed via the percona-release package. This section reproduces parts of the following: Configuring Percona Repositories with percona-release Installing Percona Distribution for PostgreSQL Debian sudo apt-get install -y wget gnupg2 lsb-release wget https://repo.percona.com/apt/percona-release_latest.generic_all.deb sudo dpkg -i percona-release_latest.generic_all.deb sudo percona-release setup ppg-12 # version 12 (others available) sudo apt install -y percona-postgresql-12 Red Hat sudo yum install -y https://repo.percona.com/yum/percona-release-latest.noarch.rpm # If RHEL 8 sudo dnf module disable postgresql # If RHEL 7 sudo yum install -y epel-release sudo yum repolist sudo percona-release setup ppg-12 sudo yum install -y percona-postgresql12-server Install from source code Debian Install common packages sudo apt-get install -y curl git wget gnupg2 lsb-release sudo apt-get update -y Install PostgreSQL development packages With Percona Distribution for PostgreSQL (version 12): wget https://repo.percona.com/apt/percona-release_latest.generic_all.deb sudo dpkg -i percona-release_latest.generic_all.deb sudo percona-release setup ppg-12 sudo apt install -y percona-postgresql-server-dev-all With PostgreSQL: wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - echo \"deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\" | sudo tee /etc/apt/sources.list.d/pgdg.list sudo apt install -y postgresql-server-dev-all Download, compile, and install extension git clone git://github.com/percona/pg_stat_monitor.git && cd pg_stat_monitor sudo make USE_PGXS = 1 sudo make USE_PGXS = 1 install Red Hat Install common packages sudo yum install -y centos-release-scl epel-release sudo yum update -y sudo yum install -y git gcc gcc-c++ llvm-toolset-7 Install PostgreSQL development packages With Percona Distribution for PostgreSQL (version 12): sudo yum install -y https://repo.percona.com/yum/percona-release-latest.noarch.rpm sudo percona-release setup ppg-12 sudo yum install -y percona-postgresql12-devel With PostgreSQL version 12: sudo yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm sudo yum install -y postgresql12-devel Download, compile, and install extension git clone git://github.com/percona/pg_stat_monitor.git && cd pg_stat_monitor sudo make PG_CONFIG = /usr/pgsql-12/bin/pg_config USE_PGXS = 1 sudo make PG_CONFIG = /usr/pgsql-12/bin/pg_config USE_PGXS = 1 install Configure Set or change the value for shared_preload_library in your postgresql.conf file: shared_preload_libraries = 'pg_stat_monitor' Set the value pg_stat_monitor.pgsm_normalized_query Start or restart your PostgreSQL instance. In a psql session: CREATE EXTENSION pg_stat_monitor ; Configuration Parameters Here are the configuration parameters, available values ranges, and default values. All require a restart of PostgreSQL except for pg_stat_monitor.pgsm_track_utility and pg_stat_monitor.pgsm_normalized_query . To make settings permanent, add them to your postgresql.conf file before starting your PostgreSQL instance. pg_stat_monitor.pgsm_max (5000-2147483647 bytes) Default: 5000 Defines the limit of shared memory. Memory is used by buckets in a circular manner and is divided between buckets equally when PostgreSQL starts. pg_stat_monitor.pgsm_query_max_len (1024-2147483647 bytes) Default: 1024 The maximum size of the query. Long queries are truncated to this length to avoid unnecessary usage of shared memory. This parameter must be set before PostgreSQL starts. pg_stat_monitor.pgsm_enable (0-1) Default: 1 (true). Enables or disables monitoring. A value of Disable means that pg_stat_monitor will not collect statistics for the entire cluster. pg_stat_monitor.pgsm_track_utility (0-1) Default: 1 (true) Controls whether utility commands (all except SELECT, INSERT, UPDATE and DELETE) are tracked. pg_stat_monitor.pgsm_normalized_query (0-1) Default: 0 (false) By default, a query shows the actual parameter instead of a placeholder. Set to 1 to change to showing value placeholders (as $n where n is an integer). pg_stat_monitor.pgsm_max_buckets (1-10) Default: 10 Sets the maximum number of available data buckets. pg_stat_monitor.pgsm_bucket_time (1-2147483647 seconds) Default: 60 Sets the lifetime of the bucket. The system switches between buckets on the basis of this value. pg_stat_monitor.pgsm_object_cache (50-2147483647) Default: 50 The maximum number of objects in the information cache. pg_stat_monitor.pgsm_respose_time_lower_bound (1-2147483647 milliseconds) Default: 1 Sets the lower bound of the execution time histogram. pg_stat_monitor.pgsm_respose_time_step (1-2147483647 milliseconds) Default: 1 Sets the time value of the steps for the histogram. pg_stat_monitor.pgsm_query_shared_buffer (500000-2147483647 bytes) Default: 500000 Sets the query shared_buffer size. pg_stat_monitor.pgsm_track_planning (0-1) Default: 1 (true) Whether to track planning statistics. pg_stat_statements pg_stat_statements is included in the official PostgreSQL postgresql-contrib available from your Linux distribution package manager. Install Debian sudo apt-get install postgresql-contrib Red Hat sudo yum install -y postgresql-contrib Configure Add these lines to your postgresql.conf file: shared_preload_libraries = 'pg_stat_statements' track_activity_query_size = 2048 # Increase tracked query string size pg_stat_statements.track = all # Track all statements including nested Restart your PostgreSQL instance. Install the extension (run in the postgres database). CREATE EXTENSION pg_stat_statements SCHEMA public ; Adding PostgreSQL queries and metrics monitoring You add PostgreSQL metrics and queries monitoring with the following command: pmm-admin add postgresql --username = <user name> --password = <password> Where <user name> and <password> are the PostgreSQL user credentials. Additionally, two positional arguments can be appended to the command line flags: a service name to be used by PMM, and a service address. If not specified, they are substituted automatically as <node>-postgresql and 127.0.0.1:5432 . The command line and the output of this command may look as follows: pmm-admin add postgresql --username = pmm --password = pmm postgres 127 .0.0.1:5432 PostgreSQL Service added. Service ID : /service_id/28f1d93a-5c16-467f-841b-8c014bf81ca6 Service name: postgres If correct installed and set up, you should be able to see data in PostgreSQL Overview dashboard, and also Query Analytics should contain PostgreSQL queries. Beside positional arguments shown above you can specify service name and service address with the following flags: --service-name , --host (the hostname or IP address of the service), and --port (the port number of the service). If both flag and positional argument are present, flag gains higher priority. Here is the previous example modified to use these flags: pmm-admin add postgresql --username = pmm --password = pmm --service-name = postgres --host = 127 .0.0.1 --port = 270175432 It is also possible to add a PostgreSQL instance using a UNIX socket with just the --socket flag followed by the path to a socket: pmm-admin add postgresql --socket = /var/run/postgresql Capturing read and write time statistics is possible only if track_io_timing setting is enabled. This can be done either in configuration file or with the following query executed on the running system: ALTER SYSTEM SET track_io_timing = ON ; SELECT pg_reload_conf () ;","title":"PostgreSQL"},{"location":"setting-up/client/postgresql.html#prerequisites","text":"We recommend that you create a PostgreSQL user for SUPERUSER level access. This lets you gather the most data with the least fuss. This user must be able to connect to the postgres database where the extension was installed. The PostgreSQL user should have local password authentication enabled to access PMM. To do this, set ident to md5 for the user in the pg_hba.conf configuration file. To create a superuser: CREATE USER pmm_user WITH SUPERUSER ENCRYPTED PASSWORD '******' ; Or, if your database runs on Amazon RDS: CREATE USER pmm_user WITH rds_superuser ENCRYPTED PASSWORD '******' ;","title":"Prerequisites"},{"location":"setting-up/client/postgresql.html#pg_stat_monitor","text":"pg_stat_monitor collects statistics and aggregates data in a data collection unit called a bucket linked together to form a bucket chain . You can specify: the number of buckets (the length of the chain); how much space is available for all buckets; a time limit for each bucket\u2019s data collection (the bucket expiry ). When a bucket\u2019s expiration time is reached, accumulated statistics are reset and data is stored in the next available bucket in the chain. When all buckets in the chain have been used, the first bucket is reused and its contents are overwritten. If a bucket fills before its expiration time is reached, data is discarded.","title":"pg_stat_monitor"},{"location":"setting-up/client/postgresql.html#compatibility","text":"pg_stat_monitor has been tested with: PostgreSQL versions 11, 12. Percona Distribution for PostgreSQL versions 11, 12. (It should also work with versions 13 of both, but hasn\u2019t been tested.)","title":"Compatibility"},{"location":"setting-up/client/postgresql.html#install","text":"This extension can be installed in two ways: For Percona Distribution for PostgreSQL: Using standard Linux package manager tools. For PostgreSQL or Percona Distribution for PostgreSQL: download and compile the source code .","title":"Install"},{"location":"setting-up/client/postgresql.html#configure","text":"Set or change the value for shared_preload_library in your postgresql.conf file: shared_preload_libraries = 'pg_stat_monitor' Set the value pg_stat_monitor.pgsm_normalized_query Start or restart your PostgreSQL instance. In a psql session: CREATE EXTENSION pg_stat_monitor ;","title":"Configure"},{"location":"setting-up/client/postgresql.html#configuration-parameters","text":"Here are the configuration parameters, available values ranges, and default values. All require a restart of PostgreSQL except for pg_stat_monitor.pgsm_track_utility and pg_stat_monitor.pgsm_normalized_query . To make settings permanent, add them to your postgresql.conf file before starting your PostgreSQL instance. pg_stat_monitor.pgsm_max (5000-2147483647 bytes) Default: 5000 Defines the limit of shared memory. Memory is used by buckets in a circular manner and is divided between buckets equally when PostgreSQL starts. pg_stat_monitor.pgsm_query_max_len (1024-2147483647 bytes) Default: 1024 The maximum size of the query. Long queries are truncated to this length to avoid unnecessary usage of shared memory. This parameter must be set before PostgreSQL starts. pg_stat_monitor.pgsm_enable (0-1) Default: 1 (true). Enables or disables monitoring. A value of Disable means that pg_stat_monitor will not collect statistics for the entire cluster. pg_stat_monitor.pgsm_track_utility (0-1) Default: 1 (true) Controls whether utility commands (all except SELECT, INSERT, UPDATE and DELETE) are tracked. pg_stat_monitor.pgsm_normalized_query (0-1) Default: 0 (false) By default, a query shows the actual parameter instead of a placeholder. Set to 1 to change to showing value placeholders (as $n where n is an integer). pg_stat_monitor.pgsm_max_buckets (1-10) Default: 10 Sets the maximum number of available data buckets. pg_stat_monitor.pgsm_bucket_time (1-2147483647 seconds) Default: 60 Sets the lifetime of the bucket. The system switches between buckets on the basis of this value. pg_stat_monitor.pgsm_object_cache (50-2147483647) Default: 50 The maximum number of objects in the information cache. pg_stat_monitor.pgsm_respose_time_lower_bound (1-2147483647 milliseconds) Default: 1 Sets the lower bound of the execution time histogram. pg_stat_monitor.pgsm_respose_time_step (1-2147483647 milliseconds) Default: 1 Sets the time value of the steps for the histogram. pg_stat_monitor.pgsm_query_shared_buffer (500000-2147483647 bytes) Default: 500000 Sets the query shared_buffer size. pg_stat_monitor.pgsm_track_planning (0-1) Default: 1 (true) Whether to track planning statistics.","title":"Configuration Parameters"},{"location":"setting-up/client/postgresql.html#pg_stat_statements","text":"pg_stat_statements is included in the official PostgreSQL postgresql-contrib available from your Linux distribution package manager.","title":"pg_stat_statements"},{"location":"setting-up/client/postgresql.html#install_1","text":"","title":"Install"},{"location":"setting-up/client/postgresql.html#configure_1","text":"Add these lines to your postgresql.conf file: shared_preload_libraries = 'pg_stat_statements' track_activity_query_size = 2048 # Increase tracked query string size pg_stat_statements.track = all # Track all statements including nested Restart your PostgreSQL instance. Install the extension (run in the postgres database). CREATE EXTENSION pg_stat_statements SCHEMA public ;","title":"Configure"},{"location":"setting-up/client/postgresql.html#adding-postgresql-queries-and-metrics-monitoring","text":"You add PostgreSQL metrics and queries monitoring with the following command: pmm-admin add postgresql --username = <user name> --password = <password> Where <user name> and <password> are the PostgreSQL user credentials. Additionally, two positional arguments can be appended to the command line flags: a service name to be used by PMM, and a service address. If not specified, they are substituted automatically as <node>-postgresql and 127.0.0.1:5432 . The command line and the output of this command may look as follows: pmm-admin add postgresql --username = pmm --password = pmm postgres 127 .0.0.1:5432 PostgreSQL Service added. Service ID : /service_id/28f1d93a-5c16-467f-841b-8c014bf81ca6 Service name: postgres If correct installed and set up, you should be able to see data in PostgreSQL Overview dashboard, and also Query Analytics should contain PostgreSQL queries. Beside positional arguments shown above you can specify service name and service address with the following flags: --service-name , --host (the hostname or IP address of the service), and --port (the port number of the service). If both flag and positional argument are present, flag gains higher priority. Here is the previous example modified to use these flags: pmm-admin add postgresql --username = pmm --password = pmm --service-name = postgres --host = 127 .0.0.1 --port = 270175432 It is also possible to add a PostgreSQL instance using a UNIX socket with just the --socket flag followed by the path to a socket: pmm-admin add postgresql --socket = /var/run/postgresql Capturing read and write time statistics is possible only if track_io_timing setting is enabled. This can be done either in configuration file or with the following query executed on the running system: ALTER SYSTEM SET track_io_timing = ON ; SELECT pg_reload_conf () ;","title":"Adding PostgreSQL queries and metrics monitoring"},{"location":"setting-up/client/proxysql.html","text":"ProxySQL Use the proxysql alias to enable ProxySQL performance metrics monitoring. USAGE pmm-admin add proxysql --username = admin --password = admin where username and password are credentials for the administration interface of the monitored ProxySQL instance. Additionally, two positional arguments can be appended to the command line flags: a service name to be used by PMM, and a service address. If not specified, they are substituted automatically as <node>-proxysql and 127.0.0.1:6032 . The output of this command may look as follows: pmm-admin add proxysql --username = admin --password = admin ProxySQL Service added. Service ID : /service_id/f69df379-6584-4db5-a896-f35ae8c97573 Service name: ubuntu-proxysql Beside positional arguments shown above you can specify service name and service address with the following flags: --service-name , and --host (the hostname or IP address of the service) and --port (the port number of the service), or --socket (the UNIX socket path). If both flag and positional argument are present, flag gains higher priority. Here is the previous example modified to use these flags for both host/port or socket connections: pmm-admin add proxysql --username = pmm --password = pmm --service-name = my-new-proxysql --host = 127 .0.0.1 --port = 6032 pmm-admin add proxysql --username = pmm --password = pmm --service-name = my-new-proxysql --socket = /tmp/proxysql_admin.sock","title":"ProxySQL"},{"location":"setting-up/client/proxysql.html#usage","text":"pmm-admin add proxysql --username = admin --password = admin where username and password are credentials for the administration interface of the monitored ProxySQL instance. Additionally, two positional arguments can be appended to the command line flags: a service name to be used by PMM, and a service address. If not specified, they are substituted automatically as <node>-proxysql and 127.0.0.1:6032 . The output of this command may look as follows: pmm-admin add proxysql --username = admin --password = admin ProxySQL Service added. Service ID : /service_id/f69df379-6584-4db5-a896-f35ae8c97573 Service name: ubuntu-proxysql Beside positional arguments shown above you can specify service name and service address with the following flags: --service-name , and --host (the hostname or IP address of the service) and --port (the port number of the service), or --socket (the UNIX socket path). If both flag and positional argument are present, flag gains higher priority. Here is the previous example modified to use these flags for both host/port or socket connections: pmm-admin add proxysql --username = pmm --password = pmm --service-name = my-new-proxysql --host = 127 .0.0.1 --port = 6032 pmm-admin add proxysql --username = pmm --password = pmm --service-name = my-new-proxysql --socket = /tmp/proxysql_admin.sock","title":"USAGE"},{"location":"setting-up/server/index.html","text":"Setting up PMM Server PMM Server runs as a Docker image , a virtual appliance , or on an AWS instance . Verifying In your browser, go to the server by its IP address. If you run your server as a virtual appliance or by using an Amazon machine image, you will need to setup the user name, password and your public key if you intend to connect to the server by using ssh. This step is not needed if you run PMM Server using Docker. In the given example, you would need to direct your browser to http://192.168.100.1 . Since you have not added any monitoring services yet, the site will show only data related to the PMM Server internal services. Accessing the Components of the Web Interface http://192.168.100.1 to access Home Dashboard http://192.168.100.1/graph/ to access Metrics Monitor http://192.168.100.1/swagger/ to access PMM API . PMM Server provides user access control, and therefore you will need user credentials to access it: The default user name is admin , and the default password is admin also. You will be proposed to change the default password at login if you didn\u2019t it.","title":"Setting up PMM Server"},{"location":"setting-up/server/index.html#verifying","text":"In your browser, go to the server by its IP address. If you run your server as a virtual appliance or by using an Amazon machine image, you will need to setup the user name, password and your public key if you intend to connect to the server by using ssh. This step is not needed if you run PMM Server using Docker. In the given example, you would need to direct your browser to http://192.168.100.1 . Since you have not added any monitoring services yet, the site will show only data related to the PMM Server internal services.","title":"Verifying"},{"location":"setting-up/server/index.html#accessing-the-components-of-the-web-interface","text":"http://192.168.100.1 to access Home Dashboard http://192.168.100.1/graph/ to access Metrics Monitor http://192.168.100.1/swagger/ to access PMM API . PMM Server provides user access control, and therefore you will need user credentials to access it: The default user name is admin , and the default password is admin also. You will be proposed to change the default password at login if you didn\u2019t it.","title":"Accessing the Components of the Web Interface"},{"location":"setting-up/server/aws.html","text":"AWS Marketplace You can run an instance of PMM Server hosted at AWS Marketplace. Assuming that you have an AWS (Amazon Web Services) account, locate Percona Monitoring and Management Server in AWS Marketplace (or use this link ). Selecting a region and instance type in the Pricing Information section will give you an estimate of the costs involved. This is only an indication of costs. You will choose regions and instance types in later steps. Percona Monitoring and Management Server is provided at no cost, but you may need to pay for infrastructure costs. Note Disk space consumed by PMM Server depends on the number of hosts being monitored. Although each environment will be unique, you can consider the data consumption figures for the PMM Demo web site which consumes approximately 230MB/host/day, or ~6.9GB/host at the default 30 day retention period. For more information, see our blog post How much disk space should I allocate for Percona Monitoring and Management? . Click Continue to Subscribe . Subscribe to this software : Check the terms and conditions and click Continue to Configuration . Configure this software : Select a value for Software Version . (The latest is 2.13.0) Select a region. (You can change this in the next step.) Click Continue to Launch . Launch this software : Choose Action : Select a launch option. Launch from Website is a quick way to make your instance ready. For more control, choose Launch through EC2 . EC2 Instance Type : Select an instance type. VPC Settings : Choose or create a VPC (virtual private cloud). Subnet Settings : Choose or create a subnet. Security Group Settings : Choose a security group or click *Create New Based On Seller Settings Key Pair Settings : Choose or create a key pair. Click Launch . Limiting Access to the instance: security group and a key pair In the Security Group section, which acts like a firewall, you may use the preselected option Create new based on seller settings to create a security group with recommended settings. In the Key Pair select an already set up EC2 key pair to limit access to your instance. Note It is important that the security group allow communication via the the following ports: 22 , 80 , and 443 . PMM should also be able to access port 3306 on the RDS that uses the instance. Applying settings Scroll up to the top of the page to view your settings. Then, click the Launch with 1 click button to continue and adjust your settings in the EC2 console. Your instance settings are summarized in a special area. Click the Launch with 1 click button to continue. Note The Launch with 1 click button may alternatively be titled as Accept Software Terms & Launch with 1-Click . Adjusting instance settings in the EC2 Console Your clicking the Launch with 1 click button, deploys your instance. To continue setting up your instance, run the EC2 console. It is available as a link at the top of the page that opens after you click the Launch with 1 click button. Your instance appears in the EC2 console in a table that lists all instances available to you. When a new instance is only created, it has no name. Make sure that you give it a name to distinguish from other instances managed via the EC2 console. Running the instance After you add your new instance it will take some time to initialize it. When the AWS console reports that the instance is now in a running state, you many continue with configuration of PMM Server. Note When started the next time after rebooting, your instance may acquire another IP address. You may choose to set up an elastic IP to avoid this problem. With your instance selected, open its IP address in a web browser. The IP address appears in the IPv4 Public IP column or as value of the Public IP field at the top of the Properties panel. To run the instance, copy and paste its public IP address to the location bar of your browser. In the Percona Monitoring and Management welcome page that opens, enter the instance ID. You can copy the instance ID from the Properties panel of your instance, select the Description tab back in the EC2 console. Click the Copy button next to the Instance ID field. This button appears as soon as you hover the cursor of your mouse over the ID. Hover the cursor over the instance ID for the Copy button to appear. Paste the instance in the Instance ID field of the Percona Monitoring and Management welcome page and click Submit . PMM Server provides user access control, and therefore you will need user credentials to access it: Default user name: admin Default password: admin You will be prompted to change the default password every time you log in. The PMM Server is now ready and the home page opens. You are creating a username and password that will be used for two purposes: authentication as a user to PMM - this will be the credentials you need in order to log in to PMM. authentication between PMM Server and PMM Clients - you will re-use these credentials when configuring pmm-client for the first time on a server, for example: pmm-admin config --server-insecure-tls --server-url = https://admin:admin@<IP Address>:443 Note For instructions about how to access your instances by using an SSH client, see Connecting to Your Linux Instance Using SSH Make sure to replace the user name ec2-user used in this document with admin . Resizing the EBS Volume Your AWS instance comes with a predefined size which can become a limitation. To make more disk space available to your instance, you need to increase the size of the EBS volume as needed and then your instance will reconfigure itself to use the new size. The procedure of resizing EBS volumes is described in the Amazon documentation: Modifying the Size, IOPS, or Type of an EBS Volume on Linux . After the EBS volume is updated, PMM Server instance will auto-detect changes in approximately 5 minutes or less and will reconfigure itself for the updated conditions. Upgrading PMM Server on AWS Upgrading EC2 instance class Upgrading to a larger EC2 instance class is supported by PMM provided you follow the instructions from the AWS manual . The PMM AMI image uses a distinct EBS volume for the PMM data volume which permits independent resize of the EC2 instance without impacting the EBS volume. Expanding the PMM Data EBS Volume The PMM data volume is mounted as an XFS formatted volume on top of an LVM volume. There are two ways to increase this volume size: Add a new disk via EC2 console or API, and expand the LVM volume to include the new disk volume. Expand existing EBS volume and grow the LVM volume. Expand existing EBS volume To expand the existing EBS volume in order to increase capacity, the following steps should be followed. Expand the disk from AWS Console/CLI to the desired capacity. Login to the PMM EC2 instance and verify that the disk capacity has increased. For example, if you have expanded disk from 16G to 32G, dmesg output should look like below: [ 535.994494] xvdb: detected capacity change from 17179869184 to 34359738368 You can check information about volume groups and logical volumes with the vgs and lvs commands: vgs VG #PV #LV #SN Attr VSize VFree DataVG 1 2 0 wz--n- <16.00g 0 lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.74 ThinPool DataVG twi-aotz-- 15.96g 1.39 1.29 Now we can use the lsblk command to see that our disk size has been identified by the kernel correctly, but LVM2 is not yet aware of the new size. We can use pvresize to make sure the PV device reflects the new size. Once pvresize is executed, we can see that the VG has the new free space available. lsblk | grep xvdb xvdb 202:16 0 32G 0 disk pvscan PV /dev/xvdb VG DataVG lvm2 [<16.00 GiB / 0 free] Total: 1 [<16.00 GiB] / in use: 1 [<16.00 GiB] / in no VG: 0 [0 ] pvresize /dev/xvdb Physical volume \"/dev/xvdb\" changed 1 physical volume(s) resized / 0 physical volume(s) not resized pvs PV VG Fmt Attr PSize PFree /dev/xvdb DataVG lvm2 a-- <32.00g 16.00g We then extend our logical volume. Since the PMM image uses thin provisioning, we need to extend both the pool and the volume: lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.77 ThinPool DataVG twi-aotz-- 15.96g 1.42 1.32 lvextend /dev/mapper/DataVG-ThinPool -l 100 %VG Size of logical volume DataVG/ThinPool_tdata changed from 16.00 GiB (4096 extents) to 31.96 GiB (8183 extents). Logical volume DataVG/ThinPool_tdata successfully resized. lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.77 ThinPool DataVG twi-aotz-- 31.96g 0.71 1.71 Once the pool and volumes have been extended, we need to now extend the thin volume to consume the newly available space. In this example we\u2019ve grown available space to almost 32GB, and already consumed 12GB, so we\u2019re extending an additional 19GB: lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.77 ThinPool DataVG twi-aotz-- 31.96g 0.71 1.71 lvextend /dev/mapper/DataVG-DataLV -L +19G Size of logical volume DataVG/DataLV changed from <12.80 GiB (3276 extents) to <31.80 GiB (8140 extents). Logical volume DataVG/DataLV successfully resized. lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <31.80g ThinPool 0.71 ThinPool DataVG twi-aotz-- 31.96g 0.71 1.71 We then expand the XFS filesystem to reflect the new size using xfs_growfs , and confirm the filesystem is accurate using the df command. df -h /srv Filesystem Size Used Avail Use% Mounted on /dev/mapper/DataVG-DataLV 13G 249M 13G 2% /srv xfs_growfs /srv meta-data=/dev/mapper/DataVG-DataLV isize=512 agcount=103, agsize=32752 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0 spinodes=0 data = bsize=4096 blocks=3354624, imaxpct=25 = sunit=16 swidth=16 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal bsize=4096 blocks=768, version=2 = sectsz=512 sunit=16 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 data blocks changed from 3354624 to 8335360 df -h /srv Filesystem Size Used Avail Use% Mounted on /dev/mapper/DataVG-DataLV 32G 254M 32G 1% /srv","title":"AWS Marketplace"},{"location":"setting-up/server/aws.html#limiting-access-to-the-instance-security-group-and-a-key-pair","text":"In the Security Group section, which acts like a firewall, you may use the preselected option Create new based on seller settings to create a security group with recommended settings. In the Key Pair select an already set up EC2 key pair to limit access to your instance. Note It is important that the security group allow communication via the the following ports: 22 , 80 , and 443 . PMM should also be able to access port 3306 on the RDS that uses the instance.","title":"Limiting Access to the instance: security group and a key pair"},{"location":"setting-up/server/aws.html#applying-settings","text":"Scroll up to the top of the page to view your settings. Then, click the Launch with 1 click button to continue and adjust your settings in the EC2 console. Your instance settings are summarized in a special area. Click the Launch with 1 click button to continue. Note The Launch with 1 click button may alternatively be titled as Accept Software Terms & Launch with 1-Click .","title":"Applying settings"},{"location":"setting-up/server/aws.html#adjusting-instance-settings-in-the-ec2-console","text":"Your clicking the Launch with 1 click button, deploys your instance. To continue setting up your instance, run the EC2 console. It is available as a link at the top of the page that opens after you click the Launch with 1 click button. Your instance appears in the EC2 console in a table that lists all instances available to you. When a new instance is only created, it has no name. Make sure that you give it a name to distinguish from other instances managed via the EC2 console.","title":"Adjusting instance settings in the EC2 Console"},{"location":"setting-up/server/aws.html#running-the-instance","text":"After you add your new instance it will take some time to initialize it. When the AWS console reports that the instance is now in a running state, you many continue with configuration of PMM Server. Note When started the next time after rebooting, your instance may acquire another IP address. You may choose to set up an elastic IP to avoid this problem. With your instance selected, open its IP address in a web browser. The IP address appears in the IPv4 Public IP column or as value of the Public IP field at the top of the Properties panel. To run the instance, copy and paste its public IP address to the location bar of your browser. In the Percona Monitoring and Management welcome page that opens, enter the instance ID. You can copy the instance ID from the Properties panel of your instance, select the Description tab back in the EC2 console. Click the Copy button next to the Instance ID field. This button appears as soon as you hover the cursor of your mouse over the ID. Hover the cursor over the instance ID for the Copy button to appear. Paste the instance in the Instance ID field of the Percona Monitoring and Management welcome page and click Submit . PMM Server provides user access control, and therefore you will need user credentials to access it: Default user name: admin Default password: admin You will be prompted to change the default password every time you log in. The PMM Server is now ready and the home page opens. You are creating a username and password that will be used for two purposes: authentication as a user to PMM - this will be the credentials you need in order to log in to PMM. authentication between PMM Server and PMM Clients - you will re-use these credentials when configuring pmm-client for the first time on a server, for example: pmm-admin config --server-insecure-tls --server-url = https://admin:admin@<IP Address>:443 Note For instructions about how to access your instances by using an SSH client, see Connecting to Your Linux Instance Using SSH Make sure to replace the user name ec2-user used in this document with admin .","title":"Running the instance"},{"location":"setting-up/server/aws.html#resizing-the-ebs-volume","text":"Your AWS instance comes with a predefined size which can become a limitation. To make more disk space available to your instance, you need to increase the size of the EBS volume as needed and then your instance will reconfigure itself to use the new size. The procedure of resizing EBS volumes is described in the Amazon documentation: Modifying the Size, IOPS, or Type of an EBS Volume on Linux . After the EBS volume is updated, PMM Server instance will auto-detect changes in approximately 5 minutes or less and will reconfigure itself for the updated conditions.","title":"Resizing the EBS Volume"},{"location":"setting-up/server/aws.html#upgrading-pmm-server-on-aws","text":"","title":"Upgrading PMM Server on AWS"},{"location":"setting-up/server/aws.html#upgrading-ec2-instance-class","text":"Upgrading to a larger EC2 instance class is supported by PMM provided you follow the instructions from the AWS manual . The PMM AMI image uses a distinct EBS volume for the PMM data volume which permits independent resize of the EC2 instance without impacting the EBS volume.","title":"Upgrading EC2 instance class"},{"location":"setting-up/server/aws.html#expanding-the-pmm-data-ebs-volume","text":"The PMM data volume is mounted as an XFS formatted volume on top of an LVM volume. There are two ways to increase this volume size: Add a new disk via EC2 console or API, and expand the LVM volume to include the new disk volume. Expand existing EBS volume and grow the LVM volume.","title":"Expanding the PMM Data EBS Volume"},{"location":"setting-up/server/aws.html#expand-existing-ebs-volume","text":"To expand the existing EBS volume in order to increase capacity, the following steps should be followed. Expand the disk from AWS Console/CLI to the desired capacity. Login to the PMM EC2 instance and verify that the disk capacity has increased. For example, if you have expanded disk from 16G to 32G, dmesg output should look like below: [ 535.994494] xvdb: detected capacity change from 17179869184 to 34359738368 You can check information about volume groups and logical volumes with the vgs and lvs commands: vgs VG #PV #LV #SN Attr VSize VFree DataVG 1 2 0 wz--n- <16.00g 0 lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.74 ThinPool DataVG twi-aotz-- 15.96g 1.39 1.29 Now we can use the lsblk command to see that our disk size has been identified by the kernel correctly, but LVM2 is not yet aware of the new size. We can use pvresize to make sure the PV device reflects the new size. Once pvresize is executed, we can see that the VG has the new free space available. lsblk | grep xvdb xvdb 202:16 0 32G 0 disk pvscan PV /dev/xvdb VG DataVG lvm2 [<16.00 GiB / 0 free] Total: 1 [<16.00 GiB] / in use: 1 [<16.00 GiB] / in no VG: 0 [0 ] pvresize /dev/xvdb Physical volume \"/dev/xvdb\" changed 1 physical volume(s) resized / 0 physical volume(s) not resized pvs PV VG Fmt Attr PSize PFree /dev/xvdb DataVG lvm2 a-- <32.00g 16.00g We then extend our logical volume. Since the PMM image uses thin provisioning, we need to extend both the pool and the volume: lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.77 ThinPool DataVG twi-aotz-- 15.96g 1.42 1.32 lvextend /dev/mapper/DataVG-ThinPool -l 100 %VG Size of logical volume DataVG/ThinPool_tdata changed from 16.00 GiB (4096 extents) to 31.96 GiB (8183 extents). Logical volume DataVG/ThinPool_tdata successfully resized. lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.77 ThinPool DataVG twi-aotz-- 31.96g 0.71 1.71 Once the pool and volumes have been extended, we need to now extend the thin volume to consume the newly available space. In this example we\u2019ve grown available space to almost 32GB, and already consumed 12GB, so we\u2019re extending an additional 19GB: lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <12.80g ThinPool 1.77 ThinPool DataVG twi-aotz-- 31.96g 0.71 1.71 lvextend /dev/mapper/DataVG-DataLV -L +19G Size of logical volume DataVG/DataLV changed from <12.80 GiB (3276 extents) to <31.80 GiB (8140 extents). Logical volume DataVG/DataLV successfully resized. lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert DataLV DataVG Vwi-aotz-- <31.80g ThinPool 0.71 ThinPool DataVG twi-aotz-- 31.96g 0.71 1.71 We then expand the XFS filesystem to reflect the new size using xfs_growfs , and confirm the filesystem is accurate using the df command. df -h /srv Filesystem Size Used Avail Use% Mounted on /dev/mapper/DataVG-DataLV 13G 249M 13G 2% /srv xfs_growfs /srv meta-data=/dev/mapper/DataVG-DataLV isize=512 agcount=103, agsize=32752 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0 spinodes=0 data = bsize=4096 blocks=3354624, imaxpct=25 = sunit=16 swidth=16 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal bsize=4096 blocks=768, version=2 = sectsz=512 sunit=16 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 data blocks changed from 3354624 to 8335360 df -h /srv Filesystem Size Used Avail Use% Mounted on /dev/mapper/DataVG-DataLV 32G 254M 32G 1% /srv","title":"Expand existing EBS volume"},{"location":"setting-up/server/dbaas.html","text":"Setting up a development environment for DBaaS Caution DBaaS functionality is Alpha. The information on this page is subject to change and may be inaccurate. Software prerequisites Docker minikube Start PMM server with DBaaS activated Install Percona operators in minikube Installing Percona operators in AWS EKS (Kubernetes) Install Operators on GKE Deleting clusters Run PMM Server as a Docker container for DBaaS Exposing PSMDB and XtraDB clusters for access by external clients Software prerequisites Docker Red Hat, CentOS yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum -y install docker-ce usermod -a -G docker centos systemctl enable docker systemctl start docker Debian, Ubuntu apt-add-repository https://download.docker.com/linux/centos/docker-ce.repo systemctl enable docker systemctl start docker minikube Red Hat, CentOS yum -y install curl curl -Lo /usr/local/sbin/minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 chmod +x /usr/local/sbin/minikube ln -s /usr/local/sbin/minikube /usr/sbin/minikube alias kubectl = 'minikube kubectl --' Start PMM server with DBaaS activated Notes To start a fully-working 3 node XtraDB cluster, consisting of sets of 3x ProxySQL, 3x PXC and 6x PMM Client containers, you will need at least 9 vCPU available for minikube. (1x vCPU for ProxySQL and PXC and 0.5vCPU for each pmm-client containers). DBaaS does not depend on PMM Client. Setting the environment variable PERCONA_TEST_DBAAS=1 enables DBaaS functionality. Add the option --network minikube if you run PMM Server and minikube in the same Docker instance. (This will share a single network and the kubeconfig will work.) Add the options --env PMM_DEBUG=1 and/or --env PMM_TRACE=1 if you need extended debug details Start PMM server: docker run --detach --publish 80 :80 --publish 443 :443 --name pmm-server --env PERCONA_TEST_DBAAS = 1 percona/pmm-server:2 Change the default administrator credentials from CLI: (This step is optional, because the same can be done from the web interface of PMM on first login.) docker exec -t pmm-server bash -c 'ln -s /srv/grafana /usr/share/grafana/data; chown -R grafana:grafana /usr/share/grafana/data; grafana-cli --homepath /usr/share/grafana admin reset-admin-password <RANDOM_PASS_GOES_IN_HERE>' Install Percona operators in minikube Configure and start minikube: minikube config set cpus 16 minikube config set memory 32768 minikube config set kubernetes-version 1 .16.15 minikube start Deploy the Percona operators configuration for PXC and PSMDB in minikube: # Prepare a set of base64 encoded values and non encoded for user and pass with administrator privileges to pmm-server (DBaaS) PMM_USER = 'admin' ; PMM_PASS = '<RANDOM_PASS_GOES_IN_HERE>' ; PMM_USER_B64 = \" $( echo -n \" ${ PMM_USER } \" | base64 ) \" ; PMM_PASS_B64 = \" $( echo -n \" ${ PMM_PASS } \" | base64 ) \" ; # Install the PXC operator curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/pmm-branch/deploy/bundle.yaml \\ | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/pmm-branch/deploy/secrets.yaml \\ | sed \"s/pmmserver:.*=/pmmserver: ${ PMM_PASS_B64 } /g\" \\ | kubectl apply -f - # Install the PSMDB operator curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/v1.6.0/deploy/bundle.yaml \\ | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/v1.6.0/deploy/secrets.yaml \\ | sed \"s/PMM_SERVER_USER:.* $ /PMM_SERVER_USER: ${ PMM_USER } /g;s/PMM_SERVER_PASSWORD:.* $ /PMM_SERVER_PASSWORD: ${ PMM_PASS } /g;\" \\ | kubectl apply -f - Check the operators are deployed: minikube kubectl -- get nodes minikube kubectl -- get pods minikube kubectl -- wait --for = condition = Available deployment percona-xtradb-cluster-operator minikube kubectl -- wait --for = condition = Available deployment percona-server-mongodb-operator Get your kubeconfig details from minikube (to register your Kubernetes cluster with PMM Server): Note You will need to copy this output to your clipboard and continue with add a Kubernetes cluster to PMM . Installing Percona operators in AWS EKS (Kubernetes) Create your cluster via eksctl or the Amazon AWS interface. For example: eksctl create cluster --write-kubeconfig --name = your-cluster-name --zones = us-west-2a,us-west-2b --kubeconfig <PATH_TO_KUBECONFIG> When your EKS cluster is running, install the PXC and PSMDB operators: # Prepare a set of base64 encoded values and non encoded for user and pass with administrator privileges to pmm-server (DBaaS) PMM_USER = 'admin' ; PMM_PASS = '<RANDOM_PASS_GOES_IN_HERE>' ; PMM_USER_B64 = \" $( echo -n \" ${ PMM_USER } \" | base64 ) \" ; PMM_PASS_B64 = \" $( echo -n \" ${ PMM_PASS } \" | base64 ) \" ; # Install the PXC operator curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/pmm-branch/deploy/bundle.yaml \\ | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/pmm-branch/deploy/secrets.yaml \\ | sed \"s/pmmserver:.*=/pmmserver: ${ PMM_PASS_B64 } /g\" \\ | kubectl apply -f - # Install the PSMDB operator curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/v1.6.0/deploy/bundle.yaml \\ | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/v1.6.0/deploy/secrets.yaml \\ | sed \"s/PMM_SERVER_USER:.* $ /PMM_SERVER_USER: ${ PMM_USER } /g;s/PMM_SERVER_PASSWORD:.* $ /PMM_SERVER_PASSWORD: ${ PMM_PASS } /g;\" \\ | kubectl apply -f - # Validate that the operators are running kubectl get pods Modify your kubeconfig file, if it\u2019s not utilizing the aws-iam-authenticator or client-certificate method for authentication with Kubernetes. Here are two examples that you can use as templates to modify a copy of your existing kubeconfig: For the aws-iam-authenticator method: --- apiVersion: v1 clusters: - cluster: certificate-authority-data: << CERT_AUTH_DATA >> server: << K8S_CLUSTER_URL >> name: << K8S_CLUSTER_NAME >> contexts: - context: cluster: << K8S_CLUSTER_NAME >> user: << K8S_CLUSTER_USER >> name: << K8S_CLUSTER_NAME >> current-context: << K8S_CLUSTER_NAME >> kind: Config preferences: {} users: - name: << K8S_CLUSTER_USER >> user: exec: apiVersion: client.authentication.k8s.io/v1alpha1 command: aws-iam-authenticator args: - \"token\" - \"-i\" - \"<< K8S_CLUSTER_NAME >>\" - --region - << AWS_REGION >> env: - name: AWS_ACCESS_KEY_ID value: \"<< AWS_ACCESS_KEY_ID >>\" - name: AWS_SECRET_ACCESS_KEY value: \"<< AWS_SECRET_ACCESS_KEY >>\" For the client-certificate method: --- apiVersion: v1 clusters: - cluster: certificate-authority-data: << CERT_AUTH_DATA >> server: << K8S_CLUSTER_URL >> name: << K8S_CLUSTER_NAME >> contexts: - context: cluster: << K8S_CLUSTER_NAME >> user: << K8S_CLUSTER_USER >> name: << K8S_CLUSTER_NAME >> current-context: << K8S_CLUSTER_NAME >> kind: Config preferences: {} users: - name: << K8S_CLUSTER_NAME >> user: client-certificate-data: << CLIENT_CERT_DATA >> client-key-data: << CLIENT_KEY_DATA >> Follow the instructions for Add a Kubernetes cluster . Note If possible, the connection details will show the cluster\u2019s external IP (not possible with minikube). Install Operators on GKE Caution These instructions are still in development. Prerequisites You should have an account on GCP https://cloud.google.com/ . Login into google cloud platform console https://console.cloud.google.com/ Navigate to Menu \u2192 Kubernetes Engine \u2192 Clusters Click button Create cluster You can specify cluster option in form or simply click on \u201cMy first cluster\u201d and button Create Wait until cluster created Click on button Connect in a the cluster\u2019s row Click button Run in Cloud shell Click Authorize Set up PXC and PSMDB operators: curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/pmm-branch/deploy/bundle.yaml | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/pmm-branch/deploy/secrets.yaml | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/pmm-branch/deploy/bundle.yaml | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/pmm-branch/deploy/secrets.yaml | kubectl apply -f - Check if it was set up successfully kubectl api-resources --api-group='psmdb.percona.com' kubectl api-resources --api-group='pxc.percona.com' Check versions kubectl api-versions | grep percona.com Create Service Account, copy and store kubeconfig - output of the following command cat <<EOF | kubectl apply -f - --- apiVersion: v1 kind: ServiceAccount metadata: name: percona-dbaas-cluster-operator --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: service-account-percona-server-dbaas-xtradb-operator subjects: - kind: ServiceAccount name: percona-dbaas-cluster-operator roleRef: kind: Role name: percona-xtradb-cluster-operator apiGroup: rbac.authorization.k8s.io --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: service-account-percona-server-dbaas-psmdb-operator subjects: - kind: ServiceAccount name: percona-dbaas-cluster-operator roleRef: kind: Role name: percona-server-mongodb-operator apiGroup: rbac.authorization.k8s.io EOF name=`kubectl get serviceAccounts percona-dbaas-cluster-operator -o json | jq -r .secrets[].name` certificate=`kubectl get secret $name -o json | jq -r '.data.\"ca.crt\"'` token=`kubectl get secret $name -o json | jq -r '.data.token' | base64 -d` server=`kubectl cluster-info | grep 'Kubernetes master' | cut -d ' ' -f 6` echo \" apiVersion: v1 kind: Config users: - name: percona-dbaas-cluster-operator user: token: $token clusters: - cluster: certificate-authority-data: $certificate server: $server name: self-hosted-cluster contexts: - context: cluster: self-hosted-cluster user: percona-dbaas-cluster-operator name: svcs-acct-context current-context: svcs-acct-context \" Start PMM Server on you local machine or other vm instance: docker run --detach --name pmm-server --publish 80:80 --publish 443:443 \\ --env PERCONA_TEST_DBAAS=1 perconalab/pmm-server-fb:PR-1240-07bef94; Login into PMM and navigate to DBaaS Register your GKE using kubeconfig from step 12. Important Please make sure there are no stray new lines in the kubeconfig, especially in long lines like certificate or token. Deleting clusters You should delete all installation operators as the operators own resources. If you only run eksctl delete cluster without cleaning up the cluster first, there will be a lot of orphaned resources as Cloud Formations, Load Balancers, EC2 instances, Network interfaces, etc. In the pmm-managed repository, in the deploy directory there are 2 example bash scripts to install and delete the operators from the EKS cluster. The install script: #!/bin/bash TOP_DIR = $( git rev-parse --show-toplevel ) PMM_USER = \" $( echo -n 'admin' | base64 ) \" ; PMM_PASS = \" $( echo -n 'admin_password' | base64 ) \" ; KUBECTL_CMD = \"kubectl --kubeconfig ${ HOME } /.kube/config_eks\" # Install the PXC operator cat ${ TOP_DIR } /deploy/pxc_operator.yaml | ${ KUBECTL_CMD } apply -f - cat ${ TOP_DIR } /deploy/secrets.yaml | sed \"s/pmmserver:.*=/pmmserver: ${ PMM_PASS } /g\" | ${ KUBECTL_CMD } apply -f - # Install the PSMDB operator cat ${ TOP_DIR } /deploy/psmdb_operator.yaml | ${ KUBECTL_CMD } apply -f - cat ${ TOP_DIR } /deploy/secrets.yaml | sed \"s/PMM_SERVER_USER:.* $ /PMM_SERVER_USER: ${ PMM_USER } /g;s/PMM_SERVER_PASSWORD:.*= $ /PMM_SERVER_PASSWORD: ${ PMM_PASS } /g;\" | ${ KUBECTL_CMD } apply -f - The delete script: #!/bin/bash TOP_DIR = $( git rev-parse --show-toplevel ) PMM_USER = \" $( echo -n 'admin' | base64 ) \" ; PMM_PASS = \" $( echo -n 'admin_password' | base64 ) \" ; KUBECTL_CMD = \"kubectl --kubeconfig ${ HOME } /.kube/config_eks\" # Delete the PXC operator cat ${ TOP_DIR } /deploy/pxc_operator.yaml | ${ KUBECTL_CMD } delete -f - cat ${ TOP_DIR } /deploy/secrets.yaml | sed \"s/pmmserver:.*=/pmmserver: ${ PMM_PASS } /g\" | ${ KUBECTL_CMD } delete -f - # Delete the PSMDB operator cat ${ TOP_DIR } /deploy/psmdb_operator.yaml | ${ KUBECTL_CMD } delete -f - cat ${ TOP_DIR } /deploy/secrets.yaml | sed \"s/PMM_SERVER_USER:.* $ /PMM_SERVER_USER: ${ PMM_USER } /g;s/PMM_SERVER_PASSWORD:.*= $ /PMM_SERVER_PASSWORD: ${ PMM_PASS } /g;\" | ${ KUBECTL_CMD } delete -f - (Both scripts are similar except the install script command is apply while in the delete script it is delete .) After deleting everything in the EKS cluster, run this command (using your own configuration path) and wait until the output only shows service/kubernetes before deleting the cluster with the eksclt delete command. kubectl --kubeconfig ~/.kube/config_eks get all Example output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 4d5h If you don\u2019t need the cluster anymore, you can uninstall everything in it and destroy it: # Delete all volumes created by the operators: kubectl [ --kubeconfig <config file> ] delete pvc --all # Delete the cluster eksctl delete cluster --name = your-cluster-name Run PMM Server as a Docker container for DBaaS Start PMM server from a feature branch: docker run --detach --name pmm-server --publish 80 :80 --publish 443 :443 --env PERCONA_TEST_DBAAS = 1 percona/pmm-server:2 ; Important Use --network minikube if running PMM Server and minikube in the same Docker instance. This way they will share single network and the kubeconfig will work. Use Docker variables --env PMM_DEBUG=1 --env PMM_TRACE=1 to see extended debug details. Change the default administrator credentials: Note This step is optional, because the same can be done from the web interface of PMM on the first login. docker exec -t pmm-server bash -c 'ln -s /srv/grafana /usr/share/grafana/data; chown -R grafana:grafana /usr/share/grafana/data; grafana-cli --homepath /usr/share/grafana admin reset-admin-password <RANDOM_PASS_GOES_IN_HERE>' Set the public address for PMM Server in PMM settings UI Follow the steps for Add a Kubernetes cluster . Follow the steps for Add a DB Cluster . Get the IP address to connect your app/service: minikube kubectl get services Exposing PSMDB and XtraDB clusters for access by external clients To make services visible externally, you create a LoadBalancer service or manually run commands to expose ports: kubectl expose deployment hello-world --type = NodePort. See also DBaaS Dashboard Install minikube Setting up a Standalone MYSQL Instance on Kubernetes & exposing it using Nginx Ingress Controller Use a Service to Access an Application in a Cluster. Exposing applications using services.","title":"Setting up a development environment for DBaaS"},{"location":"setting-up/server/dbaas.html#software-prerequisites","text":"","title":"Software prerequisites"},{"location":"setting-up/server/dbaas.html#docker","text":"Red Hat, CentOS yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum -y install docker-ce usermod -a -G docker centos systemctl enable docker systemctl start docker Debian, Ubuntu apt-add-repository https://download.docker.com/linux/centos/docker-ce.repo systemctl enable docker systemctl start docker","title":"Docker"},{"location":"setting-up/server/dbaas.html#minikube","text":"Red Hat, CentOS yum -y install curl curl -Lo /usr/local/sbin/minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 chmod +x /usr/local/sbin/minikube ln -s /usr/local/sbin/minikube /usr/sbin/minikube alias kubectl = 'minikube kubectl --'","title":"minikube"},{"location":"setting-up/server/dbaas.html#start-pmm-server-with-dbaas-activated","text":"Notes To start a fully-working 3 node XtraDB cluster, consisting of sets of 3x ProxySQL, 3x PXC and 6x PMM Client containers, you will need at least 9 vCPU available for minikube. (1x vCPU for ProxySQL and PXC and 0.5vCPU for each pmm-client containers). DBaaS does not depend on PMM Client. Setting the environment variable PERCONA_TEST_DBAAS=1 enables DBaaS functionality. Add the option --network minikube if you run PMM Server and minikube in the same Docker instance. (This will share a single network and the kubeconfig will work.) Add the options --env PMM_DEBUG=1 and/or --env PMM_TRACE=1 if you need extended debug details Start PMM server: docker run --detach --publish 80 :80 --publish 443 :443 --name pmm-server --env PERCONA_TEST_DBAAS = 1 percona/pmm-server:2 Change the default administrator credentials from CLI: (This step is optional, because the same can be done from the web interface of PMM on first login.) docker exec -t pmm-server bash -c 'ln -s /srv/grafana /usr/share/grafana/data; chown -R grafana:grafana /usr/share/grafana/data; grafana-cli --homepath /usr/share/grafana admin reset-admin-password <RANDOM_PASS_GOES_IN_HERE>'","title":"Start PMM server with DBaaS activated"},{"location":"setting-up/server/dbaas.html#install-percona-operators-in-minikube","text":"Configure and start minikube: minikube config set cpus 16 minikube config set memory 32768 minikube config set kubernetes-version 1 .16.15 minikube start Deploy the Percona operators configuration for PXC and PSMDB in minikube: # Prepare a set of base64 encoded values and non encoded for user and pass with administrator privileges to pmm-server (DBaaS) PMM_USER = 'admin' ; PMM_PASS = '<RANDOM_PASS_GOES_IN_HERE>' ; PMM_USER_B64 = \" $( echo -n \" ${ PMM_USER } \" | base64 ) \" ; PMM_PASS_B64 = \" $( echo -n \" ${ PMM_PASS } \" | base64 ) \" ; # Install the PXC operator curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/pmm-branch/deploy/bundle.yaml \\ | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/pmm-branch/deploy/secrets.yaml \\ | sed \"s/pmmserver:.*=/pmmserver: ${ PMM_PASS_B64 } /g\" \\ | kubectl apply -f - # Install the PSMDB operator curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/v1.6.0/deploy/bundle.yaml \\ | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/v1.6.0/deploy/secrets.yaml \\ | sed \"s/PMM_SERVER_USER:.* $ /PMM_SERVER_USER: ${ PMM_USER } /g;s/PMM_SERVER_PASSWORD:.* $ /PMM_SERVER_PASSWORD: ${ PMM_PASS } /g;\" \\ | kubectl apply -f - Check the operators are deployed: minikube kubectl -- get nodes minikube kubectl -- get pods minikube kubectl -- wait --for = condition = Available deployment percona-xtradb-cluster-operator minikube kubectl -- wait --for = condition = Available deployment percona-server-mongodb-operator Get your kubeconfig details from minikube (to register your Kubernetes cluster with PMM Server): Note You will need to copy this output to your clipboard and continue with add a Kubernetes cluster to PMM .","title":"Install Percona operators in minikube"},{"location":"setting-up/server/dbaas.html#installing-percona-operators-in-aws-eks-kubernetes","text":"Create your cluster via eksctl or the Amazon AWS interface. For example: eksctl create cluster --write-kubeconfig --name = your-cluster-name --zones = us-west-2a,us-west-2b --kubeconfig <PATH_TO_KUBECONFIG> When your EKS cluster is running, install the PXC and PSMDB operators: # Prepare a set of base64 encoded values and non encoded for user and pass with administrator privileges to pmm-server (DBaaS) PMM_USER = 'admin' ; PMM_PASS = '<RANDOM_PASS_GOES_IN_HERE>' ; PMM_USER_B64 = \" $( echo -n \" ${ PMM_USER } \" | base64 ) \" ; PMM_PASS_B64 = \" $( echo -n \" ${ PMM_PASS } \" | base64 ) \" ; # Install the PXC operator curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/pmm-branch/deploy/bundle.yaml \\ | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/pmm-branch/deploy/secrets.yaml \\ | sed \"s/pmmserver:.*=/pmmserver: ${ PMM_PASS_B64 } /g\" \\ | kubectl apply -f - # Install the PSMDB operator curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/v1.6.0/deploy/bundle.yaml \\ | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/v1.6.0/deploy/secrets.yaml \\ | sed \"s/PMM_SERVER_USER:.* $ /PMM_SERVER_USER: ${ PMM_USER } /g;s/PMM_SERVER_PASSWORD:.* $ /PMM_SERVER_PASSWORD: ${ PMM_PASS } /g;\" \\ | kubectl apply -f - # Validate that the operators are running kubectl get pods Modify your kubeconfig file, if it\u2019s not utilizing the aws-iam-authenticator or client-certificate method for authentication with Kubernetes. Here are two examples that you can use as templates to modify a copy of your existing kubeconfig: For the aws-iam-authenticator method: --- apiVersion: v1 clusters: - cluster: certificate-authority-data: << CERT_AUTH_DATA >> server: << K8S_CLUSTER_URL >> name: << K8S_CLUSTER_NAME >> contexts: - context: cluster: << K8S_CLUSTER_NAME >> user: << K8S_CLUSTER_USER >> name: << K8S_CLUSTER_NAME >> current-context: << K8S_CLUSTER_NAME >> kind: Config preferences: {} users: - name: << K8S_CLUSTER_USER >> user: exec: apiVersion: client.authentication.k8s.io/v1alpha1 command: aws-iam-authenticator args: - \"token\" - \"-i\" - \"<< K8S_CLUSTER_NAME >>\" - --region - << AWS_REGION >> env: - name: AWS_ACCESS_KEY_ID value: \"<< AWS_ACCESS_KEY_ID >>\" - name: AWS_SECRET_ACCESS_KEY value: \"<< AWS_SECRET_ACCESS_KEY >>\" For the client-certificate method: --- apiVersion: v1 clusters: - cluster: certificate-authority-data: << CERT_AUTH_DATA >> server: << K8S_CLUSTER_URL >> name: << K8S_CLUSTER_NAME >> contexts: - context: cluster: << K8S_CLUSTER_NAME >> user: << K8S_CLUSTER_USER >> name: << K8S_CLUSTER_NAME >> current-context: << K8S_CLUSTER_NAME >> kind: Config preferences: {} users: - name: << K8S_CLUSTER_NAME >> user: client-certificate-data: << CLIENT_CERT_DATA >> client-key-data: << CLIENT_KEY_DATA >> Follow the instructions for Add a Kubernetes cluster . Note If possible, the connection details will show the cluster\u2019s external IP (not possible with minikube).","title":"Installing Percona operators in AWS EKS (Kubernetes)"},{"location":"setting-up/server/dbaas.html#install-operators-on-gke","text":"Caution These instructions are still in development. Prerequisites You should have an account on GCP https://cloud.google.com/ . Login into google cloud platform console https://console.cloud.google.com/ Navigate to Menu \u2192 Kubernetes Engine \u2192 Clusters Click button Create cluster You can specify cluster option in form or simply click on \u201cMy first cluster\u201d and button Create Wait until cluster created Click on button Connect in a the cluster\u2019s row Click button Run in Cloud shell Click Authorize Set up PXC and PSMDB operators: curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/pmm-branch/deploy/bundle.yaml | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-xtradb-cluster-operator/pmm-branch/deploy/secrets.yaml | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/pmm-branch/deploy/bundle.yaml | kubectl apply -f - curl -sSf -m 30 https://raw.githubusercontent.com/percona/percona-server-mongodb-operator/pmm-branch/deploy/secrets.yaml | kubectl apply -f - Check if it was set up successfully kubectl api-resources --api-group='psmdb.percona.com' kubectl api-resources --api-group='pxc.percona.com' Check versions kubectl api-versions | grep percona.com Create Service Account, copy and store kubeconfig - output of the following command cat <<EOF | kubectl apply -f - --- apiVersion: v1 kind: ServiceAccount metadata: name: percona-dbaas-cluster-operator --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: service-account-percona-server-dbaas-xtradb-operator subjects: - kind: ServiceAccount name: percona-dbaas-cluster-operator roleRef: kind: Role name: percona-xtradb-cluster-operator apiGroup: rbac.authorization.k8s.io --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: service-account-percona-server-dbaas-psmdb-operator subjects: - kind: ServiceAccount name: percona-dbaas-cluster-operator roleRef: kind: Role name: percona-server-mongodb-operator apiGroup: rbac.authorization.k8s.io EOF name=`kubectl get serviceAccounts percona-dbaas-cluster-operator -o json | jq -r .secrets[].name` certificate=`kubectl get secret $name -o json | jq -r '.data.\"ca.crt\"'` token=`kubectl get secret $name -o json | jq -r '.data.token' | base64 -d` server=`kubectl cluster-info | grep 'Kubernetes master' | cut -d ' ' -f 6` echo \" apiVersion: v1 kind: Config users: - name: percona-dbaas-cluster-operator user: token: $token clusters: - cluster: certificate-authority-data: $certificate server: $server name: self-hosted-cluster contexts: - context: cluster: self-hosted-cluster user: percona-dbaas-cluster-operator name: svcs-acct-context current-context: svcs-acct-context \" Start PMM Server on you local machine or other vm instance: docker run --detach --name pmm-server --publish 80:80 --publish 443:443 \\ --env PERCONA_TEST_DBAAS=1 perconalab/pmm-server-fb:PR-1240-07bef94; Login into PMM and navigate to DBaaS Register your GKE using kubeconfig from step 12. Important Please make sure there are no stray new lines in the kubeconfig, especially in long lines like certificate or token.","title":"Install Operators on GKE"},{"location":"setting-up/server/dbaas.html#deleting-clusters","text":"You should delete all installation operators as the operators own resources. If you only run eksctl delete cluster without cleaning up the cluster first, there will be a lot of orphaned resources as Cloud Formations, Load Balancers, EC2 instances, Network interfaces, etc. In the pmm-managed repository, in the deploy directory there are 2 example bash scripts to install and delete the operators from the EKS cluster. The install script: #!/bin/bash TOP_DIR = $( git rev-parse --show-toplevel ) PMM_USER = \" $( echo -n 'admin' | base64 ) \" ; PMM_PASS = \" $( echo -n 'admin_password' | base64 ) \" ; KUBECTL_CMD = \"kubectl --kubeconfig ${ HOME } /.kube/config_eks\" # Install the PXC operator cat ${ TOP_DIR } /deploy/pxc_operator.yaml | ${ KUBECTL_CMD } apply -f - cat ${ TOP_DIR } /deploy/secrets.yaml | sed \"s/pmmserver:.*=/pmmserver: ${ PMM_PASS } /g\" | ${ KUBECTL_CMD } apply -f - # Install the PSMDB operator cat ${ TOP_DIR } /deploy/psmdb_operator.yaml | ${ KUBECTL_CMD } apply -f - cat ${ TOP_DIR } /deploy/secrets.yaml | sed \"s/PMM_SERVER_USER:.* $ /PMM_SERVER_USER: ${ PMM_USER } /g;s/PMM_SERVER_PASSWORD:.*= $ /PMM_SERVER_PASSWORD: ${ PMM_PASS } /g;\" | ${ KUBECTL_CMD } apply -f - The delete script: #!/bin/bash TOP_DIR = $( git rev-parse --show-toplevel ) PMM_USER = \" $( echo -n 'admin' | base64 ) \" ; PMM_PASS = \" $( echo -n 'admin_password' | base64 ) \" ; KUBECTL_CMD = \"kubectl --kubeconfig ${ HOME } /.kube/config_eks\" # Delete the PXC operator cat ${ TOP_DIR } /deploy/pxc_operator.yaml | ${ KUBECTL_CMD } delete -f - cat ${ TOP_DIR } /deploy/secrets.yaml | sed \"s/pmmserver:.*=/pmmserver: ${ PMM_PASS } /g\" | ${ KUBECTL_CMD } delete -f - # Delete the PSMDB operator cat ${ TOP_DIR } /deploy/psmdb_operator.yaml | ${ KUBECTL_CMD } delete -f - cat ${ TOP_DIR } /deploy/secrets.yaml | sed \"s/PMM_SERVER_USER:.* $ /PMM_SERVER_USER: ${ PMM_USER } /g;s/PMM_SERVER_PASSWORD:.*= $ /PMM_SERVER_PASSWORD: ${ PMM_PASS } /g;\" | ${ KUBECTL_CMD } delete -f - (Both scripts are similar except the install script command is apply while in the delete script it is delete .) After deleting everything in the EKS cluster, run this command (using your own configuration path) and wait until the output only shows service/kubernetes before deleting the cluster with the eksclt delete command. kubectl --kubeconfig ~/.kube/config_eks get all Example output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.100.0.1 <none> 443/TCP 4d5h If you don\u2019t need the cluster anymore, you can uninstall everything in it and destroy it: # Delete all volumes created by the operators: kubectl [ --kubeconfig <config file> ] delete pvc --all # Delete the cluster eksctl delete cluster --name = your-cluster-name","title":"Deleting clusters"},{"location":"setting-up/server/dbaas.html#run-pmm-server-as-a-docker-container-for-dbaas","text":"Start PMM server from a feature branch: docker run --detach --name pmm-server --publish 80 :80 --publish 443 :443 --env PERCONA_TEST_DBAAS = 1 percona/pmm-server:2 ; Important Use --network minikube if running PMM Server and minikube in the same Docker instance. This way they will share single network and the kubeconfig will work. Use Docker variables --env PMM_DEBUG=1 --env PMM_TRACE=1 to see extended debug details. Change the default administrator credentials: Note This step is optional, because the same can be done from the web interface of PMM on the first login. docker exec -t pmm-server bash -c 'ln -s /srv/grafana /usr/share/grafana/data; chown -R grafana:grafana /usr/share/grafana/data; grafana-cli --homepath /usr/share/grafana admin reset-admin-password <RANDOM_PASS_GOES_IN_HERE>' Set the public address for PMM Server in PMM settings UI Follow the steps for Add a Kubernetes cluster . Follow the steps for Add a DB Cluster . Get the IP address to connect your app/service: minikube kubectl get services","title":"Run PMM Server as a Docker container for DBaaS"},{"location":"setting-up/server/dbaas.html#exposing-psmdb-and-xtradb-clusters-for-access-by-external-clients","text":"To make services visible externally, you create a LoadBalancer service or manually run commands to expose ports: kubectl expose deployment hello-world --type = NodePort. See also DBaaS Dashboard Install minikube Setting up a Standalone MYSQL Instance on Kubernetes & exposing it using Nginx Ingress Controller Use a Service to Access an Application in a Cluster. Exposing applications using services.","title":"Exposing PSMDB and XtraDB clusters for access by external clients"},{"location":"setting-up/server/docker.html","text":"Docker Before you start Run PMM Server as a Docker image Backup and upgrade Downgrade and restore Percona maintain a Docker image for PMM Server at https://hub.docker.com/r/percona/pmm-server . The Docker tags used here are for the latest version of PMM 2 (2.13.0) but you can specify any available tag to use the corresponding version of PMM Server. Before you start Install Docker 1.12.6 or higher. Check disk space: PMM needs approximately 1GB of storage for each monitored database node with data retention set to one week. (By default, data retention is 30 days.) To reduce the size of the VictoriaMetrics database, you can consider disabling table statistics. Check RAM: The minimum amount of memory is 2 GB for one monitored database node. (Memory usage does not grow in proportion to the number of nodes. For example, 16GB is adequate for 20 nodes.) Run PMM Server as a Docker image Pull the image. # Pull the latest 2.x image docker pull percona/pmm-server:2 Create a persistent data container. docker create --volume /srv \\ --name pmm-data percona/pmm-server:2 /bin/true Note PMM Server expects the data volume (specified with --volume ) to be /srv . Using any other value will result in data loss when upgrading. Run the image to start PMM Server. docker run --detach --restart always \\ --publish 80 :80 --publish 443 :443 \\ --volumes-from pmm-data --name pmm-server \\ percona/pmm-server:2 Note You can disable manual updates via the Home Dashboard PMM Upgrade panel by adding -e DISABLE_UPDATES=true to the docker run command. In a web browser, visit server hostname :80 or server hostname :443 to see the PMM user interface. Backup and upgrade Find out which version is installed. docker exec -it pmm-server curl -u admin:admin http://localhost/v1/version Note Use jq to extract the quoted string value. sudo apt install jq # Example for Debian, Ubuntu docker exec -it pmm-server curl -u admin:admin http://localhost/v1/version | jq .version Check container mount points are the same ( /srv ). docker inspect pmm-data | grep Destination docker inspect pmm-server | grep Destination # With jq docker inspect pmm-data | jq '.[].Mounts[].Destination' docker inspect pmm-server | jq '.[].Mounts[].Destination' Stop the container and create backups. docker stop pmm-server docker rename pmm-server pmm-server-backup mkdir pmm-data-backup && cd $_ docker cp pmm-data:/srv . Pull and run the latest image. docker pull percona/pmm-server:2 docker run \\ --detach \\ --restart always \\ --publish 80 :80 --publish 443 :443 \\ --volumes-from pmm-data \\ --name pmm-server \\ percona/pmm-server:2 (Optional) Repeat step 1 to confirm the version, or check the PMM Upgrade panel on the Home Dashboard . Downgrade and restore Stop and remove the running version. docker stop pmm-server docker rm pmm-server Restore backups. docker rename pmm-server-backup pmm-server # cd to wherever you saved the backup docker cp srv pmm-data:/ Restore permissions. docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R root:root /srv && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/alertmanager && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R root:pmm /srv/clickhouse && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R grafana:grafana /srv/grafana && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/logs && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R postgres:postgres /srv/postgres && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/prometheus && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/victoriametrics && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R postgres:postgres /srv/logs/postgresql.log Start (don\u2019t run) the image. docker start pmm-server","title":"Docker"},{"location":"setting-up/server/docker.html#before-you-start","text":"Install Docker 1.12.6 or higher. Check disk space: PMM needs approximately 1GB of storage for each monitored database node with data retention set to one week. (By default, data retention is 30 days.) To reduce the size of the VictoriaMetrics database, you can consider disabling table statistics. Check RAM: The minimum amount of memory is 2 GB for one monitored database node. (Memory usage does not grow in proportion to the number of nodes. For example, 16GB is adequate for 20 nodes.)","title":"Before you start"},{"location":"setting-up/server/docker.html#run-pmm-server-as-a-docker-image","text":"Pull the image. # Pull the latest 2.x image docker pull percona/pmm-server:2 Create a persistent data container. docker create --volume /srv \\ --name pmm-data percona/pmm-server:2 /bin/true Note PMM Server expects the data volume (specified with --volume ) to be /srv . Using any other value will result in data loss when upgrading. Run the image to start PMM Server. docker run --detach --restart always \\ --publish 80 :80 --publish 443 :443 \\ --volumes-from pmm-data --name pmm-server \\ percona/pmm-server:2 Note You can disable manual updates via the Home Dashboard PMM Upgrade panel by adding -e DISABLE_UPDATES=true to the docker run command. In a web browser, visit server hostname :80 or server hostname :443 to see the PMM user interface.","title":"Run PMM Server as a Docker image"},{"location":"setting-up/server/docker.html#backup-and-upgrade","text":"Find out which version is installed. docker exec -it pmm-server curl -u admin:admin http://localhost/v1/version Note Use jq to extract the quoted string value. sudo apt install jq # Example for Debian, Ubuntu docker exec -it pmm-server curl -u admin:admin http://localhost/v1/version | jq .version Check container mount points are the same ( /srv ). docker inspect pmm-data | grep Destination docker inspect pmm-server | grep Destination # With jq docker inspect pmm-data | jq '.[].Mounts[].Destination' docker inspect pmm-server | jq '.[].Mounts[].Destination' Stop the container and create backups. docker stop pmm-server docker rename pmm-server pmm-server-backup mkdir pmm-data-backup && cd $_ docker cp pmm-data:/srv . Pull and run the latest image. docker pull percona/pmm-server:2 docker run \\ --detach \\ --restart always \\ --publish 80 :80 --publish 443 :443 \\ --volumes-from pmm-data \\ --name pmm-server \\ percona/pmm-server:2 (Optional) Repeat step 1 to confirm the version, or check the PMM Upgrade panel on the Home Dashboard .","title":"Backup and upgrade"},{"location":"setting-up/server/docker.html#downgrade-and-restore","text":"Stop and remove the running version. docker stop pmm-server docker rm pmm-server Restore backups. docker rename pmm-server-backup pmm-server # cd to wherever you saved the backup docker cp srv pmm-data:/ Restore permissions. docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R root:root /srv && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/alertmanager && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R root:pmm /srv/clickhouse && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R grafana:grafana /srv/grafana && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/logs && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R postgres:postgres /srv/postgres && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/prometheus && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R pmm:pmm /srv/victoriametrics && \\ docker run --rm --volumes-from pmm-data -it percona/pmm-server:2 chown -R postgres:postgres /srv/logs/postgresql.log Start (don\u2019t run) the image. docker start pmm-server","title":"Downgrade and restore"},{"location":"setting-up/server/virtual-appliance.html","text":"Virtual Appliance Supported Platforms for Running the PMM Server Virtual Appliance VirtualBox Using the Command Line VirtualBox Using the GUI VMware Workstation Player Identifying PMM Server IP Address Accessing PMM Server Accessing the Virtual Machine Next Steps Percona provides a virtual appliance for running PMM Server in a virtual machine. It is distributed as an Open Virtual Appliance (OVA) package, which is a tar archive with necessary files that follow the Open Virtualization Format (OVF). OVF is supported by most popular virtualization platforms: Supported Platforms for Running the PMM Server Virtual Appliance The virtual appliance is ideal for running PMM Server on an enterprise virtualization platform of your choice. This page explains how to run the appliance in VirtualBox and VMware Workstation Player. which is a good choice to experiment with PMM at a smaller scale on a local machine. Similar procedure should work for other platforms (including enterprise deployments on VMware ESXi, for example), but additional steps may be required. The virtual machine used for the appliance runs CentOS 7. Note The appliance must run in a network with DHCP, which will automatically assign an IP address for it. To assign a static IP manually, you need to acquire the root access. VirtualBox Using the Command Line Instead of using the VirtualBox GUI, you can do everything on the command line. Use the VBoxManage command to import, configure, and start the appliance. The following script imports the PMM Server appliance from pmm-server-2.13.0.ova and configures it to bridge the en0 adapter from the host. Then the script routes console output from the appliance to /tmp/pmm-server-console.log . This is done because the script then starts the appliance in headless (without the console) mode. To get the IP address for accessing PMM, the script waits for 1 minute until the appliance boots up and returns the lines with the IP address from the log file. # Import image VBoxManage import pmm-server-2.13.0.ova # Modify NIC settings if needed VBoxManage list bridgedifs VBoxManage modifyvm 'PMM Server 2.13.0' --nic1 bridged --bridgeadapter1 'en0: Wi-Fi (AirPort)' # Log console output into file VBoxManage modifyvm 'PMM Server 2.13.0' --uart1 0x3F8 4 --uartmode1 file /tmp/pmm-server-console.log # Start instance VBoxManage startvm --type headless 'PMM Server 2.13.0' # Wait for 1 minute and get IP address from the log sleep 60 grep \"IP:\" /tmp/pmm-server-console.log By convention OVA files start with pmm-server- followed by the full version number such as 2.13.0. To use this script, make sure to replace this placeholder with the the name of the image that you have downloaded from the PMM download site. VirtualBox Using the GUI The following procedure describes how to run the PMM Server appliance using the graphical user interface of VirtualBox: Download the OVA. The latest version is available at https://www.percona.com/downloads/pmm2/2.13.0/ova . Import the appliance. For this, open the File menu and click Import Appliance and specify the path to the OVA and click Continue . Then, select Reinitialize the MAC address of all network cards and click Import . Configure network settings to make the appliance accessible from other hosts in your network. Alert All database hosts must be in the same network as PMM Server, so do not set the network adapter to NAT. If you are running the appliance on a host with properly configured network settings, select Bridged Adapter in the Network section of the appliance settings. Start the PMM Server appliance. If it was assigned an IP address on the network by DHCP, the URL for accessing PMM will be printed in the console window. VMware Workstation Player The following procedure describes how to run the PMM Server appliance using VMware Workstation Player: Download the OVA. The latest version is available at https://www.percona.com/downloads/pmm2/2.13.0/ova . Import the appliance. Open the File menu and click Open . Specify the path to the OVA and click Continue . Alert You may get an error indicating that import failed. Click Retry and the import should succeed. Configure network settings to make the appliance accessible from other hosts in your network. If you are running the appliance on a host with properly configured network settings, select Bridged in the Network connection section of the appliance settings. Start the PMM Server appliance. Log in as root , password percona and follow the prompts to change the password. Identifying PMM Server IP Address PMM Server uses DHCP for security reasons. Use this command in the PMM Server console to find out the server\u2019s IP address: grep \"IP:\" /tmp/pmm-server-console.log Accessing PMM Server Start the virtual machine Open a web browser Enter the server\u2019s IP address Enter the user login and password to access the PMM Server web interface If you run PMM Server in your browser for the first time, you are requested to supply the user login and password. The default PMM Server credentials are: username: admin password: admin After login you will be proposed to change this default password. Enter the new password twice and click Save . The PMM Server is now ready and the home page opens. You are creating a username and password that will be used for two purposes: authentication as a user to PMM - this will be the credentials you need in order to log in to PMM. authentication between PMM Server and PMM Clients - you will re-use these credentials as a part of the server URL when configuring PMM Client for the first time on a server: Run this command as root or by using the sudo command pmm-admin config --server-insecure-tls --server-url = https://admin:admin@<IP Address>:443 Accessing the Virtual Machine To access the VM with the PMM Server appliance via SSH, you will need to provide your public key: Open the URL for accessing PMM in a web browser. The URL is provided either in the console window or in the appliance log. Go to PMM > PMM Settings > SSH Key . Enter your public key in the SSH Key field and click the Apply SSH Key button. After that you can use ssh to log in as the admin user. For example, if PMM Server is running at 192.168.100.1 and your private key is ~/.ssh/pmm-admin.key , use the following command: ssh admin@192.168.100.1 -i ~/.ssh/pmm-admin.key Next Steps Verify that PMM Server is running by connecting to the PMM web interface using the IP address assigned to the virtual appliance, then install PMM Client on all database hosts that you want to monitor.","title":"Virtual Appliance"},{"location":"setting-up/server/virtual-appliance.html#supported-platforms-for-running-the-pmm-server-virtual-appliance","text":"The virtual appliance is ideal for running PMM Server on an enterprise virtualization platform of your choice. This page explains how to run the appliance in VirtualBox and VMware Workstation Player. which is a good choice to experiment with PMM at a smaller scale on a local machine. Similar procedure should work for other platforms (including enterprise deployments on VMware ESXi, for example), but additional steps may be required. The virtual machine used for the appliance runs CentOS 7. Note The appliance must run in a network with DHCP, which will automatically assign an IP address for it. To assign a static IP manually, you need to acquire the root access.","title":"Supported Platforms for Running the PMM Server Virtual Appliance"},{"location":"setting-up/server/virtual-appliance.html#virtualbox-using-the-command-line","text":"Instead of using the VirtualBox GUI, you can do everything on the command line. Use the VBoxManage command to import, configure, and start the appliance. The following script imports the PMM Server appliance from pmm-server-2.13.0.ova and configures it to bridge the en0 adapter from the host. Then the script routes console output from the appliance to /tmp/pmm-server-console.log . This is done because the script then starts the appliance in headless (without the console) mode. To get the IP address for accessing PMM, the script waits for 1 minute until the appliance boots up and returns the lines with the IP address from the log file. # Import image VBoxManage import pmm-server-2.13.0.ova # Modify NIC settings if needed VBoxManage list bridgedifs VBoxManage modifyvm 'PMM Server 2.13.0' --nic1 bridged --bridgeadapter1 'en0: Wi-Fi (AirPort)' # Log console output into file VBoxManage modifyvm 'PMM Server 2.13.0' --uart1 0x3F8 4 --uartmode1 file /tmp/pmm-server-console.log # Start instance VBoxManage startvm --type headless 'PMM Server 2.13.0' # Wait for 1 minute and get IP address from the log sleep 60 grep \"IP:\" /tmp/pmm-server-console.log By convention OVA files start with pmm-server- followed by the full version number such as 2.13.0. To use this script, make sure to replace this placeholder with the the name of the image that you have downloaded from the PMM download site.","title":"VirtualBox Using the Command Line"},{"location":"setting-up/server/virtual-appliance.html#virtualbox-using-the-gui","text":"The following procedure describes how to run the PMM Server appliance using the graphical user interface of VirtualBox: Download the OVA. The latest version is available at https://www.percona.com/downloads/pmm2/2.13.0/ova . Import the appliance. For this, open the File menu and click Import Appliance and specify the path to the OVA and click Continue . Then, select Reinitialize the MAC address of all network cards and click Import . Configure network settings to make the appliance accessible from other hosts in your network. Alert All database hosts must be in the same network as PMM Server, so do not set the network adapter to NAT. If you are running the appliance on a host with properly configured network settings, select Bridged Adapter in the Network section of the appliance settings. Start the PMM Server appliance. If it was assigned an IP address on the network by DHCP, the URL for accessing PMM will be printed in the console window.","title":"VirtualBox Using the GUI"},{"location":"setting-up/server/virtual-appliance.html#vmware-workstation-player","text":"The following procedure describes how to run the PMM Server appliance using VMware Workstation Player: Download the OVA. The latest version is available at https://www.percona.com/downloads/pmm2/2.13.0/ova . Import the appliance. Open the File menu and click Open . Specify the path to the OVA and click Continue . Alert You may get an error indicating that import failed. Click Retry and the import should succeed. Configure network settings to make the appliance accessible from other hosts in your network. If you are running the appliance on a host with properly configured network settings, select Bridged in the Network connection section of the appliance settings. Start the PMM Server appliance. Log in as root , password percona and follow the prompts to change the password.","title":"VMware Workstation Player"},{"location":"setting-up/server/virtual-appliance.html#identifying-pmm-server-ip-address","text":"PMM Server uses DHCP for security reasons. Use this command in the PMM Server console to find out the server\u2019s IP address: grep \"IP:\" /tmp/pmm-server-console.log","title":"Identifying PMM Server IP Address"},{"location":"setting-up/server/virtual-appliance.html#accessing-pmm-server","text":"Start the virtual machine Open a web browser Enter the server\u2019s IP address Enter the user login and password to access the PMM Server web interface If you run PMM Server in your browser for the first time, you are requested to supply the user login and password. The default PMM Server credentials are: username: admin password: admin After login you will be proposed to change this default password. Enter the new password twice and click Save . The PMM Server is now ready and the home page opens. You are creating a username and password that will be used for two purposes: authentication as a user to PMM - this will be the credentials you need in order to log in to PMM. authentication between PMM Server and PMM Clients - you will re-use these credentials as a part of the server URL when configuring PMM Client for the first time on a server: Run this command as root or by using the sudo command pmm-admin config --server-insecure-tls --server-url = https://admin:admin@<IP Address>:443","title":"Accessing PMM Server"},{"location":"setting-up/server/virtual-appliance.html#accessing-the-virtual-machine","text":"To access the VM with the PMM Server appliance via SSH, you will need to provide your public key: Open the URL for accessing PMM in a web browser. The URL is provided either in the console window or in the appliance log. Go to PMM > PMM Settings > SSH Key . Enter your public key in the SSH Key field and click the Apply SSH Key button. After that you can use ssh to log in as the admin user. For example, if PMM Server is running at 192.168.100.1 and your private key is ~/.ssh/pmm-admin.key , use the following command: ssh admin@192.168.100.1 -i ~/.ssh/pmm-admin.key","title":"Accessing the Virtual Machine"},{"location":"setting-up/server/virtual-appliance.html#next-steps","text":"Verify that PMM Server is running by connecting to the PMM web interface using the IP address assigned to the virtual appliance, then install PMM Client on all database hosts that you want to monitor.","title":"Next Steps"},{"location":"using/index.html","text":"Using: Overview User Interface Using the web-based user interface Finding dashboards Rendering dashboard images Viewing graph details Annotating events Integrated alerting Query Analytics Specialized dashboard for detailed query analysis Percona Enterprise Platform Security Threat Tool : Enabling and seeing the results of database security checks","title":"Using: Overview"},{"location":"using/alerting.html","text":"Integrated Alerting Integrated Alerting lets you know when certain system events occur. Warning Integrated alerting is a technical preview and is subject to change. To activate Integrated Alerting , select PMM\u2192PMM Settings\u2192Advanced Settings , turn on Integrated Alerting and click Apply changes . Definitions Prerequisites Open the Integrated Alerting page Add a Notification Channel Add an Alert Rule Add an Alert Rule Template Definitions Alerts are generated when their criteria ( alert rules ) are met; an alert is the result of an alert rule expression evaluating to true . Alert rules are based on alert rule templates . We provide a default set of templates. You can also create your own. Note PMM\u2019s Integrated Alerting is a customized and separate instance of the Prometheus Alertmanager, and distinct from Grafana\u2019s alerting functionality. Prerequisites Set up a communication channel: When the Communication tab appears, select it. Enter details for Email or Slack . ( Read more ) Open the Integrated Alerting page From the left menu, select Alerting , Integrated Alerting Note The Alerting menu also lists Alert Rules and Notification Channels . These are for Grafana\u2019s alerting functionality. This page has four tabs. Alerts : Shows alerts (if any). Alert Rules : Shows rule definitions. Alert Rule Templates : Lists rule templates. Notification Channels : Lists notification channels. Add a Notification Channel On the Integrated Alerting page, go to the Notification Channels tab. Click Add . Fill in the details: Name Type Email: Addresses Pager Duty Routing key Service key Slack Channel Click Add to add the notification channel, or Cancel to abort the operation. Add an Alert Rule On the Integrated Alerting page, go to the Alert Rules tab. Click Add . Fill in the details Template Name Threshold Duration(s) Severity Filters Channels Activate Click Add to add the alert rule, or Cancel to abort the operation. Add an Alert Rule Template On the Integrated Alerting page, go to the Alert Rule Templates tab. Click Add . Enter a template in the Alert Rule Template text box. --- templates: - name: mysql_too_many_connections version: 1 summary: MySQL connections in use tiers: [anonymous, registered] expr: |- max_over_time(mysql_global_status_threads_connected[5m]) / ignoring (job) mysql_global_variables_max_connections * 100 > [[ .threshold ]] params: - name: threshold summary: A percentage from configured maximum unit: '%' type: float range: [0, 100] value: 80 for: 5m severity: warning labels: foo: bar annotations: description: |- More than [[ .threshold ]]% of MySQL connections are in use on {{ $labels.instance }} VALUE = {{ $value }} LABELS: {{ $labels }} summary: MySQL too many connections (instance {{ $labels.instance }}) Click Add to add the alert rule template, or Cancel to abort the operation.","title":"Integrated Alerting"},{"location":"using/alerting.html#definitions","text":"Alerts are generated when their criteria ( alert rules ) are met; an alert is the result of an alert rule expression evaluating to true . Alert rules are based on alert rule templates . We provide a default set of templates. You can also create your own. Note PMM\u2019s Integrated Alerting is a customized and separate instance of the Prometheus Alertmanager, and distinct from Grafana\u2019s alerting functionality.","title":"Definitions"},{"location":"using/alerting.html#prerequisites","text":"Set up a communication channel: When the Communication tab appears, select it. Enter details for Email or Slack . ( Read more )","title":"Prerequisites"},{"location":"using/alerting.html#open-the-integrated-alerting-page","text":"From the left menu, select Alerting , Integrated Alerting Note The Alerting menu also lists Alert Rules and Notification Channels . These are for Grafana\u2019s alerting functionality. This page has four tabs. Alerts : Shows alerts (if any). Alert Rules : Shows rule definitions. Alert Rule Templates : Lists rule templates. Notification Channels : Lists notification channels.","title":"Open the Integrated Alerting page"},{"location":"using/alerting.html#add-a-notification-channel","text":"On the Integrated Alerting page, go to the Notification Channels tab. Click Add . Fill in the details: Name Type Email: Addresses Pager Duty Routing key Service key Slack Channel Click Add to add the notification channel, or Cancel to abort the operation.","title":"Add a Notification Channel"},{"location":"using/alerting.html#add-an-alert-rule","text":"On the Integrated Alerting page, go to the Alert Rules tab. Click Add . Fill in the details Template Name Threshold Duration(s) Severity Filters Channels Activate Click Add to add the alert rule, or Cancel to abort the operation.","title":"Add an Alert Rule"},{"location":"using/alerting.html#add-an-alert-rule-template","text":"On the Integrated Alerting page, go to the Alert Rule Templates tab. Click Add . Enter a template in the Alert Rule Template text box. --- templates: - name: mysql_too_many_connections version: 1 summary: MySQL connections in use tiers: [anonymous, registered] expr: |- max_over_time(mysql_global_status_threads_connected[5m]) / ignoring (job) mysql_global_variables_max_connections * 100 > [[ .threshold ]] params: - name: threshold summary: A percentage from configured maximum unit: '%' type: float range: [0, 100] value: 80 for: 5m severity: warning labels: foo: bar annotations: description: |- More than [[ .threshold ]]% of MySQL connections are in use on {{ $labels.instance }} VALUE = {{ $value }} LABELS: {{ $labels }} summary: MySQL too many connections (instance {{ $labels.instance }}) Click Add to add the alert rule template, or Cancel to abort the operation.","title":"Add an Alert Rule Template"},{"location":"using/interface.html","text":"User Interface PMM\u2019s user interface is a browser application based on Grafana. Dashboards Logging in User interface controls: Common Top row menu bar Second row menu bar Vertical menu bar (left) Navigation By name By menu Panels Panel menu Rendering dashboard images Annotations Dashboards The interface is a collection of web pages called dashboards . Dashboards are grouped into folders . You can customize these, renaming them or creating new ones. The area inside dashboards is populated by panels , many of which are in collapsible panel groups. A panel can show a value, a graph, a chart, or a visual representation of a set. Logging in Start a web browser and enter the server name or IP address of the PMM server host. Enter the username and password given to you by your system administrator. The defaults are: Username: admin Password: admin Click Log in If this is your first time logging in, you\u2019ll be asked to set a new password. (We recommend you do.) Enter a new password in both fields and click Submit . If you wish, you can click Skip and continue using the default password. The PMM Home dashboard loads. User interface controls: Common Top row menu bar Items (left) Description (Display only) (Name) / (Optional) Folder name (Name) Dashboard name Mark as favorite Share dashboard Items (right) Description Dashboard settings Cycle view mode (time range) Time range selector Time range zoom out Refresh dashboard (Time interval) Refresh period Second row menu bar This menu bar is context sensitive; it changes according to the page you are on. The items are grouped left and right. (With wide menus on small screens, items may wrap to the next row.) Left: Filters and controls for the viewed data Right: Links to other dashboards Left group items Items Description Interval Data interval Region Filter by region Environment Filter by environment Cluster Filter by cluster Replication Set Filter by replication set Node Name Filter by node name Service Name Filter by service name PMM Annotations View annotations Right group items Items Description Home Home dashboard Query Analytics Query Analytics Compare Nodes compare (Service Type) Service type menu (see below) HA HA dashboards Services Services menu PMM PMM menu Note The Compare menu links to the Instances Overview dashboard for the current service type. Services menu The Services menu choice determines the Service Type menu. Menu Items Service type menu Description Services MongoDB Instances Overview MongoDB MongoDB dashboards MySQL Instances Overview MySQL MySQL dashboards Nodes Overview OS OS dashboards PostgreSQL Instances Overview PostgreSQL PostgreSQL dashboards PMM menu This item lists shortcuts to utility pages. Menu Items PMM PMM Add Instance PMM Database Checks PMM Inventory PMM Settings Vertical menu bar (left) The vertical menu bar (left) is part of the Grafana framework and is visible on every page. Items (Top) Name Home Search Create Dashboards Explore Alerting Configuration Server Admin DBaaS Note The DBaaS icon appears only if a server feature flag has been set. Icons (Bottom) Description (Profile icon) User menu Help Navigation There are several ways to open a dashboard. By name Click the dashboard name (to the right of the icon) A search field appears labeled Search dashboards by name Click it and begin typing any part of the dashboard name (in this example, \u201c Instances \u201d) Click one of the search results to go to that dashboard To abandon the search, click the icon at the end of the search bar Tip To search within the current folder, click the folder\u2019s name. By menu Use the second row main menu. (See Second row menu bar .) Panels Charts, graphs and set-based panels reveal additional information when the mouse is moved over them. Some panels have an information icon in the top left corner. Mouse over this to reveal panel information. Panel menu At the top of each panel and to the right of the panel name is the panel menu . Tip The presence of the menu is hidden until you mouse over it. Look for the symbol in the title bar of a panel. Item Description View Open the panel in full window mode Share Render the panel\u2019s image for sharing Explore Run PromQL queries Inspect See the panel\u2019s data or definition More (Only charts and graphs) Additional options View The View menu items opens panels in full-window mode. This is useful for graphs with several metrics. Exit a panel\u2019s full window mode by pressing Escape or clicking the left arrow next to the dashboard name. Rendering dashboard images PMM Server can\u2019t currently directly render dashboard images exported by Grafana without these additional set-up steps. Part 1: Install dependencies Connect to your PMM Server Docker container. docker exec -it pmm-server bash Install Grafana plugins. grafana-cli plugins install grafana-image-renderer Restart Grafana. supervisorctl restart grafana Install additional libraries. yum install -y libXcomposite libXdamage libXtst cups libXScrnSaver pango \\ atk adwaita-cursor-theme adwaita-icon-theme at at-spi2-atk at-spi2-core \\ cairo-gobject colord-libs dconf desktop-file-utils ed emacs-filesystem \\ gdk-pixbuf2 glib-networking gnutls gsettings-desktop-schemas \\ gtk-update-icon-cache gtk3 hicolor-icon-theme jasper-libs json-glib \\ libappindicator-gtk3 libdbusmenu libdbusmenu-gtk3 libepoxy \\ liberation-fonts liberation-narrow-fonts liberation-sans-fonts \\ liberation-serif-fonts libgusb libindicator-gtk3 libmodman libproxy \\ libsoup libwayland-cursor libwayland-egl libxkbcommon m4 mailx nettle \\ patch psmisc redhat-lsb-core redhat-lsb-submod-security rest spax time \\ trousers xdg-utils xkeyboard-config alsa-lib Part 2 - Share the image Navigate to the dashboard you want to share. Open the panel menu. Select Share to reveal the Share Panel . Click Direct link rendered image . A new browser tab opens. Wait for the image to be rendered then use your browser\u2019s image save function to download the image. If the necessary plugins are not installed, a message in the Share Panel will say so. Annotations Annotations mark a moment in time. They are useful for marking system changes or other significant application events. They can be set globally or for specific nodes or services. You create them on the command line with the pmm-admin annotate command. Annotations show as a vertical dashed line on a dashboard graph. Reveal the annotation text by mousing over the caret indicator below the line. You turn annotations on or off with the PMM Annotations switch in the second row menu bar.","title":"User Interface"},{"location":"using/interface.html#dashboards","text":"The interface is a collection of web pages called dashboards . Dashboards are grouped into folders . You can customize these, renaming them or creating new ones. The area inside dashboards is populated by panels , many of which are in collapsible panel groups. A panel can show a value, a graph, a chart, or a visual representation of a set.","title":"Dashboards"},{"location":"using/interface.html#logging-in","text":"Start a web browser and enter the server name or IP address of the PMM server host. Enter the username and password given to you by your system administrator. The defaults are: Username: admin Password: admin Click Log in If this is your first time logging in, you\u2019ll be asked to set a new password. (We recommend you do.) Enter a new password in both fields and click Submit . If you wish, you can click Skip and continue using the default password. The PMM Home dashboard loads.","title":"Logging in"},{"location":"using/interface.html#user-interface-controls-common","text":"","title":"User interface controls: Common"},{"location":"using/interface.html#top-row-menu-bar","text":"Items (left) Description (Display only) (Name) / (Optional) Folder name (Name) Dashboard name Mark as favorite Share dashboard Items (right) Description Dashboard settings Cycle view mode (time range) Time range selector Time range zoom out Refresh dashboard (Time interval) Refresh period","title":"Top row menu bar"},{"location":"using/interface.html#second-row-menu-bar","text":"This menu bar is context sensitive; it changes according to the page you are on. The items are grouped left and right. (With wide menus on small screens, items may wrap to the next row.) Left: Filters and controls for the viewed data Right: Links to other dashboards","title":"Second row menu bar"},{"location":"using/interface.html#vertical-menu-bar-left","text":"The vertical menu bar (left) is part of the Grafana framework and is visible on every page. Items (Top) Name Home Search Create Dashboards Explore Alerting Configuration Server Admin DBaaS Note The DBaaS icon appears only if a server feature flag has been set. Icons (Bottom) Description (Profile icon) User menu Help","title":"Vertical menu bar (left)"},{"location":"using/interface.html#navigation","text":"There are several ways to open a dashboard.","title":"Navigation"},{"location":"using/interface.html#by-name","text":"Click the dashboard name (to the right of the icon) A search field appears labeled Search dashboards by name Click it and begin typing any part of the dashboard name (in this example, \u201c Instances \u201d) Click one of the search results to go to that dashboard To abandon the search, click the icon at the end of the search bar Tip To search within the current folder, click the folder\u2019s name.","title":"By name"},{"location":"using/interface.html#by-menu","text":"Use the second row main menu. (See Second row menu bar .)","title":"By menu"},{"location":"using/interface.html#panels","text":"Charts, graphs and set-based panels reveal additional information when the mouse is moved over them. Some panels have an information icon in the top left corner. Mouse over this to reveal panel information.","title":"Panels"},{"location":"using/interface.html#panel-menu","text":"At the top of each panel and to the right of the panel name is the panel menu . Tip The presence of the menu is hidden until you mouse over it. Look for the symbol in the title bar of a panel. Item Description View Open the panel in full window mode Share Render the panel\u2019s image for sharing Explore Run PromQL queries Inspect See the panel\u2019s data or definition More (Only charts and graphs) Additional options","title":"Panel menu"},{"location":"using/interface.html#rendering-dashboard-images","text":"PMM Server can\u2019t currently directly render dashboard images exported by Grafana without these additional set-up steps. Part 1: Install dependencies Connect to your PMM Server Docker container. docker exec -it pmm-server bash Install Grafana plugins. grafana-cli plugins install grafana-image-renderer Restart Grafana. supervisorctl restart grafana Install additional libraries. yum install -y libXcomposite libXdamage libXtst cups libXScrnSaver pango \\ atk adwaita-cursor-theme adwaita-icon-theme at at-spi2-atk at-spi2-core \\ cairo-gobject colord-libs dconf desktop-file-utils ed emacs-filesystem \\ gdk-pixbuf2 glib-networking gnutls gsettings-desktop-schemas \\ gtk-update-icon-cache gtk3 hicolor-icon-theme jasper-libs json-glib \\ libappindicator-gtk3 libdbusmenu libdbusmenu-gtk3 libepoxy \\ liberation-fonts liberation-narrow-fonts liberation-sans-fonts \\ liberation-serif-fonts libgusb libindicator-gtk3 libmodman libproxy \\ libsoup libwayland-cursor libwayland-egl libxkbcommon m4 mailx nettle \\ patch psmisc redhat-lsb-core redhat-lsb-submod-security rest spax time \\ trousers xdg-utils xkeyboard-config alsa-lib Part 2 - Share the image Navigate to the dashboard you want to share. Open the panel menu. Select Share to reveal the Share Panel . Click Direct link rendered image . A new browser tab opens. Wait for the image to be rendered then use your browser\u2019s image save function to download the image. If the necessary plugins are not installed, a message in the Share Panel will say so.","title":"Rendering dashboard images"},{"location":"using/interface.html#annotations","text":"Annotations mark a moment in time. They are useful for marking system changes or other significant application events. They can be set globally or for specific nodes or services. You create them on the command line with the pmm-admin annotate command. Annotations show as a vertical dashed line on a dashboard graph. Reveal the annotation text by mousing over the caret indicator below the line. You turn annotations on or off with the PMM Annotations switch in the second row menu bar.","title":"Annotations"},{"location":"using/query-analytics.html","text":"Query Analytics The Query Analytics dashboard shows how queries are executed and where they spend their time. It helps you analyze database queries over time, optimize database performance, and find and remedy the source of problems. Note Query Analytics supports MySQL, MongoDB and PostgreSQL. The minimum requirements for MySQL are: MySQL 5.1 or later (if using the slow query log) MySQL 5.6.9 or later (if using Performance Schema) Query Analytics data retrieval is not instantaneous and can be delayed due to network conditions. In such situations no data is reported and a gap appears in the sparkline. Query Analytics displays metrics in both visual and numeric form. Performance-related characteristics appear as plotted graphics with summaries. The dashboard contains three panels: Filters Panel Overview Panel Details Panel Filters Panel The Filter panel occupies the left side of the dashboard. It lists filters, grouped by category. Selecting one reduces the Overview list to those items matching the filter. A maximum of the first five of each category are shown. If there are more, the list is expanded by clicking Show all beside the category name, and collapsed again with Show top 5 . Applying a filter may make other filters inapplicable. These become grayed out and inactive. Click the chart symbol ( ) to navigate directly to an item\u2019s associated dashboard. Separately, the global Time range setting filters results by time, either your choice of Absolute time range , or one of the pre-defined Relative time ranges . Overview Panel To the right of the Filters panel and occupying the upper portion of the dashboard is the Overview panel. Each row of the table represents the metrics for a chosen object type, one of: Query Service Name Database Schema User Name Client Host At the top of the second column is the dimension menu. Use this to choose the object type. On the right side of the dimension column is the Dimension Search bar. Enter a string and press Enter to limit the view to queries containing only the specified keywords. Delete the search text and press Enter to see the full list again. Columns The first column is the object\u2019s identifier. For Query , it is the query\u2019s Fingerprint . The second column is the Main metric , containing a reduced graphical representation of the metric over time, called a sparkline , and a horizontal meter, filled to reflect a percentage of the total value. Additional values are revealed as mouse-over tool-tips. Tool-tips For the Query dimension, hovering over the information icon ( ) reveals the query ID and its example. Hovering on a column header reveals an informative tool-tip for that column. Hovering on the main metric sparkline highlights the data point and a tooltip shows the data value under the cursor. Hovering on the main metric meter reveals the percentage of the total, and other details specific to the main metric. Hovering on column values reveals more details on the value. The contents depends on the type of value. Adding and removing columns Metrics columns are added with the Add column button. When clicked, a text field and list of available metrics are revealed. Select a metric or enter a search string to reduce the list. Selecting a metric adds it to the panel. A metric column is removed by clicking on the column heading and selecting Remove column . The value plotted in the main metric column can be changed by clicking a metric column heading and selecting Swap with main metric . Sorting The entire list is sorted by one of the columns. Click either the up or down caret to sort the list by that column\u2019s ascending or descending values. Pagination The pagination device lets you move forwards or backwards through pages, jump to a specific page, and choose how many items are listed per page. Queries are grouped into pages of 25, 50 or 100 items. Details Panel Selecting an item in the Overview panel opens the Details panel with a Details Tab . If the dimension is Query , the panel also contains the Examples Tab , Explain Tab , and Tables Tab . Details Tab The Details tab contains a Query time distribution bar (only for MySQL databases) and a set of Metrics in collapsible subpanels. The Query time distribution bar shows a query\u2019s total time made up of colored segments, each segment representing the proportion of time spent on one of the follow named activities: query_time : Statement execution time. lock_time : Time to acquire locks. blk_read_time : Total time the statement spent reading blocks (if track_io_timing is enabled, otherwise zero). blk_write_time : Total time the statement spent writing blocks (if track_io_timing is enabled, otherwise zero). innodb_io_r_wait : Time for InnoDB to read the data from storage. innodb_queue_wait : Time the query spent either waiting to enter the InnoDB queue, or in it pending execution. innodb_rec_lock_wait : Time the query waited for row locks. other : Remaining uncategorized query time. Metrics is a table with these headings: Metric : The Metric name, with a question-mark tool-tip that reveals a description of the metric on mouse-over. Rate/Second : A sparkline chart of real-time values per unit time. Sum : A summation of the metric for the selected query, and the percentage of the total. Per Query Stats : The value of the metric per query. Each row in the table is a metric. The contents depends on the chosen dimension. Examples Tab (For Query dimension.) The Examples tab shows an example of the selected query\u2019s fingerprint or table element. Explain Tab (For Query dimension.) The Explain tab shows the explain output for the selected query, in Classic or JSON formats: MySQL: Classic and JSON MongoDB: JSON only PostgreSQL: Not supported Tables Tab (For Query dimension.) The Tables tab shows information on the tables and indexes involved in the selected query. Query Analytics for MongoDB MongoDB is conceptually different from relational database management systems, such as MySQL and MariaDB. Relational database management systems store data in tables that represent single entities. Complex objects are represented by linking several tables. In contrast, MongoDB uses the concept of a document where all essential information pertaining to a complex object is stored in one place. Query Analytics can monitor MongoDB queries. Although MongoDB is not a relational database management system, you analyze its databases and collections in the same interface using the same tools.","title":"Query Analytics"},{"location":"using/query-analytics.html#filters-panel","text":"The Filter panel occupies the left side of the dashboard. It lists filters, grouped by category. Selecting one reduces the Overview list to those items matching the filter. A maximum of the first five of each category are shown. If there are more, the list is expanded by clicking Show all beside the category name, and collapsed again with Show top 5 . Applying a filter may make other filters inapplicable. These become grayed out and inactive. Click the chart symbol ( ) to navigate directly to an item\u2019s associated dashboard. Separately, the global Time range setting filters results by time, either your choice of Absolute time range , or one of the pre-defined Relative time ranges .","title":"Filters Panel"},{"location":"using/query-analytics.html#overview-panel","text":"To the right of the Filters panel and occupying the upper portion of the dashboard is the Overview panel. Each row of the table represents the metrics for a chosen object type, one of: Query Service Name Database Schema User Name Client Host At the top of the second column is the dimension menu. Use this to choose the object type. On the right side of the dimension column is the Dimension Search bar. Enter a string and press Enter to limit the view to queries containing only the specified keywords. Delete the search text and press Enter to see the full list again. Columns The first column is the object\u2019s identifier. For Query , it is the query\u2019s Fingerprint . The second column is the Main metric , containing a reduced graphical representation of the metric over time, called a sparkline , and a horizontal meter, filled to reflect a percentage of the total value. Additional values are revealed as mouse-over tool-tips. Tool-tips For the Query dimension, hovering over the information icon ( ) reveals the query ID and its example. Hovering on a column header reveals an informative tool-tip for that column. Hovering on the main metric sparkline highlights the data point and a tooltip shows the data value under the cursor. Hovering on the main metric meter reveals the percentage of the total, and other details specific to the main metric. Hovering on column values reveals more details on the value. The contents depends on the type of value. Adding and removing columns Metrics columns are added with the Add column button. When clicked, a text field and list of available metrics are revealed. Select a metric or enter a search string to reduce the list. Selecting a metric adds it to the panel. A metric column is removed by clicking on the column heading and selecting Remove column . The value plotted in the main metric column can be changed by clicking a metric column heading and selecting Swap with main metric . Sorting The entire list is sorted by one of the columns. Click either the up or down caret to sort the list by that column\u2019s ascending or descending values. Pagination The pagination device lets you move forwards or backwards through pages, jump to a specific page, and choose how many items are listed per page. Queries are grouped into pages of 25, 50 or 100 items.","title":"Overview Panel"},{"location":"using/query-analytics.html#details-panel","text":"Selecting an item in the Overview panel opens the Details panel with a Details Tab . If the dimension is Query , the panel also contains the Examples Tab , Explain Tab , and Tables Tab .","title":"Details Panel"},{"location":"using/query-analytics.html#details-tab","text":"The Details tab contains a Query time distribution bar (only for MySQL databases) and a set of Metrics in collapsible subpanels. The Query time distribution bar shows a query\u2019s total time made up of colored segments, each segment representing the proportion of time spent on one of the follow named activities: query_time : Statement execution time. lock_time : Time to acquire locks. blk_read_time : Total time the statement spent reading blocks (if track_io_timing is enabled, otherwise zero). blk_write_time : Total time the statement spent writing blocks (if track_io_timing is enabled, otherwise zero). innodb_io_r_wait : Time for InnoDB to read the data from storage. innodb_queue_wait : Time the query spent either waiting to enter the InnoDB queue, or in it pending execution. innodb_rec_lock_wait : Time the query waited for row locks. other : Remaining uncategorized query time. Metrics is a table with these headings: Metric : The Metric name, with a question-mark tool-tip that reveals a description of the metric on mouse-over. Rate/Second : A sparkline chart of real-time values per unit time. Sum : A summation of the metric for the selected query, and the percentage of the total. Per Query Stats : The value of the metric per query. Each row in the table is a metric. The contents depends on the chosen dimension.","title":"Details Tab"},{"location":"using/query-analytics.html#examples-tab","text":"(For Query dimension.) The Examples tab shows an example of the selected query\u2019s fingerprint or table element.","title":"Examples Tab"},{"location":"using/query-analytics.html#explain-tab","text":"(For Query dimension.) The Explain tab shows the explain output for the selected query, in Classic or JSON formats: MySQL: Classic and JSON MongoDB: JSON only PostgreSQL: Not supported","title":"Explain Tab"},{"location":"using/query-analytics.html#tables-tab","text":"(For Query dimension.) The Tables tab shows information on the tables and indexes involved in the selected query.","title":"Tables Tab"},{"location":"using/query-analytics.html#query-analytics-for-mongodb","text":"MongoDB is conceptually different from relational database management systems, such as MySQL and MariaDB. Relational database management systems store data in tables that represent single entities. Complex objects are represented by linking several tables. In contrast, MongoDB uses the concept of a document where all essential information pertaining to a complex object is stored in one place. Query Analytics can monitor MongoDB queries. Although MongoDB is not a relational database management system, you analyze its databases and collections in the same interface using the same tools.","title":"Query Analytics for MongoDB"},{"location":"using/platform/index.html","text":"About Percona Enterprise Platform Percona Enterprise Platform provides value-added services to PMM. The services comprise: Security Threat Tool","title":"About Percona Enterprise Platform"},{"location":"using/platform/dbaas.html","text":"DBaaS Dashboard Caution DBaaS functionality is Alpha. The information on this page is subject to change and may be inaccurate. Note You must run PMM Server with a DBaaS feature flag to activate the features described here. Kubernetes clusters Add a Kubernetes cluster Unregister a Kubernetes cluster View a Kubernetes cluster\u2019s configuration DB clusters Add a DB Cluster Delete a DB Cluster Edit a DB Cluster Restart a DB Cluster Suspend or resume a DB Cluster The DBaaS dashboard is where you add, remove, and operate on Kubernetes and database clusters. To open the DBaaS dashboard: From the main menu, select PMM \u2192 PMM DBaaS ; Or, from the left menu, select DBaaS . Kubernetes clusters Add a Kubernetes cluster Click Register new Kubernetes Cluster Enter values for the Kubernetes Cluster Name and Kubeconfig file in the corresponding fields. Click Register . A message will momentarily display telling you whether the registration was successful or not. Unregister a Kubernetes cluster Note You can\u2019t unregister a Kubernetes cluster if there DB clusters associated with it. Click Unregister . Confirm the action by clicking Proceed , or abandon by clicking Cancel . View a Kubernetes cluster\u2019s configuration Find the row with the Kubernetes cluster you want to see. In the Actions column, open the menu and click Show configuration . DB clusters Add a DB Cluster You must create at least one Kubernetes cluster to create a DB cluster. To monitor a DB cluster, set up a public address for PMM Server first. Tip Resource consumption in Kubernetes can cause problems. Use this formula to ensure your nodes have enough resources to start the requested configuration: ( 2 * # of nodes in DB cluster * CPU per node ) + (.5 * # of nodes in db cluster) = total # of CPUs that must be free for cluster to start The first part of the equation is resources for the cluster. It is doubled because each DB cluster member must also have a proxy started with it. The second part is to start the container(s) that automatically monitor each member of the DB cluster. (You can also specify CPU in decimal tenths, e.g. .1 CPUs or 1.5 CPUs.) Select the DB Cluster tab. Click Create DB Cluster . In section 1, Basic Options : Enter a value for Cluster name that complies with domain naming rules. Select a cluster from the Kubernetes Cluster menu. Select a database type from the Database Type menu. Expand section 2, Advanced Options . Select Topology , either Cluster or Single Node . Select the number of nodes. (The lower limit is 3.) Select a preset for Resources per Node . Small , Medium and Large are fixed preset values for Memory , CPU , and Disk . Values for the Custom preset can be edited. When both Basic Options and Advanced Options section icons are green, the Create Cluster button becomes active. (If it is inactive, check the values for fields in sections whose icon is red.) Click Create Cluster to create your cluster. A row appears with information on your cluster: Name : The cluster name Database type : The cluster database type Connection : Host : The hostname Port : The port number Username : The connection username Password : The connection password (click the eye icon to reveal) DB Cluster Parameters : K8s cluster name : The Kubernetes cluster name CPU : The number of CPUs allocated to the cluster Memory : The amount of memory allocated to the cluster Disk : The amount of disk space allocated to the cluster Cluster Status : PENDING : The cluster is being created ACTIVE : The cluster is active FAILED : The cluster could not be created DELETING : The cluster is being deleted Delete a DB Cluster Find the row with the database cluster you want to delete. In the Actions column, open the menu and click Delete . Confirm the action by clicking Proceed , or abandon by clicking Cancel . Edit a DB Cluster Select the DB Cluster tab. Find the row with the database cluster you want to change. In the Actions column, open the menu and click Edit . A paused cluster can\u2019t be edited. Restart a DB Cluster Select the DB Cluster tab. Identify the database cluster to be changed. In the Actions column, open the menu and click Restart . Suspend or resume a DB Cluster Select the DB Cluster tab. Identify the DB cluster to suspend or resume. In the Actions column, open the menu and click the required action: For active clusters, click Suspend . For paused clusters, click Resume . See also Setting up a development environment for DBaaS","title":"DBaaS Dashboard"},{"location":"using/platform/dbaas.html#kubernetes-clusters","text":"","title":"Kubernetes clusters"},{"location":"using/platform/dbaas.html#add-a-kubernetes-cluster","text":"Click Register new Kubernetes Cluster Enter values for the Kubernetes Cluster Name and Kubeconfig file in the corresponding fields. Click Register . A message will momentarily display telling you whether the registration was successful or not.","title":"Add a Kubernetes cluster"},{"location":"using/platform/dbaas.html#unregister-a-kubernetes-cluster","text":"Note You can\u2019t unregister a Kubernetes cluster if there DB clusters associated with it. Click Unregister . Confirm the action by clicking Proceed , or abandon by clicking Cancel .","title":"Unregister a Kubernetes cluster"},{"location":"using/platform/dbaas.html#view-a-kubernetes-clusters-configuration","text":"Find the row with the Kubernetes cluster you want to see. In the Actions column, open the menu and click Show configuration .","title":"View a Kubernetes cluster's configuration"},{"location":"using/platform/dbaas.html#db-clusters","text":"","title":"DB clusters"},{"location":"using/platform/dbaas.html#add-a-db-cluster","text":"You must create at least one Kubernetes cluster to create a DB cluster. To monitor a DB cluster, set up a public address for PMM Server first. Tip Resource consumption in Kubernetes can cause problems. Use this formula to ensure your nodes have enough resources to start the requested configuration: ( 2 * # of nodes in DB cluster * CPU per node ) + (.5 * # of nodes in db cluster) = total # of CPUs that must be free for cluster to start The first part of the equation is resources for the cluster. It is doubled because each DB cluster member must also have a proxy started with it. The second part is to start the container(s) that automatically monitor each member of the DB cluster. (You can also specify CPU in decimal tenths, e.g. .1 CPUs or 1.5 CPUs.) Select the DB Cluster tab. Click Create DB Cluster . In section 1, Basic Options : Enter a value for Cluster name that complies with domain naming rules. Select a cluster from the Kubernetes Cluster menu. Select a database type from the Database Type menu. Expand section 2, Advanced Options . Select Topology , either Cluster or Single Node . Select the number of nodes. (The lower limit is 3.) Select a preset for Resources per Node . Small , Medium and Large are fixed preset values for Memory , CPU , and Disk . Values for the Custom preset can be edited. When both Basic Options and Advanced Options section icons are green, the Create Cluster button becomes active. (If it is inactive, check the values for fields in sections whose icon is red.) Click Create Cluster to create your cluster. A row appears with information on your cluster: Name : The cluster name Database type : The cluster database type Connection : Host : The hostname Port : The port number Username : The connection username Password : The connection password (click the eye icon to reveal) DB Cluster Parameters : K8s cluster name : The Kubernetes cluster name CPU : The number of CPUs allocated to the cluster Memory : The amount of memory allocated to the cluster Disk : The amount of disk space allocated to the cluster Cluster Status : PENDING : The cluster is being created ACTIVE : The cluster is active FAILED : The cluster could not be created DELETING : The cluster is being deleted","title":"Add a DB Cluster"},{"location":"using/platform/dbaas.html#delete-a-db-cluster","text":"Find the row with the database cluster you want to delete. In the Actions column, open the menu and click Delete . Confirm the action by clicking Proceed , or abandon by clicking Cancel .","title":"Delete a DB Cluster"},{"location":"using/platform/dbaas.html#edit-a-db-cluster","text":"Select the DB Cluster tab. Find the row with the database cluster you want to change. In the Actions column, open the menu and click Edit . A paused cluster can\u2019t be edited.","title":"Edit a DB Cluster"},{"location":"using/platform/dbaas.html#restart-a-db-cluster","text":"Select the DB Cluster tab. Identify the database cluster to be changed. In the Actions column, open the menu and click Restart .","title":"Restart a DB Cluster"},{"location":"using/platform/dbaas.html#suspend-or-resume-a-db-cluster","text":"Select the DB Cluster tab. Identify the DB cluster to suspend or resume. In the Actions column, open the menu and click the required action: For active clusters, click Suspend . For paused clusters, click Resume . See also Setting up a development environment for DBaaS","title":"Suspend or resume a DB Cluster"},{"location":"using/platform/security-threat-tool.html","text":"Security Threat Tool The Security Threat Tool runs regular checks against connected databases, alerting you if any servers pose a potential security threat. How to enable List of checks made All checks run on the PMM Client side. Results are sent to PMM Server where a summary count is shown on the Home Dashboard , with details in the PMM Database Checks dashboard. Checks are automatically downloaded from Percona Enterprise Platform and run every 24 hours. (This period is not configurable.) Check results data always remains on the PMM Server. It is not related to anonymous data sent for Telemetry purposes. The Failed security checks panel on the Home Dashboard shows the number of failed checks classed as critical (red), major (amber), and trivial (blue). Key Critical / Major / Trivial Details are in the PMM Database Checks dashboard (select PMM\u2192PMM Database Checks ). How to enable The Security Threat Tool is disabled by default. Enable it in PMM Settings\u2192Advanced Settings . After activating the Security Threat Tool, you must wait 24 hours for data to appear in the dashboard. List of checks made Check ID Description mongodb_auth MongoDB authentication is disabled mongodb_version MongoDB/PSMDB version is not the latest mysql_anonymous_users There are accounts with no username mysql_empty_password There are users without passwords mysql_version MySQL/PS/MariaDB version is not the latest postgresql_super_role PostgreSQL has users (besides postgres , rdsadmin , and pmm_user ) with the role \u2018SUPER\u2019 postgresql_version PostgreSQL version is not the latest","title":"Security Threat Tool"},{"location":"using/platform/security-threat-tool.html#how-to-enable","text":"The Security Threat Tool is disabled by default. Enable it in PMM Settings\u2192Advanced Settings . After activating the Security Threat Tool, you must wait 24 hours for data to appear in the dashboard.","title":"How to enable"},{"location":"using/platform/security-threat-tool.html#list-of-checks-made","text":"Check ID Description mongodb_auth MongoDB authentication is disabled mongodb_version MongoDB/PSMDB version is not the latest mysql_anonymous_users There are accounts with no username mysql_empty_password There are users without passwords mysql_version MySQL/PS/MariaDB version is not the latest postgresql_super_role PostgreSQL has users (besides postgres , rdsadmin , and pmm_user ) with the role \u2018SUPER\u2019 postgresql_version PostgreSQL version is not the latest","title":"List of checks made"}]}